# %%
import pandas as pd
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine

load_dotenv()  # ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÇ‡∏´‡∏•‡∏î .env

user = os.getenv('DB_USER')
password = os.getenv('DB_PASSWORD')
host = os.getenv('DB_HOST')
port = os.getenv('DB_PORT')  
database = 'fininsurance'

engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}')

query = """
SELECT quo_num, id_motor1, id_motor2, datestart
FROM fin_system_pay
WHERE datestart >= '2025-05-01' AND datestart < '2025-07-01'
AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
"""

df = pd.read_sql(query, engine)
df


# %%
import pandas as pd
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine

load_dotenv() 

user = os.getenv('DB_USER')
password = os.getenv('DB_PASSWORD')
host = os.getenv('DB_HOST')
port = os.getenv('DB_PORT')  
database = 'fininsurance'

engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}')

query = """
SELECT quo_num, idcar, carprovince, camera, no_car, brandplan, seriesplan, sub_seriesplan, yearplan, detail_car, vehGroup, vehBodyTypeDesc, seatingCapacity, weight_car, cc_car, color_car, datestart
FROM fin_system_select_plan
WHERE datestart >= '2025-05-01' AND datestart < '2025-07-01'
AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
"""

df1 = pd.read_sql(query, engine)
df1


# %%
df_merged = pd.merge(df, df1, on='quo_num', how='left')
df_merged

# %%
df_merged = df_merged.drop_duplicates(subset=['id_motor2'])
df_merged

# %%
df_merged = df_merged.drop_duplicates(subset=['idcar'])
df_merged

# %%
# df_merged = df_merged.drop(columns=['datestart_x', 'datestart_y', 'quo_num'])
# df_merged

# %%
df_merged = df_merged.drop(columns=['datestart_x', 'datestart_y'])
df_merged

# %%
rename_columns = {
    "id_motor2": "car_id",
    "id_motor1": "engine_number",
    "idcar": "car_registration",
    "carprovince": "car_province",
    "camera": "camera",
    "no_car": "car_no",
    "brandplan": "car_brand",
    "seriesplan": "car_series",
    "sub_seriesplan": "car_subseries",
    "yearplan": "car_year",
    "detail_car": "car_detail",
    "vehGroup": "vehicle_group",
    "vehBodyTypeDesc": "vehBodyTypeDesc",
    "seatingCapacity": "seat_count",
    "weight_car": "vehicle_weight",
    "cc_car": "engine_capacity",
    "color_car": "vehicle_color"
}

df = df_merged.rename(columns=rename_columns)
df

# %%
df = df.replace(r'^\s*$', pd.NA, regex=True)  
df = df[df.count(axis=1) > 1]
df

# %%
import pandas as pd
import numpy as np

# ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô NaN ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df_temp = df.replace(r'^\s*$', np.nan, regex=True)

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (non-null)
df['non_empty_count'] = df_temp.notnull().sum(axis=1)

# >>>> ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ <<<<
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
valid_car_id_mask = df['car_id'].astype(str).str.strip().ne('') & df['car_id'].notna()

# ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà car_id ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞ car_id ‡∏ß‡πà‡∏≤‡∏á
df_with_id = df[valid_car_id_mask]
df_without_id = df[~valid_car_id_mask]

# ‡∏Ñ‡∏±‡∏î‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà car_id ‡∏ã‡πâ‡∏≥ ‡πÇ‡∏î‡∏¢‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
df_with_id_cleaned = df_with_id.sort_values('non_empty_count', ascending=False).drop_duplicates(subset='car_id', keep='first')

# ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö
df_cleaned = pd.concat([df_with_id_cleaned, df_without_id], ignore_index=True)

# ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢
df_cleaned = df_cleaned.drop(columns=['non_empty_count'])
df_cleaned = df_cleaned.replace(
    to_replace=r'^\s*$|(?i:^none$)|^-$',  # << ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
    value=np.nan,
    regex=True
)


df_cleaned.columns = df_cleaned.columns.str.lower()
df_cleaned


# %%
df_cleaned.replace(np.nan, "NaN").isin(["none", "-", "None"]).sum()
df_cleaned

# %%
df_cleaned = df_cleaned.replace(r'^\.$', np.nan, regex=True)
df_cleaned

# %%
import numpy as np

# ‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤ "‡∏≠‡∏∑‡πà‡∏ô‡πÜ" ‡∏î‡πâ‡∏ß‡∏¢ np.nan
df_cleaned['seat_count'] = df_cleaned['seat_count'].replace("‡∏≠‡∏∑‡πà‡∏ô‡πÜ", np.nan)

# ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏õ‡∏•‡∏á type ‡πÄ‡∏õ‡πá‡∏ô numeric (optional)
df_cleaned['seat_count'] = pd.to_numeric(df_cleaned['seat_count'], errors='coerce')
df_cleaned

# %%
province_list = [
    "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå", "‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£",
    "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤", "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó", "‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥",
    "‡∏ä‡∏∏‡∏°‡∏û‡∏£", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢", "‡∏ï‡∏£‡∏±‡∏á", "‡∏ï‡∏£‡∏≤‡∏î", "‡∏ï‡∏≤‡∏Å", "‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å",
    "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤", "‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä", "‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå",
    "‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™", "‡∏ô‡πà‡∏≤‡∏ô", "‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨", "‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ",
    "‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå", "‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤",
    "‡∏û‡∏±‡∏á‡∏á‡∏≤", "‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á", "‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£", "‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå",
    "‡πÅ‡∏û‡∏£‡πà", "‡∏û‡∏∞‡πÄ‡∏¢‡∏≤", "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï", "‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°", "‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£", "‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô",
    "‡∏¢‡∏∞‡∏•‡∏≤", "‡∏¢‡πÇ‡∏™‡∏ò‡∏£", "‡∏£‡∏∞‡∏ô‡∏≠‡∏á", "‡∏£‡∏∞‡∏¢‡∏≠‡∏á", "‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ", "‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î", "‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ",
    "‡∏•‡∏≥‡∏õ‡∏≤‡∏á", "‡∏•‡∏≥‡∏û‡∏π‡∏ô", "‡πÄ‡∏•‡∏¢", "‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©", "‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£", "‡∏™‡∏á‡∏Ç‡∏•‡∏≤", "‡∏™‡∏ï‡∏π‡∏•",
    "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß", "‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ",
    "‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢", "‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå",
    "‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢", "‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏±‡∏ß‡∏•‡∏≥‡∏†‡∏π", "‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á", "‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ",
    "‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç"
]


# %%
import re

def extract_clean_plate(value):
    if pd.isnull(value) or value.strip() == "":
        return None

    text = value.strip()

    # ‡∏ï‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ / ‡∏´‡∏£‡∏∑‡∏≠ ///
    text = re.split(r'[\/]', text)[0].strip()

    # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á space
    parts = text.split()
    if len(parts) > 0:
        text = parts[0].strip()
    else:
        return None

    # ‡∏•‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î
    for prov in province_list:
        if prov in text:
            text = text.replace(prov, "").strip()

    # ‡πÉ‡∏ä‡πâ regex ‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á
    reg_match = re.match(r'^((?:\d{1,2})?[‡∏Å-‡∏Æ]{1,3}\d{1,4})', text)
    if reg_match:
        final_plate = reg_match.group(1)

        # ‡∏•‡∏ö‡∏Ç‡∏µ‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        final_plate = final_plate.replace('-', '')

        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç 2 ‡∏ï‡∏±‡∏ß ‚Üí ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
        match_two_digits = re.match(r'^(\d{2})([‡∏Å-‡∏Æ].*)$', final_plate)
        if match_two_digits:
            final_plate = match_two_digits.group(1)[1:] + match_two_digits.group(2)

        # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 0 ‚Üí ‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å
        if final_plate.startswith("0"):
            final_plate = final_plate[1:]

        return final_plate
    else:
        return None

# Apply ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_registration
df_cleaned['car_registration'] = df_cleaned['car_registration'].apply(extract_clean_plate)

df_cleaned


# %%


# %% [markdown]
# db postgres update table quotation

# %%
import pandas as pd
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine
import numpy as np

# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å .env
load_dotenv()

# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å environment
user = os.getenv('DB_USER_test')
password = os.getenv('DB_PASSWORD_test')
host = os.getenv('DB_HOST_test')
port = os.getenv('DB_PORT_test')  
database = 'fininsurance'

# ‡∏™‡∏£‡πâ‡∏≤‡∏á engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}')

# SQL query
query = """
SELECT *
FROM dim_car 
"""

df5 = pd.read_sql(query, engine)
df5

# %%
df5 = df5.drop(columns=['create_at', 'update_at'])
df5

# %%
df_cleaned['car_year'] = df_cleaned['car_year'].astype('Int64')
df5['car_year'] = df5['car_year'].astype('Int64')

# %%
import numpy as np

df5 = df5.replace(["NaN", "NAN", "nan"], np.nan)
df_cleaned = df_cleaned.replace(["NaN", "NAN", "nan"], np.nan)

# %%
fix_cols = ['vehicle_group', 'vehicle_weight', 'engine_capacity']

for col in fix_cols:
    df_cleaned[col] = df_cleaned[col].astype(str).replace('nan', pd.NA)
    df5[col] = df5[col].astype(str).replace('nan', pd.NA)


# %%
# df_concat = pd.concat([df_cleaned.reset_index(drop=True), df5[['car_sk']].reset_index(drop=True)], axis=1)
# df_concat

# %%
df_result = pd.merge(df_cleaned, df5, on=['car_id', 'car_registration'], how='right')
df_result

# %%
df_result = df_result[['quo_num', 'car_sk']]
df_result

# %%
df_result = df_result.rename(columns={'quo_num': 'quotation_num'})
df_result

# %%
# df_result.to_excel('test1.xlsx', index=False, engine='openpyxl')

# %%
import pandas as pd
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine
import numpy as np

# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å .env
load_dotenv()

# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å environment
user = os.getenv('DB_USER_test')
password = os.getenv('DB_PASSWORD_test')
host = os.getenv('DB_HOST_test')
port = os.getenv('DB_PORT_test')  
database = 'fininsurance'

# ‡∏™‡∏£‡πâ‡∏≤‡∏á engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}')

# SQL query
query = """
SELECT *
FROM fact_sales_quotation 
"""

df6 = pd.read_sql(query, engine)
df6

# %%
df6 = df6.drop(columns=['create_at', 'update_at', 'car_id'])
df6

# %%
# df6 = df6.drop(columns=['car_id'])
# df6

# %%
df_result1 = pd.merge(df_result, df6, on=['quotation_num'], how='right')
df_result1

# %%
df_result1 = df_result1.rename(columns={'car_sk': 'car_id'})
df_result1

# %%
df_result1 = df_result1.drop_duplicates(subset=['quotation_num'], keep='last')
df_result1

# %%
import numpy as np
import pandas as pd

# ‡πÅ‡∏Å‡πâ NaT, NaN ‡∏ó‡∏±‡πâ‡∏á dataframe ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None
df_result1 = df_result1.where(pd.notnull(df_result1), None)


# %%
# datetime_cols = df_result1.select_dtypes(include=["datetime", "datetimetz"]).columns

# for col in datetime_cols:
#     df_result1[col] = df_result1[col].astype(str).replace("NaT", None)


# %%
import os
from sqlalchemy import create_engine, MetaData, Table, update

user = os.getenv('DB_USER_test')
password = os.getenv('DB_PASSWORD_test')
host = os.getenv('DB_HOST_test')
port = os.getenv('DB_PORT_test')
database = 'fininsurance'

engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}')

metadata = MetaData()
table = Table('fact_sales_quotation', metadata, autoload_with=engine)

records = df_result1.to_dict(orient='records')

chunk_size = 5000

for start in range(0, len(records), chunk_size):
    end = start + chunk_size
    chunk = records[start:end]

    print(f"üîÑ Updating chunk {start // chunk_size + 1}: records {start} to {end - 1}")

    with engine.begin() as conn:
        for record in chunk:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ quotation_num ‡πÅ‡∏•‡∏∞ car_id ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if 'quotation_num' not in record or pd.isna(record['quotation_num']):
                print(f"‚ö†Ô∏è Skip row: no quotation_num: {record}")
                continue
            if 'car_id' not in record or pd.isna(record['car_id']):
                print(f"‚ö†Ô∏è Skip row: no car_id: {record}")
                continue

            # ‚úÖ Update ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            stmt = (
                update(table)
                .where(table.c.quotation_num == record['quotation_num'])
                .values(car_id=record['car_id'])
            )
            conn.execute(stmt)

print("‚úÖ Update car_id completed successfully.")

# %%
# df_result1.to_excel('test.xlsx', index=False, engine='openpyxl')

# %%



