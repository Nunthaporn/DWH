from dagster import op, job
import pandas as pd
import numpy as np
import re
import os
import time
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, inspect
from sqlalchemy import create_engine, MetaData, Table, inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import OperationalError, DisconnectionError
from datetime import datetime, timedelta

# ‚úÖ Load .env
load_dotenv()

# ‚úÖ DB source (MariaDB)
source_user = os.getenv('DB_USER')
source_password = os.getenv('DB_PASSWORD')
source_host = os.getenv('DB_HOST')
source_port = os.getenv('DB_PORT')
source_db = 'fininsurance'

source_engine = create_engine(
    f"mysql+pymysql://{source_user}:{source_password}@{source_host}:{source_port}/{source_db}"
)

# ‚úÖ DB target (PostgreSQL)
target_user = os.getenv('DB_USER_test')
target_password = os.getenv('DB_PASSWORD_test')
target_host = os.getenv('DB_HOST_test')
target_port = os.getenv('DB_PORT_test')
target_db = 'fininsurance'

target_engine = create_engine(
    f"postgresql+psycopg2://{target_user}:{target_password}@{target_host}:{target_port}/{target_db}",
    pool_size=5,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    connect_args={
        "connect_timeout": 30,
        "application_name": "dim_car_etl"
    }
)

def retry_db_operation(operation, max_retries=3, delay=2):
    """Retry database operation with exponential backoff"""
    for attempt in range(max_retries):
        try:
            return operation()
        except (OperationalError, DisconnectionError) as e:
            if attempt == max_retries - 1:
                raise e
            print(f"‚ö†Ô∏è Database connection error (attempt {attempt + 1}/{max_retries}): {e}")
            print(f"‚è≥ Retrying in {delay} seconds...")
            time.sleep(delay)
            delay *= 2  # Exponential backoff

@op
def extract_car_data():
    now = datetime.now()

    start_time = now - timedelta(days=1)
    end_time = now

    start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
    end_str = end_time.strftime('%Y-%m-%d %H:%M:%S') 

    print(f"üîç Querying data from {start_str} to {end_str}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    try:
        with source_engine.connect() as conn:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            inspector = inspect(conn)
            tables = inspector.get_table_names()
            print(f"üîç Available tables: {tables}")
            
            if 'fin_system_pay' not in tables:
                print("‚ùå ERROR: Table 'fin_system_pay' not found!")
                return pd.DataFrame()
            if 'fin_system_select_plan' not in tables:
                print("‚ùå ERROR: Table 'fin_system_select_plan' not found!")
                return pd.DataFrame()
            
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤
            count_pay = conn.execute(text(f"SELECT COUNT(*) FROM fin_system_pay WHERE datestart BETWEEN '{start_str}' AND '{end_str}' AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')")).scalar()
            count_plan = conn.execute(text(f"SELECT COUNT(*) FROM fin_system_select_plan WHERE datestart BETWEEN '{start_str}' AND '{end_str}' AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')")).scalar()
            print(f"üìä Records in date range - fin_system_pay: {count_pay}, fin_system_select_plan: {count_plan}")
            
            # ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô 7 ‡∏ß‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á 3 ‡∏ß‡∏±‡∏ô
            if count_pay == 0 and count_plan == 0:
                print("‚ö†Ô∏è No data in 7 days, trying 3 days...")
                start_time_3 = now - timedelta(days=3)
                start_str_3 = start_time_3.strftime('%Y-%m-%d %H:%M:%S')
                count_pay_3 = conn.execute(text(f"SELECT COUNT(*) FROM fin_system_pay WHERE datestart BETWEEN '{start_str_3}' AND '{end_str}' AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')")).scalar()
                count_plan_3 = conn.execute(text(f"SELECT COUNT(*) FROM fin_system_select_plan WHERE datestart BETWEEN '{start_str_3}' AND '{end_str}' AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')")).scalar()
                
                if count_pay_3 > 0 or count_plan_3 > 0:
                    print(f"‚úÖ Found data in 3 days - fin_system_pay: {count_pay_3}, fin_system_select_plan: {count_plan_3}")
                    start_str = start_str_3
                    start_time = start_time_3
                else:
                    print("‚ö†Ô∏è No data in 3 days, using last 1000 records")
                    # ‡πÉ‡∏ä‡πâ LIMIT 1000 ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                    start_str = None
                    end_str = None
            
    except Exception as e:
        print(f"‚ùå ERROR connecting to database: {e}")
        return pd.DataFrame()

    # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö query ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    if start_str and end_str:
        query_pay = f"""
            SELECT quo_num, id_motor1, id_motor2, update_at
            FROM fin_system_pay
            WHERE update_at BETWEEN '{start_str}' AND '{end_str}'
            AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
            ORDER BY update_at DESC
        """
        
        query_plan = f"""
            SELECT quo_num, idcar, carprovince, camera, no_car, brandplan, seriesplan, sub_seriesplan,
                   yearplan, detail_car, vehGroup, vehBodyTypeDesc, seatingCapacity,
                   weight_car, cc_car, color_car, update_at
            FROM fin_system_select_plan
            WHERE update_at BETWEEN '{start_str}' AND '{end_str}'
            AND type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
            ORDER BY update_at DESC
        """
    else:
        # ‚úÖ ‡πÉ‡∏ä‡πâ LIMIT ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        query_pay = """
            SELECT quo_num, id_motor1, id_motor2, update_at
            FROM fin_system_pay
            WHERE type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
            ORDER BY update_at DESC
        """
        
        query_plan = """
            SELECT quo_num, idcar, carprovince, camera, no_car, brandplan, seriesplan, sub_seriesplan,
                   yearplan, detail_car, vehGroup, vehBodyTypeDesc, seatingCapacity,
                   weight_car, cc_car, color_car, update_at
            FROM fin_system_select_plan
            WHERE type_insure IN ('‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ', '‡∏ï‡∏£‡∏≠')
            ORDER BY update_at DESC
        """
    
    try:
        df_pay = pd.read_sql(query_pay, source_engine)
        print(f"üì¶ df_pay: {df_pay.shape}")
    except Exception as e:
        print(f"‚ùå ERROR querying fin_system_pay: {e}")
        df_pay = pd.DataFrame()

    try:
        df_plan = pd.read_sql(query_plan, source_engine)
        print(f"üì¶ df_plan: {df_plan.shape}")
    except Exception as e:
        print(f"‚ùå ERROR querying fin_system_select_plan: {e}")
        df_plan = pd.DataFrame()

    # ‚úÖ Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    if not df_pay.empty and not df_plan.empty:
        # ‚úÖ ‡∏•‡∏ö duplicates ‡∏Å‡πà‡∏≠‡∏ô merge
        df_pay = df_pay.drop_duplicates(subset=['quo_num'], keep='first')
        df_plan = df_plan.drop_duplicates(subset=['quo_num'], keep='first')
        
        df_merged = pd.merge(df_pay, df_plan, on='quo_num', how='left')
        print(f"üìä After merge: {df_merged.shape}")
    elif not df_pay.empty:
        print("‚ö†Ô∏è Only fin_system_pay has data, using it alone")
        df_merged = df_pay.copy()
    elif not df_plan.empty:
        print("‚ö†Ô∏è Only fin_system_select_plan has data, using it alone")
        df_merged = df_plan.copy()
    else:
        print("‚ùå No data found in both tables")
        df_merged = pd.DataFrame()
    
    # ‚úÖ ‡∏•‡∏ö duplicates ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    if not df_merged.empty:
        df_merged = df_merged.drop_duplicates()
        print(f"üìä After removing duplicates: {df_merged.shape}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    if 'id_motor2' in df_merged.columns:
        valid_car_ids = df_merged['id_motor2'].notna().sum()
        print(f"‚úÖ Valid car_ids: {valid_car_ids}/{len(df_merged)}")
    
    return df_merged

@op
def clean_car_data(df: pd.DataFrame):
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df.empty:
        print("‚ö†Ô∏è WARNING: Input DataFrame is empty!")
        print("üîç Returning empty DataFrame with expected columns")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
        expected_columns = [
            'quotation_num', 'car_id', 'engine_number', 'car_registration', 
            'car_province', 'camera', 'car_no', 'car_brand', 'car_series', 
            'car_subseries', 'car_year', 'car_detail', 'vehicle_group', 
            'vehbodytypedesc', 'seat_count', 'vehicle_weight', 'engine_capacity', 
            'vehicle_color'
        ]
        return pd.DataFrame(columns=expected_columns)
    
    print(f"üîç Starting data cleaning with {len(df)} records")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    required_cols = ['id_motor2', 'idcar', 'quo_num']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è WARNING: Missing required columns: {missing_cols}")
        print(f"üîç Available columns: {list(df.columns)}")
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        available_cols = [col for col in required_cols if col in df.columns]
        if not available_cols:
            print("‚ùå ERROR: No required columns found!")
            return pd.DataFrame()
    
    # ‚úÖ ‡∏•‡∏ö duplicates ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    print(f"üìä Before removing duplicates: {df.shape}")
    if 'id_motor2' in df.columns:
        df = df.drop_duplicates(subset=['id_motor2'], keep='first')
        print(f"üìä After removing id_motor2 duplicates: {df.shape}")
    
    # ‚úÖ ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå update_at ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
    df = df.drop(columns=['update_at_x', 'update_at_y'], errors='ignore')

    rename_columns = {
        "quo_num": "quotation_num",
        "id_motor2": "car_id",
        "id_motor1": "engine_number",
        "idcar": "car_registration",
        "carprovince": "car_province",
        "camera": "camera",
        "no_car": "car_no",
        "brandplan": "car_brand",
        "seriesplan": "car_series",
        "sub_seriesplan": "car_subseries",
        "yearplan": "car_year",
        "detail_car": "car_detail",
        "vehGroup": "vehicle_group",
        "vehBodyTypeDesc": "vehbodytypedesc",
        "seatingCapacity": "seat_count",
        "weight_car": "vehicle_weight",
        "cc_car": "engine_capacity",
        "color_car": "vehicle_color"
    }

    # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    existing_columns = [col for col in rename_columns.keys() if col in df.columns]
    if existing_columns:
        rename_dict = {col: rename_columns[col] for col in existing_columns}
        df = df.rename(columns=rename_dict)
        print(f"‚úÖ Renamed {len(existing_columns)} columns: {list(rename_dict.values())}")
    else:
        print("‚ö†Ô∏è WARNING: No columns to rename found!")
        print(f"üîç Available columns: {list(df.columns)}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    print("üîç Column renaming check:")
    if 'car_id' in df.columns:
        car_id_count = df['car_id'].notna().sum()
        print(f"‚úÖ car_id column exists with {car_id_count} valid values")
    else:
        print("‚ö†Ô∏è WARNING: car_id column not found after renaming!")
        print(f"üîç Available columns: {list(df.columns)}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        df['car_id'] = None
        print("‚ûï Created empty car_id column")
    
    df = df.replace(r'^\s*$', pd.NA, regex=True)
    df_temp = df.replace(r'^\s*$', np.nan, regex=True)
    df['non_empty_count'] = df_temp.notnull().sum(axis=1)

    valid_mask = df['car_id'].astype(str).str.strip().ne('') & df['car_id'].notna()
    df_with_id = df[valid_mask]
    df_without_id = df[~valid_mask]
    print(f"üìä Records with valid car_id: {len(df_with_id)}")
    print(f"üìä Records without car_id: {len(df_without_id)}")
    
    df_with_id_cleaned = df_with_id.sort_values('non_empty_count', ascending=False).drop_duplicates(subset='car_id', keep='first')
    print(f"üìä After removing car_id duplicates: {len(df_with_id_cleaned)}")
    
    df_cleaned = pd.concat([df_with_id_cleaned, df_without_id], ignore_index=True)
    df_cleaned = df_cleaned.drop(columns=['non_empty_count'])

    df_cleaned.columns = df_cleaned.columns.str.lower()
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
    print("üîç After converting columns to lowercase:")
    if 'car_id' in df_cleaned.columns:
        car_id_count = df_cleaned['car_id'].notna().sum()
        print(f"‚úÖ car_id column exists with {car_id_count} valid values")
    else:
        print("‚ö†Ô∏è WARNING: car_id column not found after lowercase conversion!")
        print(f"üîç Available columns: {list(df_cleaned.columns)}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        df_cleaned['car_id'] = None
        print("‚ûï Created empty car_id column after lowercase conversion")

    df_cleaned['seat_count'] = df_cleaned['seat_count'].replace("‡∏≠‡∏∑‡πà‡∏ô‡πÜ", np.nan)
    df_cleaned['seat_count'] = pd.to_numeric(df_cleaned['seat_count'], errors='coerce')

    province_list = ["‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå", "‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£",
        "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤", "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó", "‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥",
        "‡∏ä‡∏∏‡∏°‡∏û‡∏£", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢", "‡∏ï‡∏£‡∏±‡∏á", "‡∏ï‡∏£‡∏≤‡∏î", "‡∏ï‡∏≤‡∏Å", "‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å",
        "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤", "‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä", "‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå",
        "‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™", "‡∏ô‡πà‡∏≤‡∏ô", "‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨", "‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ",
        "‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå", "‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤",
        "‡∏û‡∏±‡∏á‡∏á‡∏≤", "‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á", "‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£", "‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "‡πÅ‡∏û‡∏£‡πà", "‡∏û‡∏∞‡πÄ‡∏¢‡∏≤", "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï", "‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°", "‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£", "‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô",
        "‡∏¢‡∏∞‡∏•‡∏≤", "‡∏¢‡πÇ‡∏™‡∏ò‡∏£", "‡∏£‡∏∞‡∏ô‡∏≠‡∏á", "‡∏£‡∏∞‡∏¢‡∏≠‡∏á", "‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ", "‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î", "‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ",
        "‡∏•‡∏≥‡∏õ‡∏≤‡∏á", "‡∏•‡∏≥‡∏û‡∏π‡∏ô", "‡πÄ‡∏•‡∏¢", "‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©", "‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£", "‡∏™‡∏á‡∏Ç‡∏•‡∏≤", "‡∏™‡∏ï‡∏π‡∏•",
        "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß", "‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ",
        "‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢", "‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå",
        "‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢", "‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏±‡∏ß‡∏•‡∏≥‡∏†‡∏π", "‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á", "‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ",
        "‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç"]

    def extract_clean_plate(value):
        if pd.isnull(value) or str(value).strip() == "":
            return None
        
        try:
            # ‡πÅ‡∏ö‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢ / ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å
            parts = re.split(r'[\/]', str(value).strip())
            if not parts or not parts[0]:
                return None
            
            # ‡πÅ‡∏ö‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å
            words = parts[0].split()
            if not words:
                return None
            
            text = words[0]
            
            # ‡∏•‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏≠‡∏≠‡∏Å
            for prov in province_list:
                if prov in text:
                    text = text.replace(prov, "").strip()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏£‡∏ñ
            reg_match = re.match(r'^((?:\d{1,2})?[‡∏Å-‡∏Æ]{1,3}\d{1,4})', text)
            if reg_match:
                final_plate = reg_match.group(1).replace('-', '')
                match_two_digits = re.match(r'^(\d{2})([‡∏Å-‡∏Æ].*)$', final_plate)
                if match_two_digits:
                    final_plate = match_two_digits.group(1)[1:] + match_two_digits.group(2)
                if final_plate.startswith("0"):
                    final_plate = final_plate[1:]
                return final_plate
            else:
                return None
        except (IndexError, AttributeError, TypeError) as e:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡πÉ‡∏´‡πâ return None
            return None

    df_cleaned['car_registration'] = df_cleaned['car_registration'].apply(extract_clean_plate)

    # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö pattern ‡πÉ‡∏´‡∏°‡πà: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "-", ".", "none", "NaN", "UNDEFINE", "undefined"
    pattern_to_none = r'^[-\.]+$|^(?i:none|nan|undefine|undefined)$'
    df_cleaned = df_cleaned.replace(pattern_to_none, np.nan, regex=True)

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î engine_number ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ A-Z, a-z, 0-9
    def clean_engine_number(value):
        if pd.isnull(value):
            return None
        cleaned = re.sub(r'[^A-Za-z0-9]', '', str(value))
        return cleaned if cleaned else None

    df_cleaned['engine_number'] = df_cleaned['engine_number'].apply(clean_engine_number)

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î car_province ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î
    def clean_province(value):
        if pd.isnull(value):
            return None
        value = str(value).strip()
        if value in province_list:
            return value
        return None

    if 'car_province' in df_cleaned.columns:
        df_cleaned['car_province'] = df_cleaned['car_province'].apply(clean_province)
    else:
        print("‚ö†Ô∏è Column 'car_province' not found in DataFrame")
        
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà applymap ‡∏ó‡∏µ‡πà deprecated ‡∏î‡πâ‡∏ß‡∏¢ map
    for col in df_cleaned.columns:
        if df_cleaned[col].dtype == 'object':
            df_cleaned[col] = df_cleaned[col].map(lambda x: x.strip() if isinstance(x, str) else x)
    
    if 'car_no' in df_cleaned.columns:
        df_cleaned['car_no'] = df_cleaned['car_no'].replace("‡πÑ‡∏°‡πà‡∏°‡∏µ", np.nan)
    else:
        print("‚ö†Ô∏è Column 'car_no' not found in DataFrame")
        
    if 'car_brand' in df_cleaned.columns:
        df_cleaned['car_brand'] = df_cleaned['car_brand'].replace("-", np.nan)

    def has_thai_chars(value):
        if pd.isnull(value):
            return False
        return bool(re.search(r'[‡∏Å-‡πô]', str(value)))

    df_cleaned = df_cleaned[~df_cleaned['car_id'].apply(has_thai_chars)]
    
    series_noise_pattern = r"^[-‚Äì_\.\/\+].*|^<=200CC$|^>250CC$|^'NQR 75$"

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_series ‡πÅ‡∏•‡∏∞ car_subseries ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'car_series' in df_cleaned.columns:
        df_cleaned['car_series'] = df_cleaned['car_series'].replace(series_noise_pattern, np.nan, regex=True)
    else:
        print("‚ö†Ô∏è Column 'car_series' not found in DataFrame")
        
    if 'car_subseries' in df_cleaned.columns:
        df_cleaned['car_subseries'] = df_cleaned['car_subseries'].replace(series_noise_pattern, np.nan, regex=True)
    else:
        print("‚ö†Ô∏è Column 'car_subseries' not found in DataFrame")

    def remove_leading_vowels(value):
        if pd.isnull(value):
            return value
        # ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÑ‡∏ó‡∏¢‡∏ï‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        return re.sub(r"^[\u0E30-\u0E39\u0E47-\u0E4E\u0E3A]+", "", value.strip())

    if 'car_series' in df_cleaned.columns:
        df_cleaned['car_series'] = df_cleaned['car_series'].apply(remove_leading_vowels)
    if 'car_subseries' in df_cleaned.columns:
        df_cleaned['car_subseries'] = df_cleaned['car_subseries'].apply(remove_leading_vowels)
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î car_brand - ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    if 'car_brand' in df_cleaned.columns:
        df_cleaned['car_brand'] = df_cleaned['car_brand'].apply(remove_leading_vowels)
    else:
        print("‚ö†Ô∏è Column 'car_brand' not found in DataFrame")
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î car_brand ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° - ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    def clean_car_brand(value):
        if pd.isnull(value):
            return None
        
        value = str(value).strip()
        
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
        value = re.sub(r'^[-‚Äì_\.\/\+"\']+', '', value)  # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        value = re.sub(r'[-‚Äì_\.\/\+"\']+$', '', value)  # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏î‡πâ‡∏≤‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        value = re.sub(r'\s+', ' ', value)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
        value = value.strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if value in ['', 'nan', 'None', 'NULL', 'undefined', 'UNDEFINED']:
            return None
        
        return value
    
    if 'car_brand' in df_cleaned.columns:
        df_cleaned['car_brand'] = df_cleaned['car_brand'].apply(clean_car_brand)

        # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î car_brand
        print("üîç Car brand cleaning examples:")
        sample_brands = df_cleaned['car_brand'].dropna().head(10)
        for brand in sample_brands:
            print(f"   - {brand}")
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ car_brand
        brand_stats = df_cleaned['car_brand'].value_counts().head(10)
        print("üîç Top 10 car brands:")
        for brand, count in brand_stats.items():
            print(f"   - {brand}: {count}")

    def clean_engine_capacity(value):
        if pd.isnull(value):
            return None
        value = str(value).strip()
        # ‡∏•‡∏ö‡∏à‡∏∏‡∏î‡∏ã‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô "1..2" ‚Üí "1.2"
        value = re.sub(r'\.{2,}', '.', value)
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
        try:
            float_val = float(value)
            return float_val
        except ValueError:
            return None

    if 'engine_capacity' in df_cleaned.columns:
        df_cleaned['engine_capacity'] = df_cleaned['engine_capacity'].apply(clean_engine_capacity)
    else:
        print("‚ö†Ô∏è Column 'engine_capacity' not found in DataFrame")

    def clean_float_only(value):
        if pd.isnull(value):
            return None
        value = str(value).strip()
        value = re.sub(r'\.{2,}', '.', value)
        if value in ['', '.', '-']:
            return None
        try:
            return float(value)
        except ValueError:
            return None

    if 'engine_capacity' in df_cleaned.columns:
        df_cleaned['engine_capacity'] = df_cleaned['engine_capacity'].apply(clean_float_only)
    if 'vehicle_weight' in df_cleaned.columns:
        df_cleaned['vehicle_weight'] = df_cleaned['vehicle_weight'].apply(clean_float_only)
    else:
        print("‚ö†Ô∏è Column 'vehicle_weight' not found in DataFrame")
    if 'seat_count' in df_cleaned.columns:
        df_cleaned['seat_count'] = df_cleaned['seat_count'].apply(lambda x: int(clean_float_only(x)) if clean_float_only(x) is not None else None)
    else:
        print("‚ö†Ô∏è Column 'seat_count' not found in DataFrame")
    if 'car_year' in df_cleaned.columns:
        df_cleaned['car_year'] = df_cleaned['car_year'].apply(lambda x: int(clean_float_only(x)) if clean_float_only(x) is not None else None)
    else:
        print("‚ö†Ô∏è Column 'car_year' not found in DataFrame")

    if 'seat_count' in df_cleaned.columns:
        df_cleaned['seat_count'] = pd.to_numeric(df_cleaned['seat_count'], errors='coerce').astype('Int64')
    if 'car_year' in df_cleaned.columns:
        df_cleaned['car_year'] = pd.to_numeric(df_cleaned['car_year'], errors='coerce').astype('Int64')
    df_cleaned = df_cleaned.replace(r'NaN', np.nan, regex=True)
    df_cleaned = df_cleaned.drop_duplicates()
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    if 'car_id' in df_cleaned.columns:
        car_id_duplicates = df_cleaned['car_id'].duplicated()
        if car_id_duplicates.any():
            print(f"‚ö†Ô∏è WARNING: Found {car_id_duplicates.sum()} duplicate car_ids after cleaning!")
            duplicate_ids = df_cleaned[car_id_duplicates]['car_id'].tolist()
            print(f"üîç Sample duplicate car_ids: {duplicate_ids[:5]}")
            # ‡∏•‡∏ö duplicates
            df_cleaned = df_cleaned.drop_duplicates(subset=['car_id'], keep='first')
            print(f"üìä After removing final car_id duplicates: {df_cleaned.shape}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ NaN ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    print("üîç NaN check after cleaning:")
    final_nan_check = df_cleaned.isna().sum()
    final_nan_cols = final_nan_check[final_nan_check > 0]
    
    if len(final_nan_cols) > 0:
        print("‚ö†Ô∏è Columns with NaN after cleaning:")
        for col, count in final_nan_cols.items():
            percentage = (count / len(df_cleaned)) * 100
            print(f"   - {col}: {count} NaN values ({percentage:.2f}%)")
    else:
        print("‚úÖ No NaN values found after cleaning")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NaN (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
    if 'car_id' in df_cleaned.columns:
        car_id_nan = df_cleaned['car_id'].isna().sum()
        total_records = len(df_cleaned)
        print(f"üîç car_id status: {total_records - car_id_nan}/{total_records} valid records")
        
        if car_id_nan > 0:
            print(f"‚ö†Ô∏è WARNING: {car_id_nan} records have NaN car_id")
            # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ car_id ‡πÄ‡∏õ‡πá‡∏ô NaN
            df_cleaned = df_cleaned[df_cleaned['car_id'].notna()].copy()
            print(f"‚úÖ Removed {car_id_nan} records with NaN car_id")
            print(f"üìä Remaining records: {len(df_cleaned)}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if len(df_cleaned) == 0:
                print("‚ö†Ô∏è WARNING: No records remaining after removing NaN car_id!")
                print("üîç This means all records had NaN car_id values")
                # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ raise error
                return pd.DataFrame(columns=df_cleaned.columns)
        else:
            print("‚úÖ All car_id values are valid")
    else:
        print("‚ö†Ô∏è WARNING: Column 'car_id' not found in DataFrame!")
        print(f"üîç Available columns: {list(df_cleaned.columns)}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏ß‡πà‡∏≤‡∏á
        df_cleaned['car_id'] = None
        print("‚ûï Created empty car_id column")
    
    print(f"üìä Final cleaned data shape: {df_cleaned.shape}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà
    if 'car_id' not in df_cleaned.columns:
        print("‚ö†Ô∏è WARNING: Column 'car_id' is missing after cleaning!")
        print(f"üîç Available columns: {list(df_cleaned.columns)}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏ß‡πà‡∏≤‡∏á
        df_cleaned['car_id'] = None
        print("‚ûï Created empty car_id column after cleaning")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô car_id ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    car_id_count = df_cleaned['car_id'].notna().sum()
    print(f"‚úÖ Records with valid car_id: {car_id_count}/{len(df_cleaned)}")
    
    if car_id_count == 0:
        print("‚ö†Ô∏è WARNING: No valid car_id records found!")
        if len(df_cleaned) > 0:
            print("üîç Sample of car_id values:")
            print(df_cleaned['car_id'].head(10))
        else:
            print("üîç DataFrame is empty")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    if 'car_id' in df_cleaned.columns:
        df_cleaned = df_cleaned.drop_duplicates(subset=['car_id'], keep='first')
        print(f"üìä Final records after removing car_id duplicates: {len(df_cleaned)}")

    return df_cleaned

@op
def load_car_data(df: pd.DataFrame):
    table_name = 'dim_car'
    pk_column = 'car_id'

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df.empty:
        print("‚ö†Ô∏è WARNING: Input DataFrame is empty!")
        print("üîç Skipping database operations")
        return
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå car_id ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô DataFrame ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    print(f"üîç Available columns in DataFrame: {list(df.columns)}")
    print(f"üîç DataFrame shape: {df.shape}")
    
    if pk_column not in df.columns:
        print(f"‚ö†Ô∏è WARNING: Column '{pk_column}' not found in DataFrame!")
        print(f"üîç Available columns: {list(df.columns)}")
        print(f"üìä DataFrame info:")
        print(df.info())
        print("üîç Skipping database operations due to missing primary key column")
        return
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏µ column 'quotation_num' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡∏™‡∏£‡πâ‡∏≤‡∏á
    def check_and_add_column():
        with target_engine.connect() as conn:
            inspector = inspect(conn)
            columns = [col['name'] for col in inspector.get_columns(table_name)]
            if 'quotation_num' not in columns:
                print("‚ûï Adding missing column 'quotation_num' to dim_car")
                conn.execute(f'ALTER TABLE {table_name} ADD COLUMN quotation_num VARCHAR')
                conn.commit()
    
    retry_db_operation(check_and_add_column)

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á car_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å DataFrame ‡πÉ‡∏´‡∏°‡πà
    df = df[~df[pk_column].duplicated(keep='first')].copy()
    print(f"üìä After removing duplicates: {df.shape}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ NaN ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ database
    print("üîç Checking for NaN values before database insertion:")
    nan_check = df.isna().sum()
    columns_with_nan = nan_check[nan_check > 0]
    
    if len(columns_with_nan) > 0:
        print("‚ö†Ô∏è Columns with NaN values:")
        for col, count in columns_with_nan.items():
            print(f"   - {col}: {count} NaN values")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN
        print("üîç Sample records with NaN values:")
        for col in columns_with_nan.index:
            sample_nan = df[df[col].isna()][[pk_column, col]].head(3)
            if not sample_nan.empty:
                print(f"   {col} NaN examples:")
                for _, row in sample_nan.iterrows():
                    print(f"     - {pk_column}: {row[pk_column]}, {col}: NaN")
    else:
        print("‚úÖ No NaN values found in the data")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (car_id ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô NaN)
    critical_nan = df[pk_column].isna().sum()
    if critical_nan > 0:
        print(f"‚ö†Ô∏è WARNING: {critical_nan} records have NaN in {pk_column} (primary key)")
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ car_id ‡πÄ‡∏õ‡πá‡∏ô NaN
        df = df[df[pk_column].notna()].copy()
        print(f"‚úÖ Removed {critical_nan} records with NaN {pk_column}")
    
    print(f"üìä Final data shape after NaN check: {df.shape}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df.empty:
        print("‚ö†Ô∏è WARNING: No valid data remaining after NaN check!")
        print("üîç Skipping database operations")
        return

    # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
    today_str = datetime.now().strftime('%Y-%m-%d')

    # ‚úÖ Load ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å PostgreSQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    print("üîç Loading existing data from database...")
    with target_engine.connect() as conn:
        df_existing = pd.read_sql(
            f"SELECT {pk_column} FROM {table_name}",
            conn
        )

    print(f"üìä Existing records in database: {len(df_existing)}")

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á car_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
    df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()

    # ‚úÖ Identify car_id ‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô DB)
    new_ids = set(df[pk_column]) - set(df_existing[pk_column])
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()
    print(f"üÜï New car_ids to insert: {len(df_to_insert)}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ insert
    if not df_to_insert.empty:
        df_to_insert = df_to_insert.drop_duplicates(subset=[pk_column], keep='first')
        print(f"üìä After removing duplicates: {len(df_to_insert)} records to insert")

    # ‚úÖ Identify car_id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    common_ids = set(df[pk_column]) & set(df_existing[pk_column])
    df_common_new = df[df[pk_column].isin(common_ids)].copy()
    df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()
    print(f"üîÑ Existing car_ids to update: {len(df_common_new)}")

    # ‚úÖ Merge ‡∏î‡πâ‡∏ß‡∏¢ suffix (_new, _old) - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ update
    if not df_common_new.empty and not df_common_old.empty:
        merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))
        
        # ‚úÖ ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô key ‡πÅ‡∏•‡∏∞ audit fields)
        exclude_columns = [pk_column, 'car_sk', 'create_at', 'update_at']
        compare_cols = [
            col for col in df.columns
            if col not in exclude_columns
            and f"{col}_new" in merged.columns
            and f"{col}_old" in merged.columns
        ]
    else:
        merged = pd.DataFrame()
        compare_cols = []

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not compare_cols:
        print("‚ö†Ô∏è No comparable columns found for update")
        df_diff_renamed = pd.DataFrame()
    else:
        # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å pd.NA
        def is_different(row):
            for col in compare_cols:
                val_new = row.get(f"{col}_new")
                val_old = row.get(f"{col}_old")
                if pd.isna(val_new) and pd.isna(val_old):
                    continue
                if val_new != val_old:
                    return True
            return False

        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
        df_diff = merged[merged.apply(is_different, axis=1)].copy()

        if not df_diff.empty:
            # ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö update ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ car_id ‡∏õ‡∏Å‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡πÄ‡∏ï‡∏¥‡∏° _new)
            update_cols = [f"{col}_new" for col in compare_cols]
            all_cols = [pk_column] + update_cols

            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_diff ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            available_cols = [col for col in all_cols if col in df_diff.columns]
            if len(available_cols) != len(all_cols):
                missing_cols = set(all_cols) - set(df_diff.columns)
                print(f"‚ö†Ô∏è Missing columns in df_diff: {missing_cols}")
                print(f"üîç Available columns: {list(df_diff.columns)}")
            
            df_diff_renamed = df_diff[available_cols].copy()
            # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ column ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á (‡∏•‡∏ö _new ‡∏≠‡∏≠‡∏Å)
            new_column_names = [pk_column] + [col.replace('_new', '') for col in available_cols if col != pk_column]
            df_diff_renamed.columns = new_column_names
        else:
            df_diff_renamed = pd.DataFrame()

    print(f"üÜï Insert: {len(df_to_insert)} rows")
    print(f"üîÑ Update: {len(df_diff_renamed)} rows")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
    from collections import Counter
    all_car_ids = []
    if not df_to_insert.empty:
        all_car_ids.extend(df_to_insert[pk_column].tolist())
    if not df_diff_renamed.empty:
        all_car_ids.extend(df_diff_renamed[pk_column].tolist())
    
    if all_car_ids:
        car_id_counts = Counter(all_car_ids)
        duplicates = {car_id: count for car_id, count in car_id_counts.items() if count > 1}
        if duplicates:
            print(f"‚ö†Ô∏è WARNING: Found {len(duplicates)} duplicate car_ids in all data!")
            for car_id, count in list(duplicates.items())[:5]:
                print(f"   - {car_id}: {count} times")
        else:
            print("‚úÖ No duplicate car_ids found in all data")

    # ‚úÖ Load table metadata
    def load_table_metadata():
        return Table(table_name, MetaData(), autoload_with=target_engine)
    
    metadata = retry_db_operation(load_table_metadata)

    # ‚úÖ Insert (‡∏Å‡∏£‡∏≠‡∏á car_id ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NaN)
    df_to_insert_valid = pd.DataFrame()  # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏Å‡πà‡∏≠‡∏ô
    
    if not df_to_insert.empty:
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NaN ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ Insert
        print("üîç Checking NaN in data to insert:")
        insert_nan_check = df_to_insert.isna().sum()
        insert_nan_cols = insert_nan_check[insert_nan_check > 0]
        if len(insert_nan_cols) > 0:
            print("‚ö†Ô∏è Insert data has NaN in columns:")
            for col, count in insert_nan_cols.items():
                print(f"   - {col}: {count} NaN values")
        
        df_to_insert_valid = df_to_insert[df_to_insert[pk_column].notna()].copy()
        dropped = len(df_to_insert) - len(df_to_insert_valid)
        if dropped > 0:
            print(f"‚ö†Ô∏è Skipped {dropped} records with NaN {pk_column}")
        
        if df_to_insert_valid.empty:
            print("‚ö†Ô∏è WARNING: No valid data to insert after NaN check!")
        else:
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô insert
            df_to_insert_valid = df_to_insert_valid[~df_to_insert_valid[pk_column].duplicated(keep='first')].copy()
            print(f"üìä Final records to insert after duplicate check: {len(df_to_insert_valid)}")
    else:
        print("‚ÑπÔ∏è No data to insert")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà clean ‡πÅ‡∏•‡πâ‡∏ß
    if not df_to_insert_valid.empty:
        # ‚úÖ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢ None ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_to_insert_clean = df_to_insert_valid.replace({np.nan: None})
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö car_id ‡∏ã‡πâ‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        df_to_insert_clean = df_to_insert_clean.drop_duplicates(subset=[pk_column], keep='first')
        print(f"üìä Final clean records to insert: {len(df_to_insert_clean)}")
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á car_id ‡∏ó‡∏µ‡πà‡∏à‡∏∞ insert
        if len(df_to_insert_clean) > 0:
            sample_car_ids = df_to_insert_clean[pk_column].head(5).tolist()
            print(f"üîç Sample car_ids to insert: {sample_car_ids}")
            
            def insert_operation():
                with target_engine.begin() as conn:
                    # ‚úÖ ‡πÉ‡∏ä‡πâ batch insert ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
                    batch_size = 5000  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î batch
                    records = df_to_insert_clean.to_dict(orient='records')
                    
                    for i in range(0, len(records), batch_size):
                        batch = records[i:i + batch_size]
                        try:
                            # ‚úÖ ‡πÉ‡∏ä‡πâ executemany ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch insert
                            stmt = pg_insert(metadata)
                            conn.execute(stmt, batch)
                            print(f"‚úÖ Inserted batch {i//batch_size + 1}/{(len(records) + batch_size - 1)//batch_size} ({len(batch)} records)")
                        except Exception as e:
                            print(f"‚ùå Error inserting batch {i//batch_size + 1}: {e}")
                            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡πÉ‡∏´‡πâ insert ‡∏ó‡∏µ‡∏•‡∏∞ record
                            for record in batch:
                                try:
                                    stmt = pg_insert(metadata).values(**record)
                                    conn.execute(stmt)
                                except Exception as single_error:
                                    print(f"‚ùå Failed to insert record with {pk_column}: {record.get(pk_column)} - {single_error}")
            
            retry_db_operation(insert_operation)
    else:
        print("‚ÑπÔ∏è No data to insert")

    # ‚úÖ Update
    if not df_diff_renamed.empty:
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NaN ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ Update
        print("üîç Checking NaN in data to update:")
        update_nan_check = df_diff_renamed.isna().sum()
        update_nan_cols = update_nan_check[update_nan_check > 0]
        if len(update_nan_cols) > 0:
            print("‚ö†Ô∏è Update data has NaN in columns:")
            for col, count in update_nan_cols.items():
                print(f"   - {col}: {count} NaN values")
        
        # ‚úÖ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢ None ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_diff_renamed_clean = df_diff_renamed.replace({np.nan: None})
        
        def update_operation():
            with target_engine.begin() as conn:
                # ‚úÖ ‡πÉ‡∏ä‡πâ batch update ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ update ‡∏ó‡∏µ‡∏•‡∏∞ record
                batch_size = 1000
                records = df_diff_renamed_clean.to_dict(orient='records')
                
                for i in range(0, len(records), batch_size):
                    batch = records[i:i + batch_size]
                    try:
                        # ‚úÖ ‡πÉ‡∏ä‡πâ executemany ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch update
                        stmt = pg_insert(metadata)
                        conn.execute(stmt, batch)
                        print(f"‚úÖ Updated batch {i//batch_size + 1}/{(len(records) + batch_size - 1)//batch_size} ({len(batch)} records)")
                    except Exception as e:
                        print(f"‚ùå Error updating batch {i//batch_size + 1}: {e}")
                        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡πÉ‡∏´‡πâ update ‡∏ó‡∏µ‡∏•‡∏∞ record
                        for record in batch:
                            try:
                                stmt = pg_insert(metadata).values(**record)
                                conn.execute(stmt)
                            except Exception as single_error:
                                print(f"‚ùå Failed to update record with {pk_column}: {record.get(pk_column)} - {single_error}")
        
        retry_db_operation(update_operation)
    else:
        print("‚ÑπÔ∏è No data to update")

    print("‚úÖ Insert/update completed.")

@job
def dim_car_etl():
    load_car_data(clean_car_data(extract_car_data()))

# if __name__ == "__main__":
#     df_raw = extract_car_data()
#     # print("‚úÖ Extracted logs:", df_raw.shape)

#     df_clean = clean_car_data((df_raw))
# #     print("‚úÖ Cleaned columns:", df_clean.columns)

#     output_path = "dim_car.xlsx"
#     df_clean.to_excel(output_path, index=False, engine='openpyxl')
#     print(f"üíæ Saved to {output_path}")

#     load_car_data(df_clean)
#     print("üéâ Test completed! Data upserted to dim_car.")


