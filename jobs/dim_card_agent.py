from dagster import op, job
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table, inspect, text, func
from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ DB source: MariaDB
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
)

# ‚úÖ DB target: PostgreSQL
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

@op
def extract_card_agent_data() -> pd.DataFrame:
    query = """
        SELECT 
            ic_ins.cuscode AS agent_id,
            ic_ins.title,
            CONCAT(ic_ins.name, ' ', ic_ins.lastname) AS agent_name,

            ic_ins.card_no AS id_card_ins,
            ic_ins.type AS type_ins,
            ic_ins.revoke_type_code AS revoke_type_ins,
            ic_ins.company AS company_ins,
            CASE 
                WHEN ic_ins.create_date IS NULL OR ic_ins.create_date = '0000-00-00' THEN NULL
                ELSE ic_ins.create_date
            END AS card_issued_date_ins,
            CASE 
                WHEN ic_ins.expire_date IS NULL OR ic_ins.expire_date = '0000-00-00' THEN NULL
                ELSE ic_ins.expire_date
            END AS card_expiry_date_ins,

            ic_life.card_no AS id_card_life,
            ic_life.type AS type_life,
            ic_life.revoke_type_code AS revoke_type_life,
            ic_life.company AS company_life,
            CASE 
                WHEN ic_life.create_date IS NULL OR ic_life.create_date = '0000-00-00' THEN NULL
                ELSE ic_life.create_date
            END AS card_issued_date_life,
            CASE 
                WHEN ic_life.expire_date IS NULL OR ic_life.expire_date = '0000-00-00' THEN NULL
                ELSE ic_life.expire_date
            END AS card_expiry_date_life

        FROM tbl_ins_card ic_ins
        LEFT JOIN tbl_ins_card ic_life
            ON ic_life.cuscode = ic_ins.cuscode AND ic_life.ins_type = 'LIFE'
        WHERE ic_ins.ins_type = 'INS'
            AND ic_ins.cuscode LIKE 'FNG%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%test%%'
            AND ic_ins.name NOT LIKE '%%‡πÄ‡∏ó‡∏™‡∏£‡∏∞‡∏ö‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%Tes ‡∏£‡∏∞‡∏ö‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î‡πà‡∏ó%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î ‡∏™‡∏≠‡∏ö%%'
    """
    df = pd.read_sql(query, source_engine)

    print("üì¶ df:", df.shape)

    return df

@op
def clean_card_agent_data(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = df.columns.str.lower()

    # Normalize ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á‡∏ß‡πà‡∏≤‡∏á/NaN
    df.replace(['NaN', 'nan', 'None'], np.nan, inplace=True)
    df = df.replace(['nan', 'NaN', 'null', ''], np.nan)
    df = df.replace(r'^\s*$', np.nan, regex=True)

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    date_columns = [c for c in df.columns if 'date' in c]
    for c in date_columns:
        df[c] = pd.to_datetime(df[c], errors='coerce').where(lambda s: s.notna(), None)

    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ï‡∏£
    for c in ['id_card_ins', 'id_card_life']:
        if c in df.columns:
            s = df[c].astype(str)
            s = s.replace(['None','nan','NaN'], None)
            s = s.str.replace(r'\D+', '', regex=True).replace('', None)
            df[c] = s

    # # --------- ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ key ‡∏ó‡∏µ‡πà normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô space/case ---------
    # # agent_id ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î space ‡∏£‡∏≠‡∏ö ‡πÜ (‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏ô df['agent_id'])
    # df['agent_id_stripped'] = df['agent_id'].astype(str).str.strip()
    # # ‡πÉ‡∏ä‡πâ lower ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    # aid_norm = df['agent_id_stripped'].str.lower()

    # df['has_defect'] = aid_norm.str.endswith('-defect').astype(int)
    # df['base_key']  = aid_norm.str.replace(r'-defect$', '', regex=True)
    # # base_id ‡∏ó‡∏µ‡πà ‚Äú‡∏™‡∏ß‡∏¢‚Äù ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÑ‡∏°‡πà lower)
    # df['base_pretty'] = df['agent_id_stripped'].str.replace(r'-defect$', '', regex=True)

    # # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    # exclude_for_score = {'agent_id', 'agent_id_stripped', 'has_defect', 'base_key', 'base_pretty'}
    # score_cols = [c for c in df.columns if c not in exclude_for_score]

    # def _is_present(x):
    #     if x is None: return False
    #     if isinstance(x, float) and pd.isna(x): return False
    #     if isinstance(x, str) and x.strip() == '': return False
    #     return True

    # df['completeness_score'] = df[score_cols].apply(lambda s: sum(_is_present(v) for v in s.values), axis=1)

    # --- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏Å‡∏é ---
    out_rows = []
    for base, g in df.groupby('base_key', sort=False):
        has_def = (g['has_defect'] == 1).any()
        has_norm = (g['has_defect'] == 0).any()

        g_sorted = g.sort_values(['completeness_score', 'has_defect'], ascending=[False, False])
        chosen = g_sorted.iloc[0].copy()

        if has_def and has_norm:
            # ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà ‚Üí ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡πâ‡∏ß "‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö" agent_id ‡πÉ‡∏´‡πâ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢ -defect
            base_pretty = chosen['base_pretty']  # ‡∏à‡∏≤‡∏Å id ‡∏ó‡∏µ‡πà strip ‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏Ñ‡∏™/‡∏ü‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ï‡πÄ‡∏î‡∏¥‡∏°)
            chosen['agent_id'] = f"{base_pretty}-defect"
        else:
            # ‡∏°‡∏µ‡∏ä‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Üí ‡πÑ‡∏°‡πà‡∏¢‡∏∏‡πà‡∏á agent_id
            # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ agent_id ‡∏°‡∏µ space ‡∏£‡∏≠‡∏ö ‡πÜ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà strip ‡πÅ‡∏•‡πâ‡∏ß
            chosen['agent_id'] = chosen['agent_id_stripped']

        out_rows.append(chosen)

    dfc = pd.DataFrame(out_rows).reset_index(drop=True)

    # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡πà‡∏ß‡∏¢
    dfc = dfc.drop(columns=['has_defect','completeness_score','base_key','base_pretty','agent_id_stripped'])

    # ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥ agent_id ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ source ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÅ‡∏ñ‡∏ß)
    dfc = dfc.drop_duplicates(subset=['agent_id'], keep='first')

    # ‡πÅ‡∏õ‡∏•‡∏á NaN/NaT ‚Üí None
    dfc = dfc.where(pd.notnull(dfc), None)

    print("\nüìä Cleaning completed (normalize for matching only; force '-defect' ONLY if both exist)")
    # Debug ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•
    sample = dfc[dfc['agent_id'].str.contains('113033', na=False)].head(5)
    if not sample.empty:
        print("üîé sample around 113033:", sample[['agent_id','agent_name']].to_dict('records'))
    return dfc

@op
def load_card_agent_data(df: pd.DataFrame):
    table_name = "dim_agent_card"
    pk_column = "agent_id"

    # 0) ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î NaN -> None
    df = df.where(pd.notnull(df), None)

    # 1) ‡∏î‡∏∂‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô/‡πÅ‡∏™‡∏ô)
    with target_engine.connect() as conn:
        df_existing = pd.read_sql(f"SELECT {pk_column} FROM {table_name}", conn)

    # 2) ‡πÅ‡∏ö‡πà‡∏á new vs common
    all_existing = set(df_existing[pk_column])
    new_ids = set(df[pk_column]) - all_existing
    common_ids = set(df[pk_column]) & all_existing

    df_to_insert = df[df[pk_column].isin(new_ids)].copy()
    df_to_upsert = df[df[pk_column].isin(common_ids)].copy()  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UPDATE

    print(f"üÜï Insert candidates: {len(df_to_insert)}")
    print(f"üîÑ Update candidates: {len(df_to_upsert)}")

    # 3) ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏°‡∏ó‡∏≤‡∏î‡∏≤‡∏ó‡πâ‡∏≤
    metadata_tbl = Table(table_name, MetaData(), autoload_with=target_engine)

    # ‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô key ‡πÅ‡∏•‡∏∞ audit/uuid)
    EXCLUDE = {pk_column, "card_ins_uuid", "create_at", "update_at"}
    updateable_cols = [c.name for c in metadata_tbl.columns if c.name not in EXCLUDE]

    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç WHERE ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö on_conflict_do_update ‚Üí ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á"
    # EXCLUDED.col IS DISTINCT FROM table.col
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏∏‡∏î ‡∏ï‡∏±‡∏î where=where_changed ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏î‡πâ
    stmt_proto = pg_insert(metadata_tbl)  # ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á excluded
    where_changed = None
    for c in updateable_cols:
        cond = getattr(stmt_proto.excluded, c).is_distinct_from(getattr(metadata_tbl.c, c))
        where_changed = cond if where_changed is None else (where_changed | cond)

    def chunk_dataframe(dfx: pd.DataFrame, size=1000):
        for i in range(0, len(dfx), size):
            yield dfx.iloc[i:i+size]

    def to_records(dfx: pd.DataFrame) -> list[dict]:
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ñ‡∏ß‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏•‡∏∞‡∏•‡πâ‡∏≤‡∏á NaN/NaT -> None
        out = []
        for _, row in dfx.iterrows():
            rec = {}
            for col, val in row.items():
                if pd.isna(val) or val == pd.NaT or val == "":
                    rec[col] = None
                else:
                    rec[col] = val
            out.append(rec)
        return out

    # 4) INSERT ‡∏Å‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏î‡πâ‡∏ß‡∏¢ ON CONFLICT DO UPDATE (blind upsert) ‚Üí ‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß
    #    ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ create_at/update_at ‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡πà DB ‡∏ù‡∏±‡πà‡∏á UPDATE; ‡∏™‡πà‡∏ß‡∏ô INSERT ‡πÄ‡∏£‡∏≤‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
    if not df_to_insert.empty:
        with target_engine.begin() as conn:
            for i, batch_df in enumerate(chunk_dataframe(df_to_insert, 5000), start=1):
                recs = to_records(batch_df)
                if not recs:
                    continue

                # ‡πÄ‡∏ï‡∏¥‡∏° audit fields ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö INSERT
                now = datetime.now()
                for r in recs:
                    r.setdefault("create_at", now)
                    r.setdefault("update_at", now)

                stmt = pg_insert(metadata_tbl).values(recs)

                set_map = {c: getattr(stmt.excluded, c) for c in updateable_cols}
                # UPDATE time ‡πÉ‡∏´‡πâ DB ‡πÉ‡∏™‡πà‡πÄ‡∏≠‡∏á (‡∏ü‡∏≤‡∏Å update)
                set_map["update_at"] = func.now()

                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=set_map,
                    where=where_changed  # ‡∏ï‡∏±‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                )
                conn.execute(stmt)
                print(f"üü¢ Upsert (insert-batch) {i}: {len(recs)} rows")

    # 5) UPDATE (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö common_ids) ‚Äî ‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ã‡πá‡∏ï create_at
    if not df_to_upsert.empty:
        with target_engine.begin() as conn:
            for i, batch_df in enumerate(chunk_dataframe(df_to_upsert, 1000), start=1):
                recs = to_records(batch_df)
                if not recs:
                    continue

                # ‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏ï‡∏¥‡∏° create_at ‡∏ï‡∏≠‡∏ô UPDATE
                for r in recs:
                    if "create_at" in r:
                        del r["create_at"]

                stmt = pg_insert(metadata_tbl).values(recs)
                set_map = {c: getattr(stmt.excluded, c) for c in updateable_cols}
                set_map["update_at"] = func.now()

                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=set_map,
                    where=where_changed
                )
                conn.execute(stmt)
                print(f"üîµ Upsert (update-batch) {i}: {len(recs)} rows")

    print("‚úÖ Insert/update completed.")

    # 6) ‡∏ï‡∏£‡∏ß‡∏à audit fields ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢
    to_verify = list(new_ids)[:3] + list(common_ids)[:2]
    if to_verify:
        ids_str = ",".join(f"'{x}'" for x in to_verify)
        q = f"SELECT {pk_column}, create_at, update_at FROM {table_name} WHERE {pk_column} IN ({ids_str}) ORDER BY update_at DESC"
        with target_engine.connect() as conn:
            vf = pd.read_sql(text(q), conn)
            print("üîç Recent records audit fields:")
            for _, row in vf.iterrows():
                print(f"   {pk_column}: {row[pk_column]}, create_at: {row['create_at']}, update_at: {row['update_at']}")

@job
def dim_card_agent_etl():
    load_card_agent_data(clean_card_agent_data(extract_card_agent_data()))

# if __name__ == "__main__":
#     df_raw = extract_card_agent_data()

#     df_clean = clean_card_agent_data((df_raw))
#     print("‚úÖ Cleaned columns:", df_clean.columns)

#     # output_path = "dim_card_agent.csv"
#     # df_clean.to_csv(output_path, index=False, encoding='utf-8-sig')
#     # print(f"üíæ Saved to {output_path}")

#     # output_path = "dim_card_agent.xlsx"
#     # df_clean.to_excel(output_path, index=False, engine='openpyxl')
#     # print(f"üíæ Saved to {output_path}")

#     load_card_agent_data(df_clean)
#     print("üéâ completed! Data upserted to dim_card_agent.")