from dagster import op, job
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table, inspect, text
from sqlalchemy.dialects.postgresql import insert as pg_insert

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ DB source: MariaDB
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
)

# ‚úÖ DB target: PostgreSQL
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

@op
def extract_card_agent_data() -> pd.DataFrame:
    query = """
        SELECT 
            ic_ins.cuscode AS agent_id,
            ic_ins.title,
            CONCAT(ic_ins.name, ' ', ic_ins.lastname) AS agent_name,

            ic_ins.card_no AS id_card_ins,
            ic_ins.type AS type_ins,
            ic_ins.revoke_type_code AS revoke_type_ins,
            ic_ins.company AS company_ins,
            CASE 
                WHEN ic_ins.create_date IS NULL OR ic_ins.create_date = '0000-00-00' THEN NULL
                ELSE ic_ins.create_date
            END AS card_issued_date_ins,
            CASE 
                WHEN ic_ins.expire_date IS NULL OR ic_ins.expire_date = '0000-00-00' THEN NULL
                ELSE ic_ins.expire_date
            END AS card_expiry_date_ins,

            ic_life.card_no AS id_card_life,
            ic_life.type AS type_life,
            ic_life.revoke_type_code AS revoke_type_life,
            ic_life.company AS company_life,
            CASE 
                WHEN ic_life.create_date IS NULL OR ic_life.create_date = '0000-00-00' THEN NULL
                ELSE ic_life.create_date
            END AS card_issued_date_life,
            CASE 
                WHEN ic_life.expire_date IS NULL OR ic_life.expire_date = '0000-00-00' THEN NULL
                ELSE ic_life.expire_date
            END AS card_expiry_date_life

        FROM tbl_ins_card ic_ins
        LEFT JOIN tbl_ins_card ic_life
            ON ic_life.cuscode = ic_ins.cuscode AND ic_life.ins_type = 'LIFE'
        WHERE ic_ins.ins_type = 'INS'
            AND ic_ins.cuscode LIKE 'FNG%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%test%%'
            AND ic_ins.name NOT LIKE '%%‡πÄ‡∏ó‡∏™‡∏£‡∏∞‡∏ö‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%Tes ‡∏£‡∏∞‡∏ö‡∏ö%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î‡πà‡∏ó%%'
            AND ic_ins.name NOT LIKE '%%‡∏ó‡∏î ‡∏™‡∏≠‡∏ö%%'
    """
    df = pd.read_sql(query, source_engine)

    print("üì¶ df:", df.shape)

    return df

@op
def clean_card_agent_data(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = df.columns.str.lower()

    # ‚úÖ Replace string 'NaN', 'nan', 'None' with actual np.nan
    df.replace(['NaN', 'nan', 'None'], np.nan, inplace=True)

    # ‚úÖ Handle date columns properly
    date_columns = [col for col in df.columns if 'date' in col.lower()]
    for col in date_columns:
        # convert to datetime and force errors to NaT
        df[col] = pd.to_datetime(df[col], errors='coerce')
        # convert NaT to None for PostgreSQL compatibility
        df[col] = df[col].where(pd.notnull(df[col]), None)

    # ‚úÖ Clean up ID card columns
    id_card_columns = ['id_card_ins', 'id_card_life']
    for col in id_card_columns:
        if col in df.columns:
            df[col] = df[col].astype(str)
            df[col] = df[col].replace(['None', 'nan', 'NaN'], None)
            df[col] = df[col].str.replace(r'\D+', '', regex=True)  # remove non-digit
            df[col] = df[col].replace('', None)

    # ‚úÖ Convert all NaN values to None for PostgreSQL compatibility
    df = df.where(pd.notnull(df), None)

    print("\nüìä Cleaning completed")
    return df

@op
def load_card_agent_data(df: pd.DataFrame):
    table_name = 'dim_card_agent'
    pk_column = 'agent_id'

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á agent_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å DataFrame ‡πÉ‡∏´‡∏°‡πà
    df = df[~df[pk_column].duplicated(keep='first')].copy()
    
    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà agent_id ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô None
    df = df[df[pk_column].notna()].copy()
    
    if df.empty:
        print("‚ö†Ô∏è No valid data to process")
        return

    # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞ agent_id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
    agent_ids = df[pk_column].tolist()
    
    if not agent_ids:
        df_existing = pd.DataFrame()
    else:
        # ‡πÅ‡∏õ‡∏•‡∏á agent_ids ‡πÄ‡∏õ‡πá‡∏ô tuple ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ SQLAlchemy ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ
        agent_ids_tuple = tuple(agent_ids)
        
        placeholders = ','.join(['%s'] * len(agent_ids))
        query_existing = f"""
            SELECT * FROM {table_name} 
            WHERE {pk_column} IN ({placeholders})
        """

        with target_engine.connect() as conn:
            df_existing = pd.read_sql(
                text(query_existing), 
                conn, 
                params=agent_ids
            )

    print(f"üìä New data: {len(df)} rows")
    print(f"üìä Existing data found: {len(df_existing)} rows")

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á agent_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
    if not df_existing.empty:
        df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()

    # ‚úÖ Identify agent_id ‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô DB)
    existing_ids = set(df_existing[pk_column]) if not df_existing.empty else set()
    new_ids = set(df[pk_column]) - existing_ids
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()

    # ‚úÖ Identify agent_id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    common_ids = set(df[pk_column]) & existing_ids
    df_common_new = df[df[pk_column].isin(common_ids)].copy()
    df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()

    # ‚úÖ ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô key ‡πÅ‡∏•‡∏∞ audit fields)
    exclude_columns = [pk_column, 'card_ins_uuid', 'create_at', 'update_at']
    compare_cols = [
        col for col in df.columns
        if col not in exclude_columns
    ]

    # ‚úÖ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Vectorized (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ apply)
    df_to_update = pd.DataFrame()
    if not df_common_new.empty and not df_common_old.empty:
        # Merge ‡∏î‡πâ‡∏ß‡∏¢ suffix (_new, _old)
        merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô merged DataFrame
        available_cols = []
        for col in compare_cols:
            if f"{col}_new" in merged.columns and f"{col}_old" in merged.columns:
                available_cols.append(col)
        
        if available_cols:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á boolean mask ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
            diff_mask = pd.Series(False, index=merged.index)
            
            for col in available_cols:
                col_new = f"{col}_new"
                col_old = f"{col}_old"
                
                # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö vectorized
                new_vals = merged[col_new]
                old_vals = merged[col_old]
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN values
                both_nan = (pd.isna(new_vals) & pd.isna(old_vals))
                different = (new_vals != old_vals) & ~both_nan
                
                diff_mask |= different
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
            df_diff = merged[diff_mask].copy()
            
            if not df_diff.empty:
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö update
                update_cols = [f"{col}_new" for col in available_cols]
                all_cols = [pk_column] + update_cols
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_diff
                existing_cols = [col for col in all_cols if col in df_diff.columns]
                
                if len(existing_cols) > 1:
                    df_to_update = df_diff[existing_cols].copy()
                    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ column ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
                    new_col_names = [pk_column] + [col.replace('_new', '') for col in existing_cols if col != pk_column]
                    df_to_update.columns = new_col_names

    print(f"üÜï Insert: {len(df_to_insert)} rows")
    print(f"üîÑ Update: {len(df_to_update)} rows")

    # ‚úÖ Load table metadata
    metadata = Table(table_name, MetaData(), autoload_with=target_engine)

    # ‚úÖ Insert (Batch operation)
    if not df_to_insert.empty:
        # ‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô records
        records = []
        for _, row in df_to_insert.iterrows():
            record = {}
            for col, value in row.items():
                if pd.isna(value) or value == pd.NaT or value == '':
                    record[col] = None
                else:
                    record[col] = value
            records.append(record)
        
        # Insert ‡πÅ‡∏ö‡∏ö batch
        with target_engine.begin() as conn:
            conn.execute(metadata.insert(), records)

    # ‚úÖ Update (Batch operation)
    if not df_to_update.empty:
        # ‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô records
        records = []
        for _, row in df_to_update.iterrows():
            record = {}
            for col, value in row.items():
                if pd.isna(value) or value == pd.NaT or value == '':
                    record[col] = None
                else:
                    record[col] = value
            records.append(record)
        
        # Update ‡πÅ‡∏ö‡∏ö batch
        with target_engine.begin() as conn:
            for record in records:
                stmt = pg_insert(metadata).values(**record)
                update_columns = {
                    c.name: stmt.excluded[c.name]
                    for c in metadata.columns
                    if c.name != pk_column
                }
                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns
                )
                conn.execute(stmt)

    print("‚úÖ Insert/update completed.")

@job
def dim_card_agent_etl():
    load_card_agent_data(clean_card_agent_data(extract_card_agent_data()))

# if __name__ == "__main__":
#     df_raw = extract_card_agent_data()

#     df_clean = clean_card_agent_data((df_raw))
#     print("‚úÖ Cleaned columns:", df_clean.columns)

#     # output_path = "dim_card_agent.csv"
#     # df_clean.to_csv(output_path, index=False, encoding='utf-8-sig')
#     # print(f"üíæ Saved to {output_path}")

#     # output_path = "dim_card_agent.xlsx"
#     # df_clean.to_excel(output_path, index=False, engine='openpyxl')
#     # print(f"üíæ Saved to {output_path}")

#     load_card_agent_data(df_clean)
#     print("üéâ completed! Data upserted to dim_card_agent.")