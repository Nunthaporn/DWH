from dagster import op, job
import pandas as pd
import numpy as np
import re
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, MetaData, Table, func, or_
from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime

# ============ ENV & ENGINES ============
load_dotenv()

source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
    f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_pre_ping=True,
)

target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@"
    f"{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance",
    connect_args={"options": "-c statement_timeout=300000"},
    pool_pre_ping=True,
)


# ============ EXTRACT ============
@op
def extract_agent_data():
    query_main = """
        SELECT
            cuscode, name, rank, user_registered, status,
            fin_new_group, fin_new_mem, type_agent, typebuy,
            user_email, name_store, address, city, district,
            province, province_cur, area_cur, postcode, tel,
            date_active, display_name, headteam, status_vip
        FROM wp_users
        WHERE (
            cuscode = 'WEB-T2R'
            OR (
                user_login NOT IN ('FINTEST-01','FIN-TestApp','adminmag_fin','FNG00-00001')
                AND name NOT LIKE '%%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%%' AND name NOT LIKE '%%test%%'
                AND cuscode NOT LIKE '%%Tester%%'
            )
        )
    """

    # ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö chunksize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î memory load
    chunks = pd.read_sql(text(query_main), source_engine, chunksize=30000)
    df_main = pd.concat(chunks, ignore_index=True)
    print(f"üì¶ df_main: {len(df_main):,}")

    # ‚úÖ career -> dict ‡πÅ‡∏•‡πâ‡∏ß map ‡πÅ‡∏ó‡∏ô merge (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å)
    df_career = pd.read_sql("SELECT cuscode, career FROM policy_register", source_engine)
    career_map = dict(zip(df_career["cuscode"], df_career["career"]))
    df_main["career"] = df_main["cuscode"].map(career_map)
    print(f"‚úÖ merged career in {len(df_main):,} rows")

    return df_main

# ============ CLEAN ============
@op
def clean_agent_data(df: pd.DataFrame):
    df = df.copy()

    # ------- normalize columns that we use -------
    for col in ["cuscode", "status", "fin_new_group", "fin_new_mem"]:
        if col not in df.columns:
            df[col] = None
    df["cuscode"] = df["cuscode"].astype(str).str.strip()

    # # ------- defect status (fix .str.strip()) -------
    status_s = df["status"].astype(str).str.strip().str.lower()

    is_defect_after = df["cuscode"].str.contains(r"-defect$", case=False, na=False) | status_s.eq("defect")
    df["defect_status"] = np.where(is_defect_after, "defect", None)

    # ------- build agent_region (join group+mem) -------
    def _join(a, b):
        a = "" if pd.isna(a) else str(a).strip()
        b = "" if pd.isna(b) else str(b).strip()
        if a and b:
            return a if a == b else f"{a} + {b}"
        return a or b

    df["agent_region"] = df.apply(lambda r: _join(r["fin_new_group"], r["fin_new_mem"]), axis=1)
    df = df[~df["agent_region"].str.contains("TEST", case=False, na=False)]

    # ------- filter TEST EXACT only (both group & mem == TEST) with whitelist -------
    whitelist = {"WEB-T2R", "WEB-T2R-DEFECT", "Admin-VIF"}
    cus_up = df["cuscode"].str.upper()
    g = df["fin_new_group"].astype(str).str.strip().str.upper()
    m = df["fin_new_mem"].astype(str).str.strip().str.upper()
    mask_test_exact = g.eq("TEST") & m.eq("TEST")
    mask_keep = cus_up.isin(whitelist)
    df = df[~(mask_test_exact & ~mask_keep)].copy()

    def clean_region(region_str: str) -> str:
        if not isinstance(region_str, str):
            return ""
        parts = [p.strip() for p in region_str.split('+')]
        cleaned = [re.sub(r"\d+$", "", p).strip() for p in parts]
        unique = []
        for c in cleaned:
            if c not in unique:
                unique.append(c)
        return " + ".join(unique)

    df["agent_main_region"] = df["agent_region"].apply(clean_region)

    df = df.drop(columns=["fin_new_group", "fin_new_mem", "display_name"], errors="ignore")

    # ------- rename columns -------
    rename_map = {
        "cuscode": "agent_id",
        "name": "agent_name",
        "rank": "agent_rank",
        "user_registered": "hire_date",
        "type_agent": "type_agent",
        "typebuy": "is_experienced",
        "user_email": "agent_email",
        "name_store": "store_name",
        "address": "agent_address",
        "city": "subdistrict",
        "district": "district",
        "province": "province",
        "province_cur": "current_province",
        "area_cur": "current_area",
        "postcode": "zipcode",
        "tel": "mobile_number",
        "career": "job",
        "agent_region": "agent_region",
    }
    df.rename(columns=rename_map, inplace=True)

    # ‚úÖ ‡∏•‡∏ö space ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    def clean_leading_spaces(text):
        if pd.isna(text) or text == '':
            return None
        text_str = str(text).strip()
        cleaned_text = re.sub(r'^\s+', '', text_str)
        return cleaned_text if cleaned_text != '' else None

    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].apply(clean_leading_spaces)

    # ------- finalize defect_status & drop old status -------
    if "status" in df.columns:
        status_s2 = df["status"].astype(str).str.strip().str.lower()
        df["defect_status"] = np.where(
            (df["defect_status"] == "defect") | status_s2.eq("defect"),
            "defect",
            None,
        )
        df.drop(columns=["status"], inplace=True)

    # ------- light cleaning (no dedup to keep counts!) -------
    # experience flip then map to yes/no
    df["is_experienced"] = df["is_experienced"].apply(
        lambda x: "‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢" if str(x).strip().lower() == "‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢" else "‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢"
    )
    df["is_experienced"] = df["is_experienced"].apply(
        lambda x: "no" if str(x).strip().lower() == "‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢" else "yes"
    )

    valid_rank = {str(i) for i in range(1, 11)}
    if "agent_rank" in df.columns:
        df.loc[~df["agent_rank"].isin(valid_rank), "agent_rank"] = np.nan

    # address trim
    if "agent_address" in df.columns:
        df["agent_address"] = df["agent_address"].apply(
            lambda addr: re.sub(r"(‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô|‡∏ã‡∏≠‡∏¢|‡∏ñ‡∏ô‡∏ô)[\s\-]*", "", str(addr)).strip()
            if pd.notna(addr) else None
        )

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î agent_address (‡∏™‡∏£‡∏∞/‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©)
    def clean_address(address):
        if pd.isna(address) or address == '':
            return None
        address_str = str(address).strip()
        cleaned_address = re.sub(r'^[\u0E30-\u0E3A\u0E47-\u0E4E]+', '', address_str)
        cleaned_address = re.sub(r'[-:.,]', '', cleaned_address)
        cleaned_address = re.sub(r'\s+', ' ', cleaned_address).strip()
        return cleaned_address

    df["agent_address"] = df["agent_address"].apply(clean_address)

    # phone digits only
    if "mobile_number" in df.columns:
        df["mobile_number"] = df["mobile_number"].astype(str).str.replace(r"[^0-9]", "", regex=True)

    # email clean
    def clean_email(email):
        if pd.isna(email) or email == "":
            return None
        s = str(email).strip()
        if re.findall(r"[‡∏Å-‡πô]", s):
            return None
        return s.lower() if re.match(r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$", s) else None

    if "agent_email" in df.columns:
        df["agent_email"] = df["agent_email"].apply(clean_email)

    test_names = [
        'ADVISORY TEST', 'ADVISORY TESTER', 'Sawitri Test K', 'Tes ‡∏£‡∏∞‡∏ö‡∏ö', 'test', 'Test', 'Test Fin', 'Test Northeast',
        'test t', 'test test', 'Test Test', 'test text', 'test tt', 'Test ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏•‡∏π‡∏Å‡∏ó‡∏µ‡∏° ‡∏ü‡∏¥‡∏•‡∏•‡∏¥‡∏õ', 'testdddddd ddddddd',
        'testsdt tstset', 'TestSMS Test', 'TestTester', 'Test‡∏™‡∏∏‡∏û‡∏à‡∏ô‡πå ‡∏ß‡∏á‡∏Ñ‡πå‡πÅ‡∏Å‡πâ‡∏ß', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏™‡∏°‡∏±‡∏Ñ‡∏£', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏®‡∏¥‡∏©‡∏è‡πå',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ü‡∏¥‡∏ô', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏°‡πá‡∏Ñ', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö PDPA',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡πà‡∏ô', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡∏ú‡πà‡∏≠‡∏ô', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡πÉ‡∏´‡∏°‡πà', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ö‡∏±‡∏ï‡∏£‡∏õ‡∏ä‡∏ä', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏´‡∏§‡∏©‡∏é‡πå', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö BCOHM', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡πÄ‡∏ä‡∏¥‡∏ç', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ä‡∏£‡πå', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏î‡∏µ', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÑ‡∏≠‡∏ó‡∏µ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏•‡∏±‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÅ‡∏ö‡∏ö‡πÅ‡∏ä‡∏£‡πå',
        '‡∏ô‡∏≤‡∏¢‡∏®‡∏¥‡∏ß‡∏Å‡∏£ ‡∏£‡∏∏‡πà‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö', '‡πÄ‡∏≠‡∏Å‡∏®‡∏¥‡∏©‡∏è‡πå ‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏ò‡∏±‡∏ô‡∏¢‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏™‡∏≠‡∏ö‡∏ó‡∏î', '‡πÅ‡∏°‡πá‡∏Ñ‡∏ó‡∏î‡∏™‡∏≠‡∏ö',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', 'jamesit ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö FinCare ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ô‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ô‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏ô‡∏∞', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á',
        '‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏öappsale', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£', '‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤ ‡πÄ‡∏Å‡∏£‡∏µ‡∏¢‡∏á‡πÑ‡∏Å‡∏£ ‡πÄ‡∏ó‡∏™‡∏£‡∏∞‡∏ö‡∏ö',
        '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤ ‡πÄ‡∏Å‡∏£‡∏µ‡∏¢‡∏á‡πÑ‡∏Å‡∏£ (‡πÄ‡∏ó‡∏™‡∏£‡∏∞‡∏ö‡∏ö)', '‡πÄ‡∏ó‡∏™‡πÜ'
    ]

    def clean_agent_name(name):
        if pd.isna(name) or name == "":
            return name
        s = str(name).strip()
        s = re.sub(r'^[‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πà‡πä‡πã‡πà‡πâ‡πä‡πã,._ _;???.]+', '', s)
        if any(test_name in s for test_name in test_names):
            return None
        return s

    if "agent_name" in df.columns:
        df["agent_name"] = df["agent_name"].apply(clean_agent_name)

    # thai-only fields
    def check_thai_text(text):
        if pd.isna(text) or text == "":
            return None
        t = re.sub(r"^[\s\]\*]+", "", str(text).strip())
        if re.match(r"^\d+$", t) or re.match(r"^[A-Za-z\s]+$", t):
            return None
        return t if re.findall(r"[‡∏Å-‡πô]", t) else None

    for col in ["subdistrict", "district", "province", "current_province", "current_area"]:
        if col in df.columns:
            df[col] = df[col].apply(check_thai_text)

    # zipcode
    if "zipcode" in df.columns:
        df["zipcode"] = df["zipcode"].apply(
            lambda z: str(z).strip() if pd.notna(z) and re.match(r"^\d{5}$", str(z).strip()) else None
        )

    # generic trim (skip date fields)
    date_cols = {"date_active"}
    for col in df.columns:
        if col in {"agent_id"} | date_cols:
            continue
        if df[col].dtype == "object":
            df[col] = df[col].apply(
                lambda x: (re.sub(r"^\s+", "", str(x).strip()) or None) if pd.notna(x) and x != "" else None
            )

    # hire_date -> Int YYYYMMDD
    if "hire_date" in df.columns:
        dt = pd.to_datetime(df["hire_date"], errors="coerce")
        df["hire_date"] = dt.dt.strftime("%Y%m%d").where(dt.notnull(), None)
        df["hire_date"] = df["hire_date"].astype("Int64")

    # date_active -> python datetime or None
    if "date_active" in df.columns:
        dt = pd.to_datetime(df["date_active"], errors="coerce")
        try:
            dt = dt.dt.tz_localize(None)
        except Exception:
            pass
        df["date_active"] = [
            (v.to_pydatetime() if isinstance(v, pd.Timestamp) and pd.notna(v)
             else (v if isinstance(v, datetime) else None))
            for v in dt
        ]

    df = df.replace(["None", "none", "nan", "NaN", "NaT", ""], np.nan)

    print("‚úÖ cleaned rows:", len(df))
    return df

# ============ LOAD ============
@op
def load_to_wh(df: pd.DataFrame):
    table_name = "dim_agent"
    pk_column = "agent_id"

    df = df.where(pd.notnull(df), None)

    with target_engine.connect() as conn:
        df_existing = pd.read_sql(f"SELECT {pk_column} FROM {table_name}", conn)

    new_ids = set(df[pk_column]) - set(df_existing[pk_column])
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()

    common_ids = set(df[pk_column]) & set(df_existing[pk_column])
    df_common_new = df[df[pk_column].isin(common_ids)].copy()

    print(f"üÜï Insert candidates: {len(df_to_insert)}")
    print(f"üîÑ Update candidates: {len(df_common_new)}")

    metadata_table = Table(table_name, MetaData(), autoload_with=target_engine)

    def chunk_dataframe(dfx, chunk_size=10000):
        for i in range(0, len(dfx), chunk_size):
            yield dfx.iloc[i:i + chunk_size]

    def sanitize_batch_remove_date_active(df_batch: pd.DataFrame) -> list[dict]:
        df_batch = df_batch.where(pd.notnull(df_batch), None)
        recs = []
        for rec in df_batch.to_dict(orient="records"):
            if "date_active" in rec:
                del rec["date_active"]
            recs.append(rec)
        return recs

    # ---------- UPSERT NEW (insert ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏à‡∏∞‡πÑ‡∏°‡πà conflict ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß) ----------
    if not df_to_insert.empty:
        with target_engine.begin() as conn:
            for batch_df in chunk_dataframe(df_to_insert):
                records = sanitize_batch_remove_date_active(batch_df.copy())
                if not records:
                    continue
                stmt = pg_insert(metadata_table).values(records)
                cols = [c.name for c in metadata_table.columns]

                # ‡πÑ‡∏°‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï pk ‡πÅ‡∏•‡∏∞ date_active ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
                updatable_cols = [c for c in cols if c not in [pk_column, "date_active"]]

                update_columns = {c: stmt.excluded[c] for c in updatable_cols}

                # ‡πÉ‡∏´‡πâ update_at = NOW() ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£ UPDATE ‡∏à‡∏≤‡∏Å conflict (‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ row ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
                if "update_at" in cols:
                    update_columns["update_at"] = func.now()

                # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á"
                # excluded.col IS DISTINCT FROM table.col (null-safe compare)
                change_exprs = [
                    stmt.excluded[c].is_distinct_from(getattr(metadata_table.c, c))
                    for c in updatable_cols if c not in ["create_at", "update_at"]
                ]
                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns,
                    where=or_(*change_exprs) if change_exprs else None
                )
                conn.execute(stmt)

    # ---------- UPSERT UPDATE (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á update + touch update_at) ----------
    if not df_common_new.empty:
        with target_engine.begin() as conn:
            for batch_df in chunk_dataframe(df_common_new):
                records = sanitize_batch_remove_date_active(batch_df.copy())
                if not records:
                    continue
                stmt = pg_insert(metadata_table).values(records)
                cols = [c.name for c in metadata_table.columns]

                # ‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ override ‡∏ï‡∏£‡∏á ‡πÜ
                updatable_cols = [
                    c for c in cols
                    if c not in [pk_column, "id_contact", "create_at", "update_at", "date_active"]
                ]

                update_columns = {c: stmt.excluded[c] for c in updatable_cols}

                # ‡πÉ‡∏´‡πâ update_at = NOW() ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£ UPDATE (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ where ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á)
                if "update_at" in cols:
                    update_columns["update_at"] = func.now()

                # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏£‡∏¥‡∏á ‡πÜ
                change_exprs = [
                    stmt.excluded[c].is_distinct_from(getattr(metadata_table.c, c))
                    for c in updatable_cols
                ]

                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns,
                    where=or_(*change_exprs) if change_exprs else None
                )
                conn.execute(stmt)

    print("‚úÖ Insert/update completed (date_active skipped in this step).")


# ============ BACKFILL ============

@op
def backfill_date_active(df: pd.DataFrame):
    table_name = "dim_agent"
    pk = "agent_id"
    if "date_active" not in df.columns:
        print("‚ö†Ô∏è Input df has no date_active column ‚Äî skip backfill")
        return

    s = pd.to_datetime(df["date_active"], errors="coerce")
    try:
        s = s.dt.tz_localize(None)
    except Exception:
        pass

    df_dates = pd.DataFrame({pk: df[pk].astype(str).str.strip(), "date_active": s})
    df_dates = df_dates[df_dates[pk].astype(bool)].copy()

    # keep one per pk (prefer not-null most recent)
    df_dates["__rank"] = df_dates["date_active"].notna().astype(int)
    df_dates = df_dates.sort_values([pk, "__rank", "date_active"], ascending=[True, False, False])
    df_dates = df_dates.drop(columns="__rank").drop_duplicates(subset=[pk], keep="first")

    def _coerce_py_datetime(v):
        try:
            if pd.isna(v): return None
        except Exception:
            pass
        if isinstance(v, pd.Timestamp):
            return v.to_pydatetime()
        if isinstance(v, np.datetime64):
            try:
                return pd.Timestamp(v).to_pydatetime()
            except Exception:
                return None
        if isinstance(v, str):
            if v.strip().lower() == "nat":
                return None
            try:
                return pd.Timestamp(v).to_pydatetime()
            except Exception:
                return None
        if isinstance(v, datetime):
            return v
        return None

    df_dates["date_active"] = df_dates["date_active"].apply(_coerce_py_datetime)
    df_dates = df_dates.replace({pd.NaT: None, "NaT": None})

    metadata_table = Table(table_name, MetaData(), autoload_with=target_engine)

    def chunk(dfx, n=10000):
        for i in range(0, len(dfx), n):
            yield dfx.iloc[i:i+n]

    total = 0
    with target_engine.begin() as conn:
        for b in chunk(df_dates[[pk, "date_active"]]):
            records = []
            for rec in b.to_dict(orient="records"):
                rec["date_active"] = _coerce_py_datetime(rec.get("date_active"))
                records.append(rec)
            if not records:
                continue

            stmt = pg_insert(metadata_table).values(records)
            cols = [c.name for c in metadata_table.columns]

            set_map = {"date_active": stmt.excluded["date_active"]}
            if "update_at" in cols:
                # touch update_at ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà date_active ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á
                set_map["update_at"] = func.now()

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ date_active ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            where_change = stmt.excluded["date_active"].is_distinct_from(metadata_table.c.date_active)

            stmt = stmt.on_conflict_do_update(
                index_elements=[pk],
                set_=set_map,
                where=where_change
            )
            conn.execute(stmt)
            total += len(records)

    print(f"‚úÖ Backfilled date_active for {total} agents (update_at touched only when changed)")

# ============ WRAPPER ============
@op
def clean_null_values_op(df: pd.DataFrame) -> pd.DataFrame:
    return df.replace(["None", "none", "nan", "NaN", "NaT", ""], np.nan)

@job
def dim_agent_etl():
    df_raw = extract_agent_data()
    df_clean = clean_agent_data(clean_null_values_op(df_raw))
    load_to_wh(df_clean)
    backfill_date_active(df_clean)

if __name__ == "__main__":
    df_raw = extract_agent_data()
    print("‚úÖ Extracted logs:", df_raw.shape)

    df_clean = clean_agent_data(df_raw)
    print("‚úÖ Cleaned rows:", len(df_clean))

    df_clean = clean_null_values_op(df_clean)

    # df_clean.to_excel("dim_agent1.xlsx", index=False)
    # print("üíæ Saved to dim_agent.xlsx")

    load_to_wh(df_clean)
    backfill_date_active(df_clean)
    print("üéâ completed!")


