from dagster import op, job
import pandas as pd
import numpy as np
import re
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, MetaData, Table, func, or_
from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime

# ============ ENV & ENGINES ============
load_dotenv()

source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
    f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_pre_ping=True,
)

target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@"
    f"{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance",
    connect_args={"options": "-c statement_timeout=300000"},
    pool_pre_ping=True,
)


# ============ EXTRACT ============
@op
def extract_agent_data():
    query_main = """
        SELECT
            cuscode, name, rank, user_registered, status,
            fin_new_group, fin_new_mem, type_agent, typebuy,
            user_email, name_store, address, city, district,
            province, province_cur, area_cur, postcode, tel,
            date_active, display_name, headteam, status_vip
        FROM wp_users
        WHERE (
            cuscode = 'WEB-T2R'
            OR (
                user_login NOT IN ('FINTEST-01','FIN-TestApp','adminmag_fin','FNG00-00001')
                AND name NOT LIKE '%%à¸—à¸”à¸ªà¸­à¸š%%' AND name NOT LIKE '%%test%%'
                AND cuscode NOT LIKE '%%Tester%%'
            )
        )
    """

    # âœ… à¸­à¹ˆà¸²à¸™à¹à¸šà¸š chunksize à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸” memory load
    chunks = pd.read_sql(text(query_main), source_engine, chunksize=30000)
    df_main = pd.concat(chunks, ignore_index=True)
    print(f"ðŸ“¦ df_main: {len(df_main):,}")

    # âœ… career -> dict à¹à¸¥à¹‰à¸§ map à¹à¸—à¸™ merge (à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¸)
    df_career = pd.read_sql("SELECT cuscode, career FROM policy_register", source_engine)
    career_map = dict(zip(df_career["cuscode"], df_career["career"]))
    df_main["career"] = df_main["cuscode"].map(career_map)
    print(f"âœ… merged career in {len(df_main):,} rows")

    return df_main

# ============ CLEAN ============
@op
def clean_agent_data(df: pd.DataFrame):
    df = df.copy()

    # ------- normalize columns that we use -------
    for col in ["cuscode", "status", "fin_new_group", "fin_new_mem"]:
        if col not in df.columns:
            df[col] = None
    df["cuscode"] = df["cuscode"].astype(str).str.strip()

    # # ------- defect status (fix .str.strip()) -------
    status_s = df["status"].astype(str).str.strip().str.lower()

    is_defect_after = df["cuscode"].str.contains(r"-defect$", case=False, na=False) | status_s.eq("defect")
    df["defect_status"] = np.where(is_defect_after, "defect", None)

    # ------- build agent_region (join group+mem) -------
    def _join(a, b):
        a = "" if pd.isna(a) else str(a).strip()
        b = "" if pd.isna(b) else str(b).strip()
        if a and b:
            return a if a == b else f"{a} + {b}"
        return a or b

    df["agent_region"] = df.apply(lambda r: _join(r["fin_new_group"], r["fin_new_mem"]), axis=1)
    df = df[~df["agent_region"].str.contains("TEST", case=False, na=False)]

    # ------- filter TEST EXACT only (both group & mem == TEST) with whitelist -------
    whitelist = {"WEB-T2R", "WEB-T2R-DEFECT", "Admin-VIF"}
    cus_up = df["cuscode"].str.upper()
    g = df["fin_new_group"].astype(str).str.strip().str.upper()
    m = df["fin_new_mem"].astype(str).str.strip().str.upper()
    mask_test_exact = g.eq("TEST") & m.eq("TEST")
    mask_keep = cus_up.isin(whitelist)
    df = df[~(mask_test_exact & ~mask_keep)].copy()

    def clean_region(region_str: str) -> str:
        if not isinstance(region_str, str):
            return ""
        parts = [p.strip() for p in region_str.split('+')]
        cleaned = [re.sub(r"\d+$", "", p).strip() for p in parts]
        unique = []
        for c in cleaned:
            if c not in unique:
                unique.append(c)
        return " + ".join(unique)

    df["agent_main_region"] = df["agent_region"].apply(clean_region)

    df = df.drop(columns=["fin_new_group", "fin_new_mem", "display_name"], errors="ignore")

    # ------- rename columns -------
    rename_map = {
        "cuscode": "agent_id",
        "name": "agent_name",
        "rank": "agent_rank",
        "user_registered": "hire_date",
        "type_agent": "type_agent",
        "typebuy": "is_experienced",
        "user_email": "agent_email",
        "name_store": "store_name",
        "address": "agent_address",
        "city": "subdistrict",
        "district": "district",
        "province": "province",
        "province_cur": "current_province",
        "area_cur": "current_area",
        "postcode": "zipcode",
        "tel": "mobile_number",
        "career": "job",
        "agent_region": "agent_region",
    }
    df.rename(columns=rename_map, inplace=True)

    # âœ… à¸¥à¸š space à¸”à¹‰à¸²à¸™à¸«à¸™à¹‰à¸²à¹ƒà¸™à¸—à¸¸à¸à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
    def clean_leading_spaces(text):
        if pd.isna(text) or text == '':
            return None
        text_str = str(text).strip()
        cleaned_text = re.sub(r'^\s+', '', text_str)
        return cleaned_text if cleaned_text != '' else None

    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].apply(clean_leading_spaces)

    # ------- finalize defect_status & drop old status -------
    if "status" in df.columns:
        status_s2 = df["status"].astype(str).str.strip().str.lower()
        df["defect_status"] = np.where(
            (df["defect_status"] == "defect") | status_s2.eq("defect"),
            "defect",
            None,
        )
        df.drop(columns=["status"], inplace=True)

    # ------- light cleaning (no dedup to keep counts!) -------
    # experience flip then map to yes/no
    df["is_experienced"] = df["is_experienced"].apply(
        lambda x: "à¹€à¸„à¸¢à¸‚à¸²à¸¢" if str(x).strip().lower() == "à¹„à¸¡à¹ˆà¹€à¸„à¸¢à¸‚à¸²à¸¢" else "à¹„à¸¡à¹ˆà¹€à¸„à¸¢à¸‚à¸²à¸¢"
    )
    df["is_experienced"] = df["is_experienced"].apply(
        lambda x: "no" if str(x).strip().lower() == "à¹„à¸¡à¹ˆà¹€à¸„à¸¢à¸‚à¸²à¸¢" else "yes"
    )

    valid_rank = {str(i) for i in range(1, 11)}
    if "agent_rank" in df.columns:
        df.loc[~df["agent_rank"].isin(valid_rank), "agent_rank"] = np.nan

    # address trim
    if "agent_address" in df.columns:
        df["agent_address"] = df["agent_address"].apply(
            lambda addr: re.sub(r"(à¹€à¸¥à¸‚à¸—à¸µà¹ˆ|à¸«à¸¡à¸¹à¹ˆà¸—à¸µà¹ˆ|à¸«à¸¡à¸¹à¹ˆà¸šà¹‰à¸²à¸™|à¸‹à¸­à¸¢|à¸–à¸™à¸™)[\s\-]*", "", str(addr)).strip()
            if pd.notna(addr) else None
        )

    # âœ… à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” agent_address (à¸ªà¸£à¸°/à¸­à¸±à¸à¸‚à¸£à¸°à¸žà¸´à¹€à¸¨à¸©)
    def clean_address(address):
        if pd.isna(address) or address == '':
            return None
        address_str = str(address).strip()
        cleaned_address = re.sub(r'^[\u0E30-\u0E3A\u0E47-\u0E4E]+', '', address_str)
        cleaned_address = re.sub(r'[-:.,]', '', cleaned_address)
        cleaned_address = re.sub(r'\s+', ' ', cleaned_address).strip()
        return cleaned_address

    df["agent_address"] = df["agent_address"].apply(clean_address)

    # phone digits only
    if "mobile_number" in df.columns:
        df["mobile_number"] = df["mobile_number"].astype(str).str.replace(r"[^0-9]", "", regex=True)

    # email clean
    def clean_email(email):
        if pd.isna(email) or email == "":
            return None
        s = str(email).strip()
        if re.findall(r"[à¸-à¹™]", s):
            return None
        return s.lower() if re.match(r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$", s) else None

    if "agent_email" in df.columns:
        df["agent_email"] = df["agent_email"].apply(clean_email)

    test_names = [
        'ADVISORY TEST', 'ADVISORY TESTER', 'Sawitri Test K', 'Tes à¸£à¸°à¸šà¸š', 'test', 'Test', 'Test Fin', 'Test Northeast',
        'test t', 'test test', 'Test Test', 'test text', 'test tt', 'Test à¸ªà¸¡à¸±à¸„à¸£à¸¥à¸¹à¸à¸—à¸µà¸¡ à¸Ÿà¸´à¸¥à¸¥à¸´à¸›', 'testdddddd ddddddd',
        'testsdt tstset', 'TestSMS Test', 'TestTester', 'Testà¸ªà¸¸à¸žà¸ˆà¸™à¹Œ à¸§à¸‡à¸„à¹Œà¹à¸à¹‰à¸§', 'à¸—à¸”à¸ªà¸­à¸šà¸§à¸±à¸™à¸—à¸µà¹ˆ', 'à¸—à¸”à¸ªà¸­à¸š à¸ªà¸¡à¸±à¸„à¸£', 'à¸—à¸”à¸ªà¸­à¸šà¹€à¸­à¸à¸¨à¸´à¸©à¸à¹Œ',
        'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸šà¸Ÿà¸´à¸™', 'à¸—à¸”à¸ªà¸­à¸š à¸—à¸”à¸ªà¸­à¸š', 'à¸—à¸”à¸ªà¸­à¸šà¸ªà¸¡à¸±à¸„à¸£', 'à¸—à¸”à¸ªà¸­à¸šà¹€à¸¥à¸‚à¸šà¸±à¸à¸Šà¸µ', 'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š', 'à¸—à¸”à¸ªà¸­à¸šà¹à¸¡à¹‡à¸„', 'à¸—à¸”à¸ªà¸­à¸š PDPA',
        'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š à¹à¸­à¸›à¸žà¸¥à¸´à¹€à¸„à¸Šà¸±à¹ˆà¸™', 'à¸—à¸”à¸ªà¸­à¸š à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸', 'à¸—à¸”à¸ªà¸­à¸š à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸¡à¹ˆà¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸à¸œà¹ˆà¸­à¸™', 'à¸—à¸”à¸ªà¸­à¸š à¸¥à¸¹à¸à¸„à¹‰à¸²à¸«à¸¡à¸”à¸­à¸²à¸¢à¸¸',
        'à¸—à¸”à¸ªà¸­à¸š à¸ªà¸¡à¸²à¸Šà¸´à¸à¹ƒà¸«à¸¡à¹ˆ', 'à¸—à¸”à¸ªà¸­à¸š à¸šà¸±à¸•à¸£à¸›à¸Šà¸Š', 'à¸—à¸”à¸ªà¸­à¸š à¸«à¸¤à¸©à¸Žà¹Œ', 'à¸—à¸”à¸ªà¸­à¸š à¸—à¸”à¸ªà¸­à¸š', 'à¸—à¸”à¸ªà¸­à¸š BCOHM', 'à¸—à¸”à¸ªà¸­à¸š à¹à¸šà¸šà¸—à¸”à¸ªà¸­à¸š',
        'à¸—à¸”à¸ªà¸­à¸š à¹€à¸Šà¸´à¸', 'à¸—à¸”à¸ªà¸­à¸šà¹à¸Šà¸£à¹Œ', 'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š à¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥à¸”à¸µ', 'à¸—à¸”à¸ªà¸­à¸šà¸ˆà¸²à¸à¹„à¸­à¸—à¸µ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸•à¸´à¸”à¸•à¹ˆà¸­à¸à¸¥à¸±à¸š', 'à¸—à¸”à¸ªà¸­à¸šà¸ªà¸¡à¸±à¸„à¸£à¹à¸šà¸šà¹à¸Šà¸£à¹Œ',
        'à¸™à¸²à¸¢à¸¨à¸´à¸§à¸à¸£ à¸£à¸¸à¹ˆà¸‡à¹€à¸£à¸·à¸­à¸‡à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š', 'à¹€à¸­à¸à¸¨à¸´à¸©à¸à¹Œ à¹€à¸ˆà¸£à¸´à¸à¸˜à¸±à¸™à¸¢à¸šà¸¹à¸£à¸“à¹Œ à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š', 'à¸—à¸”à¸ªà¸­à¸š à¸£à¸°à¸šà¸š', 'à¸—à¸”à¸ªà¸­à¸š à¸ªà¸­à¸šà¸—à¸”', 'à¹à¸¡à¹‡à¸„à¸—à¸”à¸ªà¸­à¸š',
        'à¸—à¸”à¸ªà¸­à¸šà¸ªà¸¡à¸±à¸„à¸£ à¸—à¸”à¸ªà¸­à¸š', 'jamesit à¸—à¸”à¸ªà¸­à¸š', 'à¸—à¸”à¸ªà¸­à¸šà¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡ à¸—à¸”à¸ªà¸­à¸šà¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡', 'à¸—à¸”à¸ªà¸­à¸š FinCare à¸—à¸”à¸ªà¸­à¸š', 'à¸—à¸”à¸ªà¸­à¸š à¸™à¸²à¸¢à¸«à¸™à¹‰à¸²à¸¡à¸µà¸œà¸¹à¹‰à¹à¸™à¸°à¸™à¸³',
        'à¸—à¸”à¸ªà¸­à¸š à¸™à¸²à¸¢à¸«à¸™à¹‰à¸²à¹€à¸à¹ˆà¸²à¸¡à¸µà¸œà¸¹à¹‰à¹à¸™à¸°à¸™à¸³', 'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š', 'à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š à¸ˆà¸£à¸´à¸‡à¸ˆà¸£à¸´à¸‡à¸™à¸°', 'à¸—à¸”à¸ªà¸­à¸š à¸£à¸°à¸šà¸šà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡',
        'à¸—à¸”à¸ªà¸­à¸šà¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡ à¸—à¸”à¸ªà¸­à¸šà¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡', 'à¸—à¸”à¸ªà¸­à¸š à¸—à¸”à¸ªà¸­à¸šappsale', 'à¸—à¸”à¸ªà¸­à¸š à¸—à¸”à¸ªà¸­à¸šà¸ªà¸¡à¸±à¸„à¸£', 'à¸—à¸”à¸ªà¸­à¸š à¸—à¸”à¸ªà¸­à¸š', 'à¸›à¸±à¸à¸à¸² à¹€à¸à¸£à¸µà¸¢à¸‡à¹„à¸à¸£ à¹€à¸—à¸ªà¸£à¸°à¸šà¸š',
        'à¸›à¸±à¸à¸à¸² à¹€à¸à¸£à¸µà¸¢à¸‡à¹„à¸à¸£ (à¹€à¸—à¸ªà¸£à¸°à¸šà¸š)', 'à¹€à¸—à¸ªà¹†'
    ]

    def clean_agent_name(name):
        if pd.isna(name) or name == "":
            return name
        s = str(name).strip()
        s = re.sub(r'^[à¸´à¸µà¸¶à¸·à¸¸à¸¹à¹ˆà¹Šà¹‹à¹ˆà¹‰à¹Šà¹‹,._ _;???.]+', '', s)
        if any(test_name in s for test_name in test_names):
            return None
        return s

    if "agent_name" in df.columns:
        df["agent_name"] = df["agent_name"].apply(clean_agent_name)

    # thai-only fields
    def check_thai_text(text):
        if pd.isna(text) or text == "":
            return None
        t = re.sub(r"^[\s\]\*]+", "", str(text).strip())
        if re.match(r"^\d+$", t) or re.match(r"^[A-Za-z\s]+$", t):
            return None
        return t if re.findall(r"[à¸-à¹™]", t) else None

    for col in ["subdistrict", "district", "province", "current_province", "current_area"]:
        if col in df.columns:
            df[col] = df[col].apply(check_thai_text)

    # zipcode
    if "zipcode" in df.columns:
        df["zipcode"] = df["zipcode"].apply(
            lambda z: str(z).strip() if pd.notna(z) and re.match(r"^\d{5}$", str(z).strip()) else None
        )

    # generic trim (skip date fields)
    date_cols = {"date_active"}
    for col in df.columns:
        if col in {"agent_id"} | date_cols:
            continue
        if df[col].dtype == "object":
            df[col] = df[col].apply(
                lambda x: (re.sub(r"^\s+", "", str(x).strip()) or None) if pd.notna(x) and x != "" else None
            )

    # hire_date -> Int YYYYMMDD
    if "hire_date" in df.columns:
        dt = pd.to_datetime(df["hire_date"], errors="coerce")
        df["hire_date"] = dt.dt.strftime("%Y%m%d").where(dt.notnull(), None)
        df["hire_date"] = df["hire_date"].astype("Int64")

    # date_active -> python datetime or None
    if "date_active" in df.columns:
        dt = pd.to_datetime(df["date_active"], errors="coerce")
        try:
            dt = dt.dt.tz_localize(None)
        except Exception:
            pass
        df["date_active"] = [
            (v.to_pydatetime() if isinstance(v, pd.Timestamp) and pd.notna(v)
             else (v if isinstance(v, datetime) else None))
            for v in dt
        ]

    df = df.replace(["None", "none", "nan", "NaN", "NaT", ""], np.nan)

    print("âœ… cleaned rows:", len(df))
    return df

# ============ LOAD ============
@op
def load_to_wh(df: pd.DataFrame):
    table_name = "dim_agent"
    pk_column = "agent_id"

    df = df.where(pd.notnull(df), None)

    with target_engine.connect() as conn:
        df_existing = pd.read_sql(f"SELECT {pk_column} FROM {table_name}", conn)

    new_ids = set(df[pk_column]) - set(df_existing[pk_column])
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()

    common_ids = set(df[pk_column]) & set(df_existing[pk_column])
    df_common_new = df[df[pk_column].isin(common_ids)].copy()

    print(f"ðŸ†• Insert candidates: {len(df_to_insert)}")
    print(f"ðŸ”„ Update candidates: {len(df_common_new)}")

    metadata_table = Table(table_name, MetaData(), autoload_with=target_engine)

    def chunk_dataframe(dfx, chunk_size=10000):
        for i in range(0, len(dfx), chunk_size):
            yield dfx.iloc[i:i + chunk_size]

    def sanitize_batch_remove_date_active(df_batch: pd.DataFrame) -> list[dict]:
        df_batch = df_batch.where(pd.notnull(df_batch), None)
        recs = []
        for rec in df_batch.to_dict(orient="records"):
            if "date_active" in rec:
                del rec["date_active"]
            recs.append(rec)
        return recs

    # ---------- UPSERT NEW (insert à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¸ˆà¸£à¸´à¸‡ à¹† à¸ˆà¸°à¹„à¸¡à¹ˆ conflict à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§) ----------
    if not df_to_insert.empty:
        with target_engine.begin() as conn:
            for batch_df in chunk_dataframe(df_to_insert):
                records = sanitize_batch_remove_date_active(batch_df.copy())
                if not records:
                    continue
                stmt = pg_insert(metadata_table).values(records)
                cols = [c.name for c in metadata_table.columns]

                # à¹„à¸¡à¹ˆà¸­à¸±à¸›à¹€à¸”à¸• pk à¹à¸¥à¸° date_active à¹ƒà¸™à¸£à¸­à¸šà¸™à¸µà¹‰
                updatable_cols = [c for c in cols if c not in [pk_column, "date_active"]]

                update_columns = {c: stmt.excluded[c] for c in updatable_cols}

                # à¹ƒà¸«à¹‰ update_at = NOW() à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸à¸´à¸”à¸à¸²à¸£ UPDATE à¸ˆà¸²à¸ conflict (à¸à¸£à¸“à¸µà¸¡à¸µ row à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§)
                if "update_at" in cols:
                    update_columns["update_at"] = func.now()

                # à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚: à¸­à¸±à¸›à¹€à¸”à¸•à¸à¹‡à¸•à¹ˆà¸­à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹„à¸«à¸™ "à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸£à¸´à¸‡"
                # excluded.col IS DISTINCT FROM table.col (null-safe compare)
                change_exprs = [
                    stmt.excluded[c].is_distinct_from(getattr(metadata_table.c, c))
                    for c in updatable_cols if c not in ["create_at", "update_at"]
                ]
                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns,
                    where=or_(*change_exprs) if change_exprs else None
                )
                conn.execute(stmt)

    # ---------- UPSERT UPDATE (à¹€à¸‰à¸žà¸²à¸° id à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆ: à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸ˆà¸£à¸´à¸‡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¸ˆà¸¶à¸‡ update + touch update_at) ----------
    if not df_common_new.empty:
        with target_engine.begin() as conn:
            for batch_df in chunk_dataframe(df_common_new):
                records = sanitize_batch_remove_date_active(batch_df.copy())
                if not records:
                    continue
                stmt = pg_insert(metadata_table).values(records)
                cols = [c.name for c in metadata_table.columns]

                # à¸à¸±à¸™à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸­à¸¢à¸²à¸à¹ƒà¸«à¹‰ override à¸•à¸£à¸‡ à¹†
                updatable_cols = [
                    c for c in cols
                    if c not in [pk_column, "id_contact", "create_at", "update_at", "date_active"]
                ]

                update_columns = {c: stmt.excluded[c] for c in updatable_cols}

                # à¹ƒà¸«à¹‰ update_at = NOW() à¹€à¸‰à¸žà¸²à¸°à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸à¸´à¸”à¸à¸²à¸£ UPDATE (à¸‹à¸¶à¹ˆà¸‡à¸¡à¸µ where à¸•à¸£à¸§à¸ˆà¸„à¸§à¸²à¸¡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡)
                if "update_at" in cols:
                    update_columns["update_at"] = func.now()

                # à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸‰à¸žà¸²à¸°à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µà¸„à¹ˆà¸²à¸•à¹ˆà¸²à¸‡à¸ˆà¸²à¸à¹€à¸”à¸´à¸¡à¸ˆà¸£à¸´à¸‡ à¹†
                change_exprs = [
                    stmt.excluded[c].is_distinct_from(getattr(metadata_table.c, c))
                    for c in updatable_cols
                ]

                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns,
                    where=or_(*change_exprs) if change_exprs else None
                )
                conn.execute(stmt)

    print("âœ… Insert/update completed (date_active skipped in this step).")


# ============ BACKFILL ============

@op
def backfill_date_active(df: pd.DataFrame):
    table_name = "dim_agent"
    pk = "agent_id"
    if "date_active" not in df.columns:
        print("âš ï¸ Input df has no date_active column â€” skip backfill")
        return

    s = pd.to_datetime(df["date_active"], errors="coerce")
    try:
        s = s.dt.tz_localize(None)
    except Exception:
        pass

    df_dates = pd.DataFrame({pk: df[pk].astype(str).str.strip(), "date_active": s})
    df_dates = df_dates[df_dates[pk].astype(bool)].copy()

    # keep one per pk (prefer not-null most recent)
    df_dates["__rank"] = df_dates["date_active"].notna().astype(int)
    df_dates = df_dates.sort_values([pk, "__rank", "date_active"], ascending=[True, False, False])
    df_dates = df_dates.drop(columns="__rank").drop_duplicates(subset=[pk], keep="first")

    def _coerce_py_datetime(v):
        try:
            if pd.isna(v): return None
        except Exception:
            pass
        if isinstance(v, pd.Timestamp):
            return v.to_pydatetime()
        if isinstance(v, np.datetime64):
            try:
                return pd.Timestamp(v).to_pydatetime()
            except Exception:
                return None
        if isinstance(v, str):
            if v.strip().lower() == "nat":
                return None
            try:
                return pd.Timestamp(v).to_pydatetime()
            except Exception:
                return None
        if isinstance(v, datetime):
            return v
        return None

    df_dates["date_active"] = df_dates["date_active"].apply(_coerce_py_datetime)
    df_dates = df_dates.replace({pd.NaT: None, "NaT": None})

    metadata_table = Table(table_name, MetaData(), autoload_with=target_engine)

    def chunk(dfx, n=10000):
        for i in range(0, len(dfx), n):
            yield dfx.iloc[i:i+n]

    total = 0
    with target_engine.begin() as conn:
        for b in chunk(df_dates[[pk, "date_active"]]):
            records = []
            for rec in b.to_dict(orient="records"):
                rec["date_active"] = _coerce_py_datetime(rec.get("date_active"))
                records.append(rec)
            if not records:
                continue

            stmt = pg_insert(metadata_table).values(records)
            cols = [c.name for c in metadata_table.columns]

            set_map = {"date_active": stmt.excluded["date_active"]}
            if "update_at" in cols:
                # touch update_at à¹€à¸‰à¸žà¸²à¸°à¸•à¸­à¸™à¸—à¸µà¹ˆ date_active à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸£à¸´à¸‡
                set_map["update_at"] = func.now()

            # à¸­à¸±à¸›à¹€à¸”à¸•à¸à¹‡à¸•à¹ˆà¸­à¹€à¸¡à¸·à¹ˆà¸­ date_active à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸£à¸´à¸‡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
            where_change = stmt.excluded["date_active"].is_distinct_from(metadata_table.c.date_active)

            stmt = stmt.on_conflict_do_update(
                index_elements=[pk],
                set_=set_map,
                where=where_change
            )
            conn.execute(stmt)
            total += len(records)

    print(f"âœ… Backfilled date_active for {total} agents (update_at touched only when changed)")

# ============ WRAPPER ============
@op
def clean_null_values_op(df: pd.DataFrame) -> pd.DataFrame:
    return df.replace(["None", "none", "nan", "NaN", "NaT", ""], np.nan)

@job
def dim_agent_etl():
    df_raw = extract_agent_data()
    df_clean = clean_agent_data(clean_null_values_op(df_raw))
    load_to_wh(df_clean)
    backfill_date_active(df_clean)

if __name__ == "__main__":
    df_raw = extract_agent_data()
    print("âœ… Extracted logs:", df_raw.shape)

    df_clean = clean_agent_data(df_raw)
    print("âœ… Cleaned rows:", len(df_clean))

    df_clean = clean_null_values_op(df_clean)

    # df_clean.to_excel("dim_agent1.xlsx", index=False)
    # print("ðŸ’¾ Saved to dim_agent.xlsx")

    load_to_wh(df_clean)
    backfill_date_active(df_clean)
    print("ðŸŽ‰ completed!")


