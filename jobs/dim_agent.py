from dagster import op, job
import pandas as pd
import numpy as np
import re
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, inspect
from sqlalchemy import create_engine, MetaData, Table, inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î .env
load_dotenv()

# ‚úÖ DB source (MariaDB)
source_user = os.getenv('DB_USER')
source_password = os.getenv('DB_PASSWORD')
source_host = os.getenv('DB_HOST')
source_port = os.getenv('DB_PORT')
source_db = 'fininsurance'

source_engine = create_engine(
    f"mysql+pymysql://{source_user}:{source_password}@{source_host}:{source_port}/{source_db}"
)

# ‚úÖ DB target (PostgreSQL)
target_user = os.getenv('DB_USER_test')
target_password = os.getenv('DB_PASSWORD_test')
target_host = os.getenv('DB_HOST_test')
target_port = os.getenv('DB_PORT_test')
target_db = 'fininsurance'

target_engine = create_engine(
    f"postgresql+psycopg2://{target_user}:{target_password}@{target_host}:{target_port}/{target_db}"
)

@op
def extract_agent_data():
    query_main = r"""
    SELECT cuscode, name, rank,
           CASE 
           WHEN user_registered 
             ELSE user_registered 
           END AS user_registered,
           status, fin_new_group, fin_new_mem,
           type_agent, typebuy, user_email, name_store, address, city, district,
           province, province_cur, area_cur, postcode, tel,date_active,card_ins_type,file_card_ins,
           card_ins_type_life,file_card_ins_life
    FROM wp_users 
    WHERE user_login NOT IN ('FINTEST-01', 'FIN-TestApp', 'Admin-VIF', 'adminmag_fin', 'FNG00-00001')
      AND name NOT LIKE '%%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%%'
      AND name NOT LIKE '%%tes%%'
      AND cuscode NOT LIKE 'bkk%%'
      AND cuscode NOT LIKE '%%east%%'
      AND cuscode NOT LIKE 'north%%'
      AND cuscode NOT LIKE 'central%%'
      AND cuscode NOT LIKE 'upc%%'
      AND cuscode NOT LIKE 'sqc_%%'
      AND cuscode NOT LIKE 'pm_%%'
      AND cuscode NOT LIKE 'Sale-Tor%%'
      AND cuscode NOT LIKE 'online%%'
      AND cuscode NOT LIKE 'Sale-Direct%%'
    """


    df_main = pd.read_sql(query_main, source_engine)

    query_career = "SELECT cuscode, career FROM policy_register"
    df_career = pd.read_sql(query_career, source_engine)

    df_merged = pd.merge(df_main, df_career, on='cuscode', how='left')

    print("üì¶ df_main:", df_main.shape)
    print("üì¶ df_career:", df_career.shape)
    print("üì¶ df_merged:", df_merged.shape)

    return df_merged

@op
def clean_agent_data(df: pd.DataFrame):
    # Combine region columns
    def combine_columns(a, b):
        a_str = str(a).strip() if pd.notna(a) else ''
        b_str = str(b).strip() if pd.notna(b) else ''
        if a_str == '' and b_str == '':
            return ''
        elif a_str == '':
            return b_str
        elif b_str == '':
            return a_str
        elif a_str == b_str:
            return a_str
        else:
            return f"{a_str} + {b_str}"

    df['agent_region'] = df.apply(lambda row: combine_columns(row['fin_new_group'], row['fin_new_mem']), axis=1)
        # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á row ‡∏ó‡∏µ‡πà agent_region = 'TEST' ‡∏≠‡∏≠‡∏Å
    df = df[df['agent_region'] != 'TEST']
    df = df.drop(columns=['fin_new_group', 'fin_new_mem'])

    # Clean date_active and status_agent
    df['date_active'] = pd.to_datetime(df['date_active'], errors='coerce')
    now = pd.Timestamp.now()
    one_month_ago = now - pd.DateOffset(months=1)

    def check_condition(row):
        if row['status'] == 'defect':
            return 'inactive'
        elif pd.notnull(row['date_active']) and row['date_active'] < one_month_ago:
            return 'inactive'
        else:
            return 'active'

    df['status_agent'] = df.apply(check_condition, axis=1)
    df = df.drop(columns=['status', 'date_active'])

    df['defect_status'] = np.where(df['cuscode'].str.contains('-defect', na=False), 'defect', None)

    # Rename columns
    rename_columns = {
        "cuscode": "agent_id",
        "name": "agent_name",
        "rank": "agent_rank",
        "user_registered": "hire_date",
        "status_agent": "status_agent",
        "type_agent": "type_agent",
        "typebuy": "is_experienced",
        "user_email": "agent_email",
        "name_store": "store_name",
        "address": "agent_address",
        "city": "subdistrict",
        "district": "district",
        "province": "province",
        "province_cur": "current_province",
        "area_cur": "current_area",
        "postcode": "zipcode",
        "tel": "mobile_number",
        "career": "job",
        "agent_region": "agent_region",
        "defect_status": "defect_status",
        "card_ins_type": "card_ins_type",
        "file_card_ins": "file_card_ins",
        "card_ins_type_life": "card_ins_type_life",
        "file_card_ins_life": "file_card_ins_life"
    }
    df = df.rename(columns=rename_columns)
    df['defect_status'] = np.where(df['agent_id'].str.contains('-defect', na=False), 'defect', None)

    # Clean fields
    df['card_ins_type_life'] = df['card_ins_type_life'].apply(lambda x: 'B' if isinstance(x, str) and '‡πÅ‡∏ó‡∏ô' in x else x)
    df['is_experienced_fix'] = df['is_experienced'].apply(lambda x: '‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' if str(x).strip().lower() == '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' else '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢')
    df = df.drop(columns=['is_experienced'])
    df.rename(columns={'is_experienced_fix': 'is_experienced'}, inplace=True)

    valid_rank = [str(i) for i in range(1, 11)]
    df.loc[~df['agent_rank'].isin(valid_rank), 'agent_rank'] = np.nan

    df['agent_address_cleaned'] = df['agent_address'].apply(lambda addr: re.sub(r'(‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô|‡∏ã‡∏≠‡∏¢|‡∏ñ‡∏ô‡∏ô)[\s\-]*', '', str(addr)).strip())
    df = df.drop(columns=['agent_address'])
    df.rename(columns={'agent_address_cleaned': 'agent_address'}, inplace=True)

    df['mobile_number'] = df['mobile_number'].str.replace(r'[^0-9]', '', regex=True)
    df = df.replace(r'^\s*$', pd.NA, regex=True)

    df_temp = df.replace(r'^\s*$', np.nan, regex=True)
    df['non_empty_count'] = df_temp.notnull().sum(axis=1)

    valid_mask = df['agent_id'].astype(str).str.strip().ne('') & df['agent_id'].notna()
    df_with_id = df[valid_mask]
    df_without_id = df[~valid_mask]
    df_with_id_cleaned = df_with_id.sort_values('non_empty_count', ascending=False).drop_duplicates(subset='agent_id', keep='first')
    df_cleaned = pd.concat([df_with_id_cleaned, df_without_id], ignore_index=True)
    df_cleaned = df_cleaned.drop(columns=['non_empty_count'])
    df_cleaned.columns = df_cleaned.columns.str.lower()

    df_cleaned["hire_date"] = pd.to_datetime(df_cleaned["hire_date"], errors='coerce')
    df_cleaned["hire_date"] = df_cleaned["hire_date"].dt.strftime('%Y%m%d')
    df_cleaned["hire_date"] = df_cleaned["hire_date"].where(df_cleaned["hire_date"].notnull(), None)
    df_cleaned["hire_date"] = df_cleaned["hire_date"].astype('Int64')

    df_cleaned["zipcode"] = df_cleaned["zipcode"].where(df_cleaned["zipcode"].str.len() == 5, np.nan)
    df_cleaned["agent_name"] = df_cleaned["agent_name"].str.lstrip()
    df_cleaned["is_experienced"] = df_cleaned["is_experienced"].apply(lambda x: 'no' if str(x).strip().lower() == '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' else 'yes')

    print("\nüìä Cleaning completed")

    return df_cleaned

@op
def load_to_wh(df: pd.DataFrame):
    table_name = 'dim_agent'
    pk_column = 'agent_id'

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á agent_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å DataFrame ‡πÉ‡∏´‡∏°‡πà
    df = df[~df[pk_column].duplicated(keep='first')].copy()

    # ‚úÖ Load ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å PostgreSQL
    with target_engine.connect() as conn:
        df_existing = pd.read_sql(f"SELECT * FROM {table_name}", conn)

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á agent_id ‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
    df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()

    # ‚úÖ Identify agent_id ‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô DB)
    new_ids = set(df[pk_column]) - set(df_existing[pk_column])
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()

    # ‚úÖ Identify agent_id ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    common_ids = set(df[pk_column]) & set(df_existing[pk_column])
    df_common_new = df[df[pk_column].isin(common_ids)].copy()
    df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()

    # ‚úÖ Merge ‡∏î‡πâ‡∏ß‡∏¢ suffix (_new, _old)
    merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))

    # ‚úÖ ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô key ‡πÅ‡∏•‡∏∞ audit fields)
    exclude_columns = [pk_column, 'id_contact', 'create_at', 'update_at']
    compare_cols = [
        col for col in df.columns
        if col not in exclude_columns
        and f"{col}_new" in merged.columns
        and f"{col}_old" in merged.columns
    ]

    # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å pd.NA
    def is_different(row):
        for col in compare_cols:
            val_new = row.get(f"{col}_new")
            val_old = row.get(f"{col}_old")
            if pd.isna(val_new) and pd.isna(val_old):
                continue
            if val_new != val_old:
                return True
        return False

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
    df_diff = merged[merged.apply(is_different, axis=1)].copy()

    # ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö update ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ agent_id ‡∏õ‡∏Å‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡πÄ‡∏ï‡∏¥‡∏° _new)
    update_cols = [f"{col}_new" for col in compare_cols]
    all_cols = [pk_column] + update_cols

    df_diff_renamed = df_diff[all_cols].copy()
    df_diff_renamed.columns = [pk_column] + compare_cols  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ column ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á

    print(f"üÜï Insert: {len(df_to_insert)} rows")
    print(f"üîÑ Update: {len(df_diff_renamed)} rows")

    # ‚úÖ Load table metadata
    metadata = Table(table_name, MetaData(), autoload_with=target_engine)

    # ‚úÖ Insert (‡∏Å‡∏£‡∏≠‡∏á agent_id ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NaN)
    if not df_to_insert.empty:
        df_to_insert_valid = df_to_insert[df_to_insert[pk_column].notna()].copy()
        dropped = len(df_to_insert) - len(df_to_insert_valid)
        if dropped > 0:
            print(f"‚ö†Ô∏è Skipped {dropped}")
        if not df_to_insert_valid.empty:
            with target_engine.begin() as conn:
                conn.execute(metadata.insert(), df_to_insert_valid.to_dict(orient='records'))

    # ‚úÖ Update
    if not df_diff_renamed.empty:
        with target_engine.begin() as conn:
            for record in df_diff_renamed.to_dict(orient='records'):
                stmt = pg_insert(metadata).values(**record)
                update_columns = {
                    c.name: stmt.excluded[c.name]
                    for c in metadata.columns
                    if c.name != pk_column
                }
                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns
                )
                conn.execute(stmt)

    print("‚úÖ Insert/update completed.")
    
@job
def dim_agent_etl():
    load_to_wh(clean_agent_data(extract_agent_data()))

if __name__ == "__main__":
    df_raw = extract_agent_data()
    print("‚úÖ Extracted logs:", df_raw.shape)

    df_clean = clean_agent_data((df_raw))
    print("‚úÖ Cleaned columns:", df_clean.columns)

    # print(df_clean.head(10))

    output_path = "dim_agent.csv"
    df_clean.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"üíæ Saved to {output_path}")

    # load_to_wh(df_clean)
    # print("üéâ Test completed! Data upserted to dim_agent.")