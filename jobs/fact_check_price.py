from dagster import op, job
import pandas as pd
import numpy as np
import os
import json
import re
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, inspect, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert

load_dotenv()

# DB: Source (MariaDB)
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
)

# DB: Target (PostgreSQL)
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

@op
def extract_fact_check_price():
    df_logs = pd.read_sql("""
        SELECT cuscode, brand, series, subseries, year, no_car, type, repair_type,
               assured_insurance_capital1, camera, addon, quo_num, create_at, results, selected, carprovince
        FROM fin_customer_logs_B2B
    """, source_engine, chunksize=50000)

    df_checkprice = pd.read_sql("""
        SELECT id_cus, datekey, brand, model, submodel, yearcar, idcar, nocar, type_ins,
               company, tunprakan, deduct, status, type_driver, type_camera, type_addon, status_send
        FROM fin_checkprice
    """, source_engine, chunksize=50000)

    return df_logs, df_checkprice

@op
def clean_fact_check_price(data):
    df_logs, df_checkprice = data

    province_list = ["‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå", "‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£",
    "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤", "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó", "‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥",
    "‡∏ä‡∏∏‡∏°‡∏û‡∏£", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢", "‡∏ï‡∏£‡∏±‡∏á", "‡∏ï‡∏£‡∏≤‡∏î", "‡∏ï‡∏≤‡∏Å", "‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å",
    "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤", "‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä", "‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå",
    "‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™", "‡∏ô‡πà‡∏≤‡∏ô", "‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨", "‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ",
    "‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå", "‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤",
    "‡∏û‡∏±‡∏á‡∏á‡∏≤", "‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á", "‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£", "‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå",
    "‡πÅ‡∏û‡∏£‡πà", "‡∏û‡∏∞‡πÄ‡∏¢‡∏≤", "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï", "‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°", "‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£", "‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô",
    "‡∏¢‡∏∞‡∏•‡∏≤", "‡∏¢‡πÇ‡∏™‡∏ò‡∏£", "‡∏£‡∏∞‡∏ô‡∏≠‡∏á", "‡∏£‡∏∞‡∏¢‡∏≠‡∏á", "‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ", "‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î", "‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ",
    "‡∏•‡∏≥‡∏õ‡∏≤‡∏á", "‡∏•‡∏≥‡∏û‡∏π‡∏ô", "‡πÄ‡∏•‡∏¢", "‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©", "‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£", "‡∏™‡∏á‡∏Ç‡∏•‡∏≤", "‡∏™‡∏ï‡∏π‡∏•",
    "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß", "‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ",
    "‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢", "‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå",
    "‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢", "‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏±‡∏ß‡∏•‡∏≥‡∏†‡∏π", "‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á", "‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ",
    "‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç"]  

    def extract_company_names(x):
        if pd.isnull(x): return [None]*4
        data = json.loads(x)
        names = [d.get('company_name') for d in data if isinstance(d, dict)] if isinstance(data, list) else [data.get('company_name')]
        return (names + [None]*4)[:4]

    def extract_selected(x):
        if pd.isnull(x): return None
        data = json.loads(x)
        if isinstance(data, list) and data: return data[0].get('company_name')
        if isinstance(data, dict): return data.get('company_name')
        return None

    logs = df_logs.copy()
    logs[['company_name1', 'company_name2', 'company_name3', 'company_name4']] = logs['results'].apply(extract_company_names).apply(pd.Series)
    logs['selecteds'] = logs['selected'].apply(extract_selected)

    logs['camera'] = logs['camera'].map({'yes': '‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ', 'no': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ'})
    logs['addon'] = logs['addon'].map({'‡πÑ‡∏°‡πà‡∏°‡∏µ': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÄ‡∏ï‡∏¥‡∏°'})

    logs.rename(columns={
        'cuscode': 'id_cus', 'brand': 'brand', 'series': 'model', 'subseries': 'submodel',
        'year': 'yearcar', 'no_car': 'car_code', 'assured_insurance_capital1': 'sum_insured',
        'camera': 'type_camera', 'addon': 'type_addon', 'quo_num': 'select_quotation',
        'create_at': 'transaction_date', 'carprovince': 'province_car'
    }, inplace=True)

    logs['type_insurance'] = '‡∏ä‡∏±‡πâ‡∏ô' + logs['type'].astype(str) + logs['repair_type'].astype(str)
    logs['input_type'] = 'auto'
    logs.drop(columns=['type', 'repair_type', 'results', 'selected'], inplace=True)

    def split_company(x):
        if pd.isnull(x): return [None]*4
        parts = [i.strip() for i in x.split('/')]
        return (parts + [None]*4)[:4]

    def extract_plate(x):
        if pd.isnull(x): return None
        x = str(x)
        for p in province_list:
            x = x.replace(p, '')
        return re.findall(r'\d{1,2}[‡∏Å-‡∏Æ]{1,2}\d{1,4}', x.replace('-', '').replace('/', ''))[0] if re.findall(r'\d{1,2}[‡∏Å-‡∏Æ]{1,2}\d{1,4}', x) else None

    check = df_checkprice.copy()
    check[['company_name1', 'company_name2', 'company_name3', 'company_name4']] = check['company'].apply(split_company).apply(pd.Series)
    check['id_car'] = check['idcar'].apply(extract_plate)
    check['province_car'] = check['idcar'].apply(lambda x: next((p for p in province_list if p in str(x)), None))
    check.drop(columns=['company', 'idcar'], inplace=True)

    check.rename(columns={
        'datekey': 'transaction_date', 'nocar': 'car_code', 'type_ins': 'type_insurance',
        'tunprakan': 'sum_insured', 'deduct': 'deductible'
    }, inplace=True)
    check['input_type'] = 'manual'

    df_combined = pd.concat([logs, check], ignore_index=True, sort=False)
    df_combined = df_combined.replace(r'^\s*$', np.nan, regex=True).applymap(lambda x: x.strip() if isinstance(x, str) else x)

    df_combined = df_combined[~df_combined['id_cus'].isin([
        'FIN-TestApp', 'FIN-TestApp2', 'FIN-TestApp3', 'FIN-TestApp6', 'FIN-TestApp-2025', 'FNGtest', '????‚ô°‚òÜumata‚ô°‚òÜ??'
    ])]

    df_combined['model'] = df_combined['model'].apply(lambda val: re.sub(r'^[\W_]+', '', str(val).strip()) if pd.notnull(val) else None)
    # Convert transaction_date to YYYYMMDD int
    df_combined['transaction_date'] = pd.to_datetime(df_combined['transaction_date'], errors='coerce')
    df_combined['transaction_date'] = df_combined['transaction_date'].dt.strftime('%Y%m%d').astype('Int64')
    df_combined = df_combined.drop_duplicates()

    return df_combined

@op
def load_fact_check_price(df: pd.DataFrame):
    table_name = 'fact_check_price'
    pk_column = ['id_cus', 'brand', 'model', 'submodel', 'yearcar', 'car_code',
                'sum_insured', 'type_camera', 'type_addon', 'transaction_date',
                'province_car', 'company_name1', 'company_name2', 'company_name3',
                'company_name4', 'selecteds', 'type_insurance', 'id_car']

    insp = inspect(target_engine)
    columns = [col['name'] for col in insp.get_columns(table_name)]
    if 'quotation_num' not in columns:
        with target_engine.begin() as conn:
            conn.execute(text(f'ALTER TABLE {table_name} ADD COLUMN quotation_num varchar;'))
        print("‚úÖ Added column 'quotation_num'")

    metadata = Table(table_name, MetaData(), autoload_with=target_engine)
    records = df.to_dict(orient='records')

    chunk_size = 5000
    for start in range(0, len(records), chunk_size):
        end = start + chunk_size
        chunk = records[start:end]
        print(f"üîÑ Upserting chunk {start // chunk_size + 1}: records {start} to {end - 1}")

        with target_engine.begin() as conn:
            for record in chunk:
                if not record.get('quotation_num'):
                    continue
                stmt = pg_insert(metadata).values(**record)
                update_columns = {c.name: stmt.excluded[c.name] for c in metadata.columns if c.name != pk_column}
                stmt = stmt.on_conflict_do_update(index_elements=[pk_column], set_=update_columns)
                conn.execute(stmt)

    print("‚úÖ Upsert completed successfully.")

@job
def fact_check_price_etl():
    load_fact_check_price(clean_fact_check_price(extract_fact_check_price()))

if __name__ == "__main__":
    df_logs, df_checkprice = extract_fact_check_price()
    print("‚úÖ Extracted logs:", df_logs.shape)
    print("‚úÖ Extracted checkprice:", df_checkprice.shape)

    df_clean = clean_fact_check_price((df_logs, df_checkprice))
    print("‚úÖ Cleaned columns:", df_clean.columns)

    # print(df_clean.head(10))

    output_path = "fact_check_price.xlsx"
    df_clean.to_excel(output_path, index=False, engine='openpyxl')
    print(f"üíæ Saved to {output_path}")

    # load_fact_check_price(df_clean)
    # print("üéâ Test completed! Data upserted to dim_car.")
