from dagster import op, job
import pandas as pd
import numpy as np
import json
import re
import os
import time
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table, inspect, text
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import OperationalError, DisconnectionError
from datetime import datetime, timedelta

# ‚úÖ Load .env
load_dotenv()

# ‚úÖ DB source (MariaDB)
source_user = os.getenv('DB_USER')
source_password = os.getenv('DB_PASSWORD')
source_host = os.getenv('DB_HOST')
source_port = os.getenv('DB_PORT')
source_db = 'fininsurance'

source_engine = create_engine(
    f"mysql+pymysql://{source_user}:{source_password}@{source_host}:{source_port}/{source_db}"
)

# ‚úÖ DB target (PostgreSQL)
target_user = os.getenv('DB_USER_test')
target_password = os.getenv('DB_PASSWORD_test')
target_host = os.getenv('DB_HOST_test')
target_port = os.getenv('DB_PORT_test')
target_db = 'fininsurance'

target_engine = create_engine(
    f"postgresql+psycopg2://{target_user}:{target_password}@{target_host}:{target_port}/{target_db}",
    pool_size=5,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    connect_args={"connect_timeout": 30, "application_name": "fact_check_price_etl"}
)

def retry_db_operation(operation, max_retries=3, delay=2):
    for attempt in range(max_retries):
        try:
            return operation()
        except (OperationalError, DisconnectionError) as e:
            if attempt == max_retries - 1:
                raise e
            print(f"‚ö†Ô∏è DB error (attempt {attempt + 1}): {e}")
            time.sleep(delay)
            delay *= 2

@op
def extract_check_price_data() -> pd.DataFrame:
    now = datetime.now()

    start_time = now.replace(minute=0, second=0, microsecond=0)
    end_time = now.replace(minute=59, second=59, microsecond=999999)

    start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
    end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')

    query_logs = """
    SELECT cuscode, brand, series, subseries, year, no_car, type, repair_type,
           assured_insurance_capital1, camera, addon, quo_num, create_at,
           results, selected, carprovince
    FROM fin_customer_logs_B2B
    WHERE create_at BETWEEN '{start_str}' AND '{end_str}'
    """
    query_checkprice = """
    SELECT id_cus, datekey, brand, model, submodel, yearcar, idcar, nocar,
           type_ins, company, tunprakan, deduct, status, type_driver,
           type_camera, type_addon, status_send
    FROM fin_checkprice
    WHERE datekey BETWEEN '{start_str}' AND '{end_str}'
    """
    df_logs = pd.read_sql(query_logs, source_engine)
    df_check = pd.read_sql(query_checkprice, source_engine)

    return pd.DataFrame({"logs": [df_logs], "check": [df_check]})

@op
def clean_check_price_data(raw: pd.DataFrame) -> pd.DataFrame:
    df = raw['logs'][0].copy()
    df1 = raw['check'][0].copy()

    def extract_company_names(x):
        if pd.isnull(x):
            return [None, None, None, None]
        data = json.loads(x)
        names = []
        if isinstance(data, list):
            for d in data:
                if isinstance(d, dict):
                    names.append(d.get('company_name'))
                if len(names) == 4:
                    break
        elif isinstance(data, dict):
            names.append(data.get('company_name'))
        while len(names) < 4:
            names.append(None)
        return names[:4]

    company_names_df = df['results'].apply(extract_company_names).apply(pd.Series)
    company_names_df.columns = ['company_name1', 'company_name2', 'company_name3', 'company_name4']
    df = pd.concat([df.drop(columns=['results']), company_names_df], axis=1)

    def extract_selected_name(x):
        if pd.isnull(x):
            return None
        data = json.loads(x)
        if isinstance(data, list) and len(data) > 0:
            return data[0].get('company_name')
        elif isinstance(data, dict):
            return data.get('company_name')
        else:
            return None

    df['selecteds'] = df['selected'].apply(extract_selected_name)
    df.drop(columns=['selected'], inplace=True)

    df['camera'] = df['camera'].map({
        'yes': '‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ',
        'no': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ'
    })
    df['addon'] = df['addon'].map({
        '‡πÑ‡∏°‡πà‡∏°‡∏µ': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÄ‡∏ï‡∏¥‡∏°'
    })

    df.rename(columns={
        'cuscode': 'id_cus',
        'series': 'model',
        'subseries': 'submodel',
        'year': 'yearcar',
        'no_car': 'car_code',
        'assured_insurance_capital1': 'sum_insured',
        'camera': 'type_camera',
        'addon': 'type_addon',
        'quo_num': 'select_quotation',
        'create_at': 'transaction_date',
        'carprovince': 'province_car'
    }, inplace=True)

    df['type_insurance'] = '‡∏ä‡∏±‡πâ‡∏ô' + df['type'].astype(str) + df['repair_type'].astype(str)
    df.drop(columns=['type', 'repair_type'], inplace=True)
    df['input_type'] = 'auto'

    def split_company_names(x):
        if pd.isnull(x):
            return [None, None, None, None]
        names = [name.strip() for name in x.split('/')]
        while len(names) < 4:
            names.append(None)
        return names[:4]

    company_names_df = df1['company'].apply(split_company_names).apply(pd.Series)
    company_names_df.columns = ['company_name1', 'company_name2', 'company_name3', 'company_name4']
    df1 = pd.concat([df1.drop(columns=['company']), company_names_df], axis=1)

    province_list = [
        "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå", "‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£", "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤",
        "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó", "‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥", "‡∏ä‡∏∏‡∏°‡∏û‡∏£", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢", "‡∏ï‡∏£‡∏±‡∏á", "‡∏ï‡∏£‡∏≤‡∏î", "‡∏ï‡∏≤‡∏Å", "‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å",
        "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤", "‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä", "‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå", "‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™", "‡∏ô‡πà‡∏≤‡∏ô",
        "‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨", "‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå", "‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤",
        "‡∏û‡∏±‡∏á‡∏á‡∏≤", "‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á", "‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£", "‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå", "‡πÅ‡∏û‡∏£‡πà", "‡∏û‡∏∞‡πÄ‡∏¢‡∏≤", "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï",
        "‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°", "‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£", "‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô", "‡∏¢‡∏∞‡∏•‡∏≤", "‡∏¢‡πÇ‡∏™‡∏ò‡∏£", "‡∏£‡∏∞‡∏ô‡∏≠‡∏á", "‡∏£‡∏∞‡∏¢‡∏≠‡∏á", "‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ",
        "‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î", "‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ", "‡∏•‡∏≥‡∏õ‡∏≤‡∏á", "‡∏•‡∏≥‡∏û‡∏π‡∏ô", "‡πÄ‡∏•‡∏¢", "‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©", "‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£", "‡∏™‡∏á‡∏Ç‡∏•‡∏≤", "‡∏™‡∏ï‡∏π‡∏•",
        "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß", "‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢",
        "‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå", "‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢", "‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏±‡∏ß‡∏•‡∏≥‡∏†‡∏π", "‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á", "‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ",
        "‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç"
    ]

    def extract_clean_plate(value):
        if pd.isnull(value) or value.strip() == "":
            return None
        text = value.strip()
        for prov in province_list:
            if prov in text:
                text = text.replace(prov, "").strip()
        if '//' in text:
            text = text.split('//')[0].strip()
        elif '/' in text:
            text = text.split('/')[0].strip()
        text_cleaned = text.replace('-', '').replace('/', '').replace(',', '').replace('+', '').replace(' ', '')
        text_cleaned = re.sub(r'^‡∏õ\d+‡πÅ‡∏•‡∏∞‡∏õ\d+\+?', '', text_cleaned)
        text_cleaned = re.sub(r'^‡∏õ\d+\+?', '', text_cleaned)
        text_cleaned = re.sub(r'^‡∏á‡∏≤‡∏ô‡∏õ\d+', '', text_cleaned)
        pattern_plate = r'(\d{1}[‡∏Å-‡∏Æ]{2}\d{1,4}|[‡∏Å-‡∏Æ]{1,3}\d{1,4})'
        match_plate = re.match(pattern_plate, text_cleaned)
        if match_plate:
            plate = match_plate.group(1)
            if len(plate) <= 3:
                return None
            return plate
        return None

    def extract_province(value):
        if pd.isnull(value) or value.strip() == "":
            return None
        for prov in province_list:
            if prov in value:
                return prov
        return None

    df1['id_car'] = df1['idcar'].apply(extract_clean_plate)
    df1['province_car'] = df1['idcar'].apply(extract_province)
    df1.drop(columns=['idcar'], inplace=True)

    df1 = df1[~df1['id_cus'].isin(['‡∏ó‡∏®‡∏û‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö', 'FIN-TestApp'])]
    df1.rename(columns={
        'datekey': 'transaction_date',
        'nocar': 'car_code',
        'type_ins': 'type_insurance',
        'tunprakan': 'sum_insured',
        'deduct': 'deductible'
    }, inplace=True)
    df1['input_type'] = 'manual'

    df_combined = pd.concat([df, df1], ignore_index=True, sort=False)
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç deprecated methods - ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ó‡∏ô
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà empty strings ‡∏î‡πâ‡∏ß‡∏¢ NaN
    for col in df_combined.columns:
        if df_combined[col].dtype == 'object':
            df_combined[col] = df_combined[col].replace(r'^\s*$', np.nan, regex=True)
    
    df_combined = df_combined.where(pd.notnull(df_combined), None)
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç applymap ‡πÄ‡∏õ‡πá‡∏ô apply
    for col in df_combined.columns:
        if df_combined[col].dtype == 'object':
            df_combined[col] = df_combined[col].apply(lambda x: x.lstrip() if isinstance(x, str) else x)
    
    df_combined = df_combined[~df_combined['id_cus'].isin([
        'FIN-TestApp', 'FIN-TestApp2', 'FIN-TestApp3',
        'FIN-TestApp6', 'FIN-TestApp-2025', 'FNGtest', '????‚ô°‚òÜumata‚òÜ??'
    ])]

    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö vectorized
    date_columns = ['transaction_date']
    for col in date_columns:
        if col in df_combined.columns:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô integer format YYYYMMDD
            df_combined[col] = pd.to_datetime(df_combined[col], errors='coerce')
            df_combined[col] = df_combined[col].dt.strftime('%Y%m%d').astype('Int64')

    # ‡πÅ‡∏õ‡∏•‡∏á data types ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
    df_combined['yearcar'] = pd.to_numeric(df_combined['yearcar'], errors='coerce').astype('Int64')
    df_combined['sum_insured'] = pd.to_numeric(df_combined['sum_insured'], errors='coerce').astype('Int64')
    df_combined['deductible'] = pd.to_numeric(df_combined['deductible'], errors='coerce').astype('Int64')
    df_combined['id_cus'] = df_combined['id_cus'].astype(str)

    return df_combined

@op
def load_check_price_data(df: pd.DataFrame):
    try:
        table_name = 'fact_check_price'
        # ‡πÉ‡∏ä‡πâ transaction_date ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ó‡∏ô composite key
        compare_column = 'transaction_date'

        print(f"üìä Processing {len(df)} rows...")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types ‡∏Å‡πà‡∏≠‡∏ô
        print("üîç Data types before processing:")
        print(f"  {compare_column}: {df[compare_column].dtype}")

        # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
        today_str = datetime.now().strftime('%Y-%m-%d')

        # ‚úÖ Load ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å PostgreSQL
        with target_engine.connect() as conn:
            df_existing = pd.read_sql(
                f"SELECT * FROM {table_name} WHERE update_at >= '{today_str}'",
                conn
            )

        print(f"üìÖ Found {len(df_existing)}")

        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
        df_existing = df_existing[~df_existing.duplicated(keep='first')].copy()

        # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô database)
        existing_ids = set(df_existing[compare_column])
        new_ids = set(df[compare_column]) - existing_ids
        df_to_insert = df[df[compare_column].isin(new_ids)].copy()

        # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏°‡∏µ‡πÉ‡∏ô database)
        common_ids = set(df[compare_column]) & existing_ids
        df_common_new = df[df[compare_column].isin(common_ids)].copy()
        df_common_old = df_existing[df_existing[compare_column].isin(common_ids)].copy()

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        merged = df_common_new.merge(df_common_old, on=compare_column, suffixes=('_new', '_old'), how='inner')

        # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô transaction_date ‡πÅ‡∏•‡∏∞ metadata columns)
        exclude_columns = [compare_column, 'check_price_id', 'create_at', 'update_at']
        
        # ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á df ‡πÅ‡∏•‡∏∞ df_existing
        all_columns = set(df_common_new.columns) & set(df_common_old.columns)
        compare_cols = [
            col for col in all_columns
            if col not in exclude_columns
            and f"{col}_new" in merged.columns
            and f"{col}_old" in merged.columns
        ]

        def is_different(row):
            for col in compare_cols:
                val_new = row.get(f"{col}_new")
                val_old = row.get(f"{col}_old")
                if pd.isna(val_new) and pd.isna(val_old):
                    continue
                if val_new != val_old:
                    return True
            return False

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
        df_diff = merged[merged.apply(is_different, axis=1)].copy()

        if not df_diff.empty and compare_cols:
            update_cols = [f"{col}_new" for col in compare_cols]
            all_cols = [compare_column] + update_cols

            # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
            existing_cols = [c for c in all_cols if c in df_diff.columns]
            
            if len(existing_cols) > 1:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ compare_column ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô
                df_diff_renamed = df_diff.loc[:, existing_cols].copy()
                # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ column ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
                new_col_names = [compare_column] + [col.replace('_new', '') for col in existing_cols if col != compare_column]
                df_diff_renamed.columns = new_col_names
            else:
                df_diff_renamed = pd.DataFrame()
        else:
            df_diff_renamed = pd.DataFrame()

        print(f"üÜï Insert: {len(df_to_insert)} rows")
        print(f"üîÑ Update: {len(df_diff_renamed)} rows")

        metadata = Table(table_name, MetaData(), autoload_with=target_engine)

        # Insert ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        if not df_to_insert.empty:
            # ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PostgreSQL
            df_to_insert_valid = df_to_insert[df_to_insert[compare_column].notna()].copy()
            df_to_insert_valid = df_to_insert_valid.replace({np.nan: None})
            
            dropped = len(df_to_insert) - len(df_to_insert_valid)
            if dropped > 0:
                print(f"‚ö†Ô∏è Skipped {dropped} rows with null {compare_column}")
            if not df_to_insert_valid.empty:
                with target_engine.begin() as conn:
                    conn.execute(metadata.insert(), df_to_insert_valid.to_dict(orient='records'))
                print(f"‚úÖ Inserted {len(df_to_insert_valid)} new records")

        # Update ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
        if not df_diff_renamed.empty and compare_cols:
            # ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PostgreSQL
            df_diff_renamed = df_diff_renamed.replace({np.nan: None})
            
            with target_engine.begin() as conn:
                for record in df_diff_renamed.to_dict(orient='records'):
                    # ‡πÉ‡∏ä‡πâ UPDATE statement ‡πÅ‡∏ó‡∏ô ON CONFLICT
                    transaction_date = record[compare_column]
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á SET clause ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
                    set_clause = []
                    update_values = {}
                    
                    for c in metadata.columns:
                        if c.name not in [compare_column, 'check_price_id', 'create_at', 'update_at']:
                            if c.name in record:
                                set_clause.append(f"{c.name} = %({c.name})s")
                                update_values[c.name] = record[c.name]
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° update_at
                    set_clause.append("update_at = %(update_at)s")
                    update_values['update_at'] = datetime.now()
                    update_values['transaction_date'] = transaction_date
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á UPDATE statement
                    update_sql = f"""
                    UPDATE {table_name} 
                    SET {', '.join(set_clause)}
                    WHERE {compare_column} = %(transaction_date)s
                    """
                    
                    conn.execute(text(update_sql), update_values)
            print(f"‚úÖ Updated {len(df_diff_renamed)} records")

        print("‚úÖ Insert/update completed.")
        
    except Exception as e:
        print(f"‚ùå Error in load_check_price_data: {str(e)}")
        print(f"üîç Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        raise

@job
def fact_check_price_etl():
    load_check_price_data(clean_check_price_data(extract_check_price_data()))

if __name__ == "__main__":
    df_raw = extract_check_price_data()
    # print("‚úÖ Extracted logs:", df_raw.shape)

    df_clean = clean_check_price_data(df_raw)
#     print("‚úÖ Cleaned columns:", df_clean.columns)

    output_path = "fact_check_price.xlsx"
    df_clean.to_excel(output_path, index=False, engine='openpyxl')
    print(f"üíæ Saved to {output_path}")

    load_check_price_data(df_clean)
    print("üéâ completed! Data upserted to fact_check_price.")
