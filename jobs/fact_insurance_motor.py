from dagster import op, job
import pandas as pd
import numpy as np
import os
import re
import time
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import OperationalError, DisconnectionError
from datetime import datetime, timedelta


# ‚úÖ Load .env
load_dotenv()

# ‚úÖ Source DB connections with timeout and retry settings
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    connect_args={
        'connect_timeout': 60,
        'read_timeout': 300,
        'write_timeout': 300,
        'charset': 'utf8mb4',
        'autocommit': True
    }
)

task_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task",
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    connect_args={
        'connect_timeout': 60,
        'read_timeout': 300,
        'write_timeout': 300,
        'charset': 'utf8mb4',
        'autocommit': True
    }
)

# ‚úÖ Target PostgreSQL
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

def remove_commas_from_numeric(value):
    """‡∏•‡∏ö‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
    if pd.isna(value) or value is None:
        return value
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô
    value_str = str(value).strip()
    
    # ‡∏•‡∏ö‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥‡∏≠‡∏≠‡∏Å
    value_str = value_str.replace(',', '')
    
    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    value_str = value_str.strip()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if value_str == '' or value_str.lower() in ['nan', 'none', 'null']:
        return None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏£‡∏ß‡∏°‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°)
    if re.match(r'^-?\d*\.?\d+$', value_str):
        return value_str
    
    return value

def clean_insurance_company(company):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô - ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
    if pd.isna(company) or company is None:
        return None
    
    company_str = str(company).strip()
    
    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á - ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏´‡∏£‡∏∑‡∏≠ SQL injection
    invalid_patterns = [
        r'[<>"\'\`\\]',  # SQL injection characters
        r'\.\./',  # path traversal
        r'[\(\)\{\}\[\]]+',  # brackets
        r'[!@#$%^&*+=|]+',  # special characters
        r'XOR',  # SQL injection
        r'if\(',  # SQL injection
        r'now\(\)',  # SQL injection
        r'\$\{',  # template injection
        r'\?\?\?\?',  # multiple question marks
        r'[0-9]+[XO][0-9]+',  # XSS patterns
    ]
    
    for pattern in invalid_patterns:
        if re.search(pattern, company_str, re.IGNORECASE):
            return None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
    if len(company_str) < 2 or len(company_str) > 100:
        return None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ï‡∏±‡∏ß)
    if not re.search(r'[‡∏Å-‡πô]', company_str):
        return None
    
    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
    # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©, ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
    cleaned_company = re.sub(r'[0-9a-zA-Z\s]+', ' ', company_str)
    
    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
    cleaned_company = re.sub(r'\s+', ' ', cleaned_company).strip()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if len(cleaned_company) < 2:
        return None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not re.search(r'[‡∏Å-‡πô]', cleaned_company):
        return None
    
    return cleaned_company

def execute_query_with_retry(engine, query, max_retries=3, delay=5):
    """Execute query with retry mechanism for connection issues"""
    for attempt in range(max_retries):
        try:
            print(f"üîÑ Attempt {attempt + 1}/{max_retries} - Executing query...")
            df = pd.read_sql(query, engine)
            print(f"‚úÖ Query executed successfully on attempt {attempt + 1}")
            return df
        except (OperationalError, DisconnectionError) as e:
            if "Lost connection" in str(e) or "connection was forcibly closed" in str(e):
                print(f"‚ö†Ô∏è Connection lost on attempt {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    print(f"‚è≥ Waiting {delay} seconds before retry...")
                    time.sleep(delay)
                    delay *= 2  # Exponential backoff
                    # Dispose and recreate engine to force new connections
                    engine.dispose()
                else:
                    print(f"‚ùå All retry attempts failed")
                    raise
            else:
                raise
        except Exception as e:
            print(f"‚ùå Unexpected error: {str(e)}")
            raise

def close_engines():
    """Safely close all database engines"""
    try:
        source_engine.dispose()
        task_engine.dispose()
        target_engine.dispose()
        print("üîí Database connections closed safely")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing connections: {str(e)}")

@op
def extract_motor_data():
    now = datetime.now()

    start_time = now.replace(minute=0, second=0, microsecond=0)
    end_time = now.replace(minute=59, second=59, microsecond=999999)

    start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
    end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')

    plan_query = """
        SELECT quo_num, company, company_prb, assured_insurance_capital1, is_addon, type, repair_type
        FROM fin_system_select_plan
        WHERE datestart BETWEEN '{start_str}' AND '{end_str}' 
        AND type_insure = '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ'
    """
    df_plan = execute_query_with_retry(source_engine, plan_query)

    # Query 2: Extract order data with retry mechanism
    order_query = """
        SELECT quo_num, responsibility1, responsibility2, responsibility3, responsibility4,
               damage1, damage2, damage3, damage4, protect1, protect2, protect3, protect4,
               IF(sendtype = '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏´‡∏°‡πà', provincenew, province) AS delivery_province,
               show_ems_price, show_ems_type
        FROM fin_order
    """
    df_order = execute_query_with_retry(task_engine, order_query)

    # Query 3: Extract payment data with retry mechanism
    pay_query = """
        SELECT quo_num, date_warranty, date_exp
        FROM fin_system_pay
        WHERE datestart BETWEEN '{start_str}' AND '{end_str}'
    """
    df_pay = execute_query_with_retry(source_engine, pay_query)

    print("üì¶ df_plan:", df_plan.shape)
    print("üì¶ df_order:", df_order.shape)
    print("üì¶ df_pay:", df_pay.shape)

    # Close source connections after extraction
    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí Source database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing source connections: {str(e)}")

    return df_plan, df_order, df_pay

@op
def clean_motor_data(data_tuple):
    df_plan, df_order, df_pay = data_tuple

    df = df_plan.merge(df_order, on="quo_num", how="left")
    df = df.merge(df_pay, on="quo_num", how="left")

    def combine_columns(a, b):
        a_str = str(a).strip() if pd.notna(a) else ''
        b_str = str(b).strip() if pd.notna(b) else ''
        if a_str == '' and b_str == '':
            return ''
        elif a_str == b_str:
            return a_str
        elif a_str == '':
            return b_str
        elif b_str == '':
            return a_str
        return f"{a_str} {b_str}"

    df = df.rename(columns={
        "quo_num": "quotation_num",
        "company": "ins_company",
        "company_prb": "act_ins_company",
        "assured_insurance_capital1": "sum_insured",
        "is_addon": "income_comp_ins",
        "responsibility1": "human_coverage_person",
        "responsibility2": "human_coverage_atime",
        "responsibility3": "property_coverage",
        "responsibility4": "deductible",
        "damage1": "vehicle_damage",
        "damage2": "deductible_amount",
        "damage3": "vehicle_theft_fire",
        "damage4": "vehicle_flood_damage",
        "protect1": "personal_accident_driver",
        "protect2": "personal_accident_passengers",
        "protect3": "medical_coverage",
        "protect4": "driver_coverage",
        "show_ems_price": "ems_amount",
        "show_ems_type": "delivery_type",
        "date_warranty": "date_warranty",
        "date_exp": "date_expired",
        "type":"insurance_class"
    })

    df = df.replace(r'^\s*$', pd.NA, regex=True)
    df_temp = df.replace(r'^\s*$', np.nan, regex=True)
    df["non_empty_count"] = df_temp.notnull().sum(axis=1)
    df = df.sort_values("non_empty_count", ascending=False).drop_duplicates(subset="quotation_num")
    df = df.drop(columns=["non_empty_count"], errors="ignore")

    df.columns = df.columns.str.lower()
    df["income_comp_ins"] = df["income_comp_ins"].apply(lambda x: True if x == 1 else False if x == 0 else None)
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ins_company
    if 'ins_company' in df.columns:
        before_count = df['ins_company'].notna().sum()
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á
        invalid_companies = df[df['ins_company'].notna()]['ins_company'].unique()
        df['ins_company'] = df['ins_company'].apply(clean_insurance_company)
        after_count = df['ins_company'].notna().sum()
        filtered_count = before_count - after_count
        print(f"üßπ Cleaned ins_company column - filtered {filtered_count} invalid records")
        if filtered_count > 0:
            print(f"   Examples of invalid companies: {list(invalid_companies[:5])}")
    
    # ‡πÅ‡∏õ‡∏•‡∏á datetime ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö NaT values
    df["date_warranty"] = pd.to_datetime(df["date_warranty"], errors="coerce")
    df["date_expired"] = pd.to_datetime(df["date_expired"], errors="coerce")
    
    # ‡πÅ‡∏õ‡∏•‡∏á NaT ‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ PostgreSQL ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à
    df["date_warranty"] = df["date_warranty"].replace({pd.NaT: None})
    df["date_expired"] = df["date_expired"].replace({pd.NaT: None})

    df['date_warranty'] = pd.to_datetime(df['date_warranty'], errors='coerce')
    df['date_warranty'] = df['date_warranty'].dt.strftime('%Y%m%d').astype('Int64')
    df['date_expired'] = pd.to_datetime(df['date_expired'], errors='coerce')
    df['date_expired'] = df['date_expired'].dt.strftime('%Y%m%d').astype('Int64')
    
    df['ems_amount'] = df['ems_amount'].fillna(0).astype(int)
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î - ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    def clean_province(province):
        if pd.isna(province) or str(province).strip() == '':
            return None
        
        province_str = str(province).strip()
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î
        remove_words = ['‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡∏ï‡∏≥‡∏ö‡∏•', '‡πÄ‡∏Ç‡∏ï', '‡πÄ‡∏°‡∏∑‡∏≠‡∏á', '‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£', '‡∏Å‡∏°.', '‡∏ñ‡∏ô‡∏ô', '‡∏ã‡∏≠‡∏¢', '‡∏´‡∏°‡∏π‡πà', '‡∏ö‡πâ‡∏≤‡∏ô']
        for word in remove_words:
            province_str = province_str.replace(word, '').strip()
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
        corrections = {
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£',
            '‡∏Å‡∏ó‡∏°': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£',
            '‡∏Å‡∏ó‡∏°.': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£',
            '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà': '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà',
            '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ': '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ': '‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ': '‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ',
            '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£',
            '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£',
            '‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°': '‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°',
            '‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤': '‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤',
            '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô': '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô',
            '‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ': '‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ',
            '‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ': '‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ',
            '‡∏™‡∏á‡∏Ç‡∏•‡∏≤': '‡∏™‡∏á‡∏Ç‡∏•‡∏≤',
            '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï': '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï',
            '‡∏û‡∏±‡∏ó‡∏¢‡∏≤': '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤': '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ö‡∏≤‡∏á‡∏ô‡∏≤': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£',
            '‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏µ': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£',
            '‡∏û‡∏£‡∏∞‡∏õ‡∏£‡∏∞‡πÅ‡∏î‡∏á': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£',
            '‡∏ö‡∏≤‡∏á‡∏ö‡πà‡∏≠': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£',
            '‡∏ö‡∏≤‡∏á‡πÄ‡∏™‡∏≤‡∏ò‡∏á': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£'
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
        for wrong, correct in corrections.items():
            if wrong in province_str:
                return correct
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None
        known_provinces = [
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£', '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà', '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ', 
            '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£', '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£', '‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°', '‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤', '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô',
            '‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏™‡∏á‡∏Ç‡∏•‡∏≤', '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï', '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢',
            '‡∏•‡∏≥‡∏õ‡∏≤‡∏á', '‡∏•‡∏≥‡∏û‡∏π‡∏ô', '‡πÅ‡∏û‡∏£‡πà', '‡∏ô‡πà‡∏≤‡∏ô', '‡∏û‡∏∞‡πÄ‡∏¢‡∏≤', '‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô',
            '‡∏ï‡∏≤‡∏Å', '‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢', '‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å', '‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£',
            '‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£', '‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå', '‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó', '‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ', '‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤', '‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á', '‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î', '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤',
            '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î',
            '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ï‡∏£‡∏≤‡∏î', '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß',
            '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î', '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å',
            '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î', '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ'
        ]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for known in known_provinces:
            if known in province_str or province_str in known:
                return known
        
        return None
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå delivery_province
    if 'delivery_province' in df.columns:
        df['delivery_province'] = df['delivery_province'].apply(clean_province)
        print(f"üßπ Cleaned delivery_province column - kept only provinces")
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå delivery_type
    if 'delivery_type' in df.columns:
        df['delivery_type'] = df['delivery_type'].replace('nor', 'normal')
        print(f"üì¶ Cleaned delivery_type column - changed 'nor' to 'normal'")

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå insurance_class (type) - ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if 'insurance_class' in df.columns:
        valid_classes = ['1', '1+', '2', '2+', '3', '3+', '‡∏û‡∏£‡∏ö']
        before_count = df['insurance_class'].notna().sum()
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        df['insurance_class'] = df['insurance_class'].astype(str).str.strip()
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        df['insurance_class'] = df['insurance_class'].apply(
            lambda x: x if x in valid_classes else None
        )
        
        after_count = df['insurance_class'].notna().sum()
        filtered_count = before_count - after_count
        print(f"üßπ Cleaned insurance_class column - filtered {filtered_count} invalid records")
        print(f"   Valid classes: {valid_classes}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        valid_counts = df['insurance_class'].value_counts()
        print(f"   Valid class distribution:")
        for class_name, count in valid_counts.items():
            print(f"     {class_name}: {count}")

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå repair_type - ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if 'repair_type' in df.columns:
        def clean_repair_type(repair_type):
            """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î repair_type - ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""
            if pd.isna(repair_type) or repair_type is None:
                return None
            
            repair_str = str(repair_type).strip()
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            thai_only = re.sub(r'[^‡∏Å-‡πô]', '', repair_str)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if len(thai_only) < 2:
                return None
            
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            if thai_only in ['‡∏≠‡∏π‡πà', '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πâ', '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πã']:
                return '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πà'
            elif thai_only in ['‡∏ã‡πà‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏á', '‡∏ã‡πà‡∏≠‡∏°‡∏®‡∏π‡∏ô‡∏¢‡πå']:
                return '‡∏ã‡πà‡∏≠‡∏°‡∏´‡πâ‡∏≤‡∏á'
            
            return thai_only
        
        before_count = df['repair_type'].notna().sum()
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á
        invalid_repairs = df[df['repair_type'].notna()]['repair_type'].unique()
        df['repair_type'] = df['repair_type'].apply(clean_repair_type)
        after_count = df['repair_type'].notna().sum()
        filtered_count = before_count - after_count
        print(f"üßπ Cleaned repair_type column - filtered {filtered_count} invalid records")
        if filtered_count > 0:
            print(f"   Examples of invalid repair types: {list(invalid_repairs[:5])}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        valid_repairs = df['repair_type'].value_counts().head(10)
        print(f"   Top 10 valid repair types:")
        for repair_type, count in valid_repairs.items():
            print(f"     {repair_type}: {count}")

    # ‚úÖ ‡∏•‡∏ö‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    numeric_columns = [
        "sum_insured", "human_coverage_person", "human_coverage_atime", "property_coverage",
        "deductible", "vehicle_damage", "deductible_amount", "vehicle_theft_fire",
        "vehicle_flood_damage", "personal_accident_driver", "personal_accident_passengers",
        "medical_coverage", "driver_coverage", "ems_amount"
    ]

    print("üßπ Removing commas from numeric columns...")
    for col in numeric_columns:
        if col in df.columns:
            # ‡∏•‡∏ö‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
            df[col] = df[col].apply(remove_commas_from_numeric)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö NaN values
            df[col] = pd.to_numeric(df[col], errors="coerce")
            # ‡πÉ‡∏ä‡πâ float64 ‡πÅ‡∏ó‡∏ô Int64 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ casting
            df[col] = df[col].astype("float64")
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ human_coverage_atime ‡∏à‡∏≤‡∏Å 100,000,000 ‡πÄ‡∏õ‡πá‡∏ô 10,000,000
    if 'human_coverage_atime' in df.columns:
        df['human_coverage_atime'] = df['human_coverage_atime'].replace(100000000, 10000000)
        print(f"üîß Fixed human_coverage_atime: changed 100,000,000 to 10,000,000")
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ vehicle_damage ‡πÅ‡∏•‡∏∞ vehicle_theft_fire ‡∏à‡∏≤‡∏Å 190000050 ‡πÄ‡∏õ‡πá‡∏ô 1,900,000
    if 'vehicle_damage' in df.columns:
        df['vehicle_damage'] = df['vehicle_damage'].replace(190000050, 1900000)
        print(f"üîß Fixed vehicle_damage: changed 190,000,050 to 1,900,000")
    
    if 'vehicle_theft_fire' in df.columns:
        df['vehicle_theft_fire'] = df['vehicle_theft_fire'].replace(190000050, 1900000)
        print(f"üîß Fixed vehicle_theft_fire: changed 190,000,050 to 1,900,000")
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ sum_insured ‡∏à‡∏≤‡∏Å 190000050 ‡πÄ‡∏õ‡πá‡∏ô 1900000 ‡πÅ‡∏•‡∏∞ 250000093 ‡πÄ‡∏õ‡πá‡∏ô 2500000
    if 'sum_insured' in df.columns:
        df['sum_insured'] = df['sum_insured'].replace(190000050, 1900000)
        df['sum_insured'] = df['sum_insured'].replace(250000093, 2500000)
        print(f"üîß Fixed sum_insured: corrected abnormal values")
    
    df = df.where(pd.notnull(df), None)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    if 'ins_company' in df.columns:
        valid_companies = df['ins_company'].value_counts().head(10)
        print(f"\nüìä Top 10 valid insurance companies:")
        for company, count in valid_companies.items():
            print(f"   {company}: {count}")
    
    if 'insurance_class' in df.columns:
        valid_classes = df['insurance_class'].value_counts().head(10)
        print(f"\nüìä Top 10 valid insurance classes:")
        for class_name, count in valid_classes.items():
            print(f"   {class_name}: {count}")

    print("\nüìä Cleaning completed")

    # Close any remaining connections
    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí Remaining database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing remaining connections: {str(e)}")

    return df

@op
def load_motor_data(df: pd.DataFrame):
    table_name = "fact_insurance_motor"
    pk_column = "quotation_num"

    df = df[df[pk_column].notna()].copy()
    metadata = MetaData()
    table = Table(table_name, metadata, autoload_with=target_engine)


    # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
    today_str = datetime.now().strftime('%Y-%m-%d')

    # ‚úÖ Load ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å PostgreSQL
    with target_engine.connect() as conn:
        existing_query = pd.read_sql(
            f"SELECT * FROM {table_name} WHERE update_at >= '{today_str}'",
            conn
        )
    df_existing = execute_query_with_retry(target_engine, existing_query)

    existing_ids = set(df_existing[pk_column])
    new_rows = df[~df[pk_column].isin(existing_ids)].copy()
    update_rows = df[df[pk_column].isin(existing_ids)].copy()

    print(f"üÜï Insert: {len(new_rows)} rows")
    print(f"üîÑ Update: {len(update_rows)} rows")

    if not new_rows.empty:
        print(f"üîÑ Inserting {len(new_rows)} new records...")
        try:
            with target_engine.begin() as conn:
                conn.execute(table.insert(), new_rows.to_dict(orient="records"))
            print(f"‚úÖ Insert completed successfully")
        except Exception as e:
            print(f"‚ùå Insert failed: {str(e)}")
            raise

    if not update_rows.empty:
        print(f"üîÑ Updating {len(update_rows)} existing records...")
        try:
            with target_engine.begin() as conn:
                for i, record in enumerate(update_rows.to_dict(orient="records")):
                    if i % 100 == 0:  # Progress indicator every 100 records
                        print(f"   Progress: {i}/{len(update_rows)} records processed")
                    
                    stmt = pg_insert(table).values(**record)
                    update_dict = {
                        c.name: stmt.excluded[c.name]
                        for c in table.columns if c.name not in [pk_column, 'create_at', 'update_at']
                    }
                    # update_at ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    update_dict['update_at'] = datetime.now()
                    stmt = stmt.on_conflict_do_update(
                        index_elements=[pk_column],
                        set_=update_dict
                    )
                    conn.execute(stmt)
            print(f"‚úÖ Update completed successfully")
        except Exception as e:
            print(f"‚ùå Update failed: {str(e)}")
            raise

    print("‚úÖ Insert/update completed.")

    # Close target engine connection after load
    try:
        target_engine.dispose()
        print("üîí Target database connection closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing target connection: {str(e)}")

    # Close any remaining connections
    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí All database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing all connections: {str(e)}")

@job
def fact_insurance_motor_etl():
    load_motor_data(clean_motor_data(extract_motor_data()))

# if __name__ == "__main__":
#     try:
#         print("üöÄ Starting fact_insurance_motor ETL process...")
        
#         # Extract data with retry mechanism
#         print("üì• Extracting data from source databases...")
#         df_raw = extract_motor_data()
#         print("‚úÖ Data extraction completed")

#         # Clean data
#         print("üßπ Cleaning and transforming data...")
#         df_clean = clean_motor_data((df_raw))
#         print("‚úÖ Data cleaning completed")
#         print("‚úÖ Cleaned columns:", df_clean.columns)

#         # Save to Excel for inspection
#         # output_path = "fact_insurance_motor.xlsx"
#         # df_clean.to_excel(output_path, index=False, engine='openpyxl')
#         # print(f"üíæ Saved to {output_path}")

#         # Uncomment to load to database
#         print("üì§ Loading data to target database...")
#         load_motor_data(df_clean)
#         print("üéâ ETL process completed! Data upserted to fact_insurance_motor.")
        
#     except Exception as e:
#         print(f"‚ùå ETL process failed: {str(e)}")
#         raise
#     finally:
#         # Always close database connections
#         close_engines()