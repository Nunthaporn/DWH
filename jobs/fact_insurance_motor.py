from dagster import op, job
import pandas as pd
import numpy as np
import os
import re
import time
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import OperationalError, DisconnectionError
from datetime import datetime, timedelta

# ‚úÖ Load .env
load_dotenv()

# ‚úÖ Source DB connections with timeout and retry settings
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    connect_args={
        'connect_timeout': 60,
        'read_timeout': 300,
        'write_timeout': 300,
        'charset': 'utf8mb4',
        'autocommit': True
    }
)

task_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task",
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    connect_args={
        'connect_timeout': 60,
        'read_timeout': 300,
        'write_timeout': 300,
        'charset': 'utf8mb4',
        'autocommit': True
    }
)

# ‚úÖ Target PostgreSQL
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

# -------------------------
# üîß Utilities
# -------------------------

def normalize_null_like_series(col: pd.Series) -> pd.Series:
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ null-like ('', 'NULL', 'None', 'NaN', 'N/A', '-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏') ‡πÄ‡∏õ‡πá‡∏ô pd.NA ‡πÅ‡∏ö‡∏ö case-insensitive"""
    if col.dtype == object or pd.api.types.is_string_dtype(col):
        s = col.astype(str)
        mask = s.str.strip().str.lower().isin({
            '', 'null', 'none', 'nan', 'na', 'n/a', '-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'
        })
        return col.mask(mask, pd.NA)
    return col

def purge_na_tokens(df: pd.DataFrame) -> pd.DataFrame:
    """
    ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏Ñ‡πà‡∏≤ null-like ‡∏ó‡∏∏‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö export/‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ DB:
    - ‡∏™‡∏ï‡∏£‡∏¥‡∏á '<NA>', 'NA', 'N/A', 'NULL', 'None', 'NaN', '' -> None (case-insensitive, trim)
    - pd.NA/NaN -> None
    """
    token_set = {'<na>', 'na', 'n/a', 'null', 'none', 'nan', ''}
    for col in df.columns:
        if df[col].dtype == object or pd.api.types.is_string_dtype(df[col]):
            s = df[col].astype(str).str.strip()
            df[col] = df[col].where(~s.str.lower().isin(token_set), None)
    df = df.astype(object).where(pd.notnull(df), None)
    return df

def remove_commas_from_numeric(value):
    """‡∏•‡∏ö‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏ï‡πà‡∏≠ cell)"""
    if pd.isna(value) or value is None:
        return value
    value_str = str(value).strip().replace(',', '').strip()
    if value_str == '' or value_str.lower() in ['nan', 'none', 'null']:
        return None
    if re.match(r'^-?\d*\.?\d+$', value_str):
        return value_str
    return value

def is_numeric_like_series(s: pd.Series) -> pd.Series:
    """
    ‡∏Ñ‡∏∑‡∏ô mask ‡∏ß‡πà‡∏≤ cell ‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö comma ‡πÅ‡∏•‡∏∞ trim)
    """
    s_str = s.astype(str).str.replace(',', '', regex=False).str.strip()
    return s_str.str.match(r'^-?\d+(\.\d+)?$', na=False)

def clean_numeric_commas_for_series(col: pd.Series) -> pd.Series:
    """
    ‡∏•‡∏ö comma ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô numeric-like
    ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
    """
    s = col.astype(str).str.replace(',', '', regex=False).str.strip()
    mask = s.str.match(r'^-?\d+(\.\d+)?$', na=False)
    out = col.copy()
    out[mask] = pd.to_numeric(s[mask], errors='coerce')
    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏ï‡∏£‡∏¥‡∏á‡∏ß‡πà‡∏≤‡∏á/‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ null-like ‡πÄ‡∏õ‡πá‡∏ô NA
    out = normalize_null_like_series(out)
    return out

def auto_clean_numeric_like_columns(df: pd.DataFrame, exclude: set | None = None, threshold: float = 0.8, min_numeric_rows: int = 5) -> pd.DataFrame:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà '‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà' ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏ö comma/‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞ cell ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πà
    - threshold: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô (‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á) ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÄ‡∏ä‡πà‡∏ô 0.8 = 80%
    - min_numeric_rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô false positive
    """
    exclude = exclude or set()
    for col in df.columns:
        if col in exclude:
            continue
        if not (df[col].dtype == object or pd.api.types.is_string_dtype(df[col])):
            continue

        s = df[col].astype(str).str.strip()
        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏ö‡πà‡∏≠‡∏¢ (‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
        thai_frac = s.str.contains(r'[‡∏Å-‡πô]', regex=True, na=False).mean()
        if thai_frac > 0.2:
            continue

        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á/‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà null-like
        non_null_mask = ~s.str.strip().str.lower().isin({'', 'null', 'none', 'nan', 'na', 'n/a', '-', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'})
        if non_null_mask.sum() == 0:
            continue

        numeric_like_mask = is_numeric_like_series(s[non_null_mask])
        numeric_like_ratio = numeric_like_mask.mean()
        numeric_like_count = numeric_like_mask.sum()

        if numeric_like_ratio >= threshold and numeric_like_count >= min_numeric_rows:
            df[col] = clean_numeric_commas_for_series(df[col])
    return df

def clean_insurance_company(company):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢, ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå, ‡∏Å‡∏±‡∏ô pattern ‡πÅ‡∏õ‡∏•‡∏Å"""
    if pd.isna(company) or company is None:
        return None
    company_str = str(company).strip()

    invalid_patterns = [
        r'[<>"\'\`\\]', r'\.\./', r'[\(\)\{\}\[\]]+', r'[!@#$%^&*+=|]+',
        r'XOR', r'if\(', r'now\(\)', r'\$\{', r'\?\?\?\?', r'[0-9]+[XO][0-9]+',
    ]
    for pattern in invalid_patterns:
        if re.search(pattern, company_str, re.IGNORECASE):
            return None

    if len(company_str) < 2 or len(company_str) > 100:
        return None
    if not re.search(r'[‡∏Å-‡πô]', company_str):
        return None

    cleaned_company = re.sub(r'[0-9a-zA-Z\s]+', ' ', company_str)
    cleaned_company = re.sub(r'\s+', ' ', cleaned_company).strip()
    if len(cleaned_company) < 2:
        return None
    if not re.search(r'[‡∏Å-‡πô]', cleaned_company):
        return None
    return cleaned_company

def execute_query_with_retry(engine, query, max_retries=3, delay=5):
    """Execute query with retry mechanism for connection issues"""
    for attempt in range(max_retries):
        try:
            print(f"üîÑ Attempt {attempt + 1}/{max_retries} - Executing query...")
            df = pd.read_sql(query, engine)
            print(f"‚úÖ Query executed successfully on attempt {attempt + 1}")
            return df
        except (OperationalError, DisconnectionError) as e:
            if "Lost connection" in str(e) or "connection was forcibly closed" in str(e):
                print(f"‚ö†Ô∏è Connection lost on attempt {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    print(f"‚è≥ Waiting {delay} seconds before retry...")
                    time.sleep(delay)
                    delay *= 2
                    engine.dispose()
                else:
                    print(f"‚ùå All retry attempts failed")
                    raise
            else:
                raise
        except Exception as e:
            print(f"‚ùå Unexpected error: {str(e)}")
            raise

def close_engines():
    """Safely close all database engines"""
    try:
        source_engine.dispose()
        task_engine.dispose()
        target_engine.dispose()
        print("üîí Database connections closed safely")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing connections: {str(e)}")

def parse_amount(value):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á: '‡∏ü‡∏£‡∏µ', '0 ‡∏ö‡∏≤‡∏ó', '1,234', '‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°', None -> ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠ NaN"""
    if pd.isna(value):
        return np.nan
    s = str(value).strip().lower()
    if s in {"", "‡∏ü‡∏£‡∏µ", "free", "‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô", "‡πÑ‡∏°‡πà‡∏°‡∏µ", "‡πÑ‡∏°‡πà‡∏Ñ‡∏¥‡∏î", "0", "0 ‡∏ö‡∏≤‡∏ó"}:
        return 0
    s = re.sub(r"[^0-9\.\-]", "", s)
    if s in {"", "-", "."}:
        return np.nan
    try:
        return float(s)
    except:
        return np.nan

# -------------------------
# üß≤ Extract
# -------------------------

@op
def extract_motor_data():
    plan_query = """
        SELECT quo_num, company, company_prb, assured_insurance_capital1, is_addon, type, repair_type
        FROM fin_system_select_plan
        WHERE update_at BETWEEN '2025-01-01' AND '2025-08-31' 
          AND type_insure = '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ'
    """
    df_plan = execute_query_with_retry(source_engine, plan_query)

    order_query = """
        SELECT quo_num, responsibility1, responsibility2, responsibility3, responsibility4,
               damage1, damage2, damage3, damage4, protect1, protect2, protect3, protect4,
               IF(sendtype = '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏´‡∏°‡πà', provincenew, province) AS delivery_province,
               show_ems_price, show_ems_type, sendtype
        FROM fin_order
    """
    df_order = execute_query_with_retry(task_engine, order_query)

    pay_query = """
        SELECT quo_num, date_warranty, date_exp
        FROM fin_system_pay
    """
    df_pay = execute_query_with_retry(source_engine, pay_query)

    print("üì¶ df_plan:", df_plan.shape)
    print("üì¶ df_order:", df_order.shape)
    print("üì¶ df_pay:", df_pay.shape)

    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí Source database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing source connections: {str(e)}")

    return df_plan, df_order, df_pay

# -------------------------
# üßº Clean / Transform
# -------------------------

@op
def clean_motor_data(data_tuple):
    df_plan, df_order, df_pay = data_tuple

    df = df_plan.merge(df_order, on="quo_num", how="left")
    df = df.merge(df_pay, on="quo_num", how="left")

    df = df.rename(columns={
        "quo_num": "quotation_num",
        "company": "ins_company",
        "company_prb": "act_ins_company",
        "assured_insurance_capital1": "sum_insured",
        "is_addon": "income_comp_ins",
        "responsibility1": "human_coverage_person",
        "responsibility2": "human_coverage_atime",
        "responsibility3": "property_coverage",
        "responsibility4": "deductible",
        "damage1": "vehicle_damage",
        "damage2": "deductible_amount",
        "damage3": "vehicle_theft_fire",
        "damage4": "vehicle_flood_damage",
        "protect1": "personal_accident_driver",
        "protect2": "personal_accident_passengers",
        "protect3": "medical_coverage",
        "protect4": "driver_coverage",
        "show_ems_price": "ems_amount",
        "show_ems_type": "delivery_type",
        "date_warranty": "date_warranty",
        "date_exp": "date_expired",
        "type": "insurance_class"
    })

    df = df.replace(r'^\s*$', pd.NA, regex=True)
    df_temp = df.replace(r'^\s*$', np.nan, regex=True)
    df["non_empty_count"] = df_temp.notnull().sum(axis=1)
    df = df.sort_values("non_empty_count", ascending=False).drop_duplicates(subset="quotation_num")
    df = df.drop(columns=["non_empty_count"], errors="ignore")

    df.columns = df.columns.str.lower()
    df["income_comp_ins"] = df["income_comp_ins"].apply(lambda x: True if x == 1 else False if x == 0 else None)

    # ‚úÖ company fields
    if 'ins_company' in df.columns:
        before = df['ins_company'].notna().sum()
        df['ins_company'] = df['ins_company'].apply(clean_insurance_company)
        df['ins_company'] = normalize_null_like_series(df['ins_company'])
        after = df['ins_company'].notna().sum()
        print(f"üßπ Cleaned ins_company - filtered {before - after} invalid")

    if 'act_ins_company' in df.columns:
        before = df['act_ins_company'].notna().sum()
        df['act_ins_company'] = df['act_ins_company'].apply(clean_insurance_company)
        df['act_ins_company'] = normalize_null_like_series(df['act_ins_company'])
        after = df['act_ins_company'].notna().sum()
        print(f"üßπ Cleaned act_ins_company - filtered {before - after} invalid")

    # ‚úÖ dates
    df["date_warranty"] = pd.to_datetime(df["date_warranty"], errors="coerce")
    df["date_expired"] = pd.to_datetime(df["date_expired"], errors="coerce")
    df["date_warranty"] = df["date_warranty"].replace({pd.NaT: None})
    df["date_expired"] = df["date_expired"].replace({pd.NaT: None})
    df['date_warranty'] = pd.to_datetime(df['date_warranty'], errors='coerce').dt.strftime('%Y%m%d').astype('Int64')
    df['date_expired'] = pd.to_datetime(df['date_expired'], errors='coerce').dt.strftime('%Y%m%d').astype('Int64')

    # ‚úÖ province
    def clean_province(province):
        if pd.isna(province) or str(province).strip() == '':
            return None
        province_str = str(province).strip()
        for w in ['‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡∏ï‡∏≥‡∏ö‡∏•', '‡πÄ‡∏Ç‡∏ï', '‡πÄ‡∏°‡∏∑‡∏≠‡∏á', '‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£', '‡∏Å‡∏°.', '‡∏ñ‡∏ô‡∏ô', '‡∏ã‡∏≠‡∏¢', '‡∏´‡∏°‡∏π‡πà', '‡∏ö‡πâ‡∏≤‡∏ô']:
            province_str = province_str.replace(w, '').strip()
        corrections = {
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡∏Å‡∏ó‡∏°': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡∏Å‡∏ó‡∏°.': '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£',
            '‡∏û‡∏±‡∏ó‡∏¢‡∏≤': '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ','‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤': '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏ö‡∏≤‡∏á‡∏ô‡∏≤': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏µ': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏û‡∏£‡∏∞‡∏õ‡∏£‡∏∞‡πÅ‡∏î‡∏á': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏ö‡∏≤‡∏á‡∏ö‡πà‡∏≠': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏ö‡∏≤‡∏á‡πÄ‡∏™‡∏≤‡∏ò‡∏á': '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£'
        }
        for wrong, correct in corrections.items():
            if wrong in province_str:
                return correct
        known = [
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà','‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ','‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ','‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£','‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°','‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤',
            '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô','‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ','‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ','‡∏™‡∏á‡∏Ç‡∏•‡∏≤','‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï','‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢','‡∏•‡∏≥‡∏õ‡∏≤‡∏á','‡∏•‡∏≥‡∏û‡∏π‡∏ô','‡πÅ‡∏û‡∏£‡πà','‡∏ô‡πà‡∏≤‡∏ô','‡∏û‡∏∞‡πÄ‡∏¢‡∏≤',
            '‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô','‡∏ï‡∏≤‡∏Å','‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢','‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å','‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå','‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£','‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£','‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ','‡∏ô‡∏Ñ‡∏£‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡πå','‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ','‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó','‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ','‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤','‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á','‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ','‡∏ô‡∏Ñ‡∏£‡∏ô‡∏≤‡∏¢‡∏Å','‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß','‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ','‡∏ï‡∏£‡∏≤‡∏î',
            '‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤','‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ'
        ]
        for k in known:
            if k in province_str or province_str in k:
                return k
        return None

    if 'delivery_province' in df.columns:
        df['delivery_province'] = df['delivery_province'].apply(clean_province)
        print("üßπ Cleaned delivery_province")

    # ‚úÖ delivery_type
    if 'delivery_type' in df.columns:
        df['delivery_type'] = df['delivery_type'].replace({'nor': 'normal', '‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°': 'normal', '‡∏õ‡∏Å‡∏ï‡∏¥': 'normal'})

    # ‚úÖ insurance_class
    if 'insurance_class' in df.columns:
        valid = ['1', '1+', '2', '2+', '3', '3+', '‡∏û‡∏£‡∏ö']
        before = df['insurance_class'].notna().sum()
        df['insurance_class'] = df['insurance_class'].astype(str).str.strip()
        df['insurance_class'] = df['insurance_class'].apply(lambda x: x if x in valid else None)
        print(f"üßπ Cleaned insurance_class - filtered {before - df['insurance_class'].notna().sum()} invalid")

    # ‚úÖ repair_type
    if 'repair_type' in df.columns:
        def clean_repair_type(val):
            if pd.isna(val) or val is None:
                return None
            s = str(val).strip()
            thai_only = re.sub(r'[^‡∏Å-‡πô]', '', s)
            if len(thai_only) < 2:
                return None
            if thai_only in ['‡∏≠‡∏π‡πà','‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πâ','‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πã']:
                return '‡∏ã‡πà‡∏≠‡∏°‡∏≠‡∏π‡πà'
            elif thai_only in ['‡∏ã‡πà‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏á','‡∏ã‡πà‡∏≠‡∏°‡∏®‡∏π‡∏ô‡∏¢‡πå']:
                return '‡∏ã‡πà‡∏≠‡∏°‡∏´‡πâ‡∏≤‡∏á'
            return thai_only
        before = df['repair_type'].notna().sum()
        df['repair_type'] = df['repair_type'].apply(clean_repair_type)
        print(f"üßπ Cleaned repair_type - filtered {before - df['repair_type'].notna().sum()} invalid")

    # ‚úÖ numeric columns (known list - ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1)
    numeric_columns = [
        "sum_insured","human_coverage_person","human_coverage_atime","property_coverage",
        "deductible","vehicle_damage","deductible_amount","vehicle_theft_fire",
        "vehicle_flood_damage","personal_accident_driver","personal_accident_passengers",
        "medical_coverage","driver_coverage"
    ]
    print("üßπ Removing commas from known numeric columns...")
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(remove_commas_from_numeric)
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("float64")

    # ‚úÖ ems_amount (‡∏û‡∏¥‡πÄ‡∏®‡∏©)
    if 'ems_amount' in df.columns:
        df['ems_amount'] = df['ems_amount'].apply(parse_amount)
        df['ems_amount'] = pd.to_numeric(df['ems_amount'], errors='coerce').fillna(0).round().astype('Int64')
        print(f"üì¶ Cleaned ems_amount - non-null count: {df['ems_amount'].notna().sum()}")

    # ‚úÖ fix outliers
    if 'human_coverage_atime' in df.columns:
        df['human_coverage_atime'] = df['human_coverage_atime'].replace(100000000, 10000000)
    if 'vehicle_damage' in df.columns:
        df['vehicle_damage'] = df['vehicle_damage'].replace(190000050, 1900000)
    if 'vehicle_theft_fire' in df.columns:
        df['vehicle_theft_fire'] = df['vehicle_theft_fire'].replace(190000050, 1900000)
    if 'sum_insured' in df.columns:
        df['sum_insured'] = df['sum_insured'].replace({190000050: 1900000, 250000093: 2500000})

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏•‡∏ö comma/‡πÅ‡∏õ‡∏•‡∏á
    df = auto_clean_numeric_like_columns(
        df,
        exclude={'quotation_num', 'ins_company', 'act_ins_company', 'delivery_type', 'delivery_province', 'insurance_class', 'repair_type'},
        threshold=0.8,
        min_numeric_rows=5
    )

    # ‚úÖ ready for DB/export
    df = df.convert_dtypes()
    df = purge_na_tokens(df)

    print("\nüìä Cleaning completed")

    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí Remaining database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing remaining connections: {str(e)}")

    return df

# -------------------------
# üöö Load
# -------------------------

@op
def load_motor_data(df: pd.DataFrame):
    table_name = "fact_insurance_motor_temp"
    pk_column = "quotation_num"

    df = df[df[pk_column].notna()].copy()
    metadata = MetaData()
    table = Table(table_name, metadata, autoload_with=target_engine)

    today_str = datetime.now().strftime('%Y-%m-%d')

    existing_query = f"SELECT {pk_column} FROM {table_name} WHERE update_at >= '{today_str}'"
    df_existing = execute_query_with_retry(target_engine, existing_query)
    existing_ids = set(df_existing[pk_column]) if not df_existing.empty else set()

    new_rows = df[~df[pk_column].isin(existing_ids)].copy()
    update_rows = df[df[pk_column].isin(existing_ids)].copy()

    print(f"üÜï Insert: {len(new_rows)} rows")
    print(f"üîÑ Update: {len(update_rows)} rows")

    if not new_rows.empty:
        print(f"üîÑ Inserting {len(new_rows)} new records...")
        try:
            with target_engine.begin() as conn:
                conn.execute(table.insert(), new_rows.to_dict(orient="records"))
            print(f"‚úÖ Insert completed successfully")
        except Exception as e:
            print(f"‚ùå Insert failed: {str(e)}")
            raise

    if not update_rows.empty:
        print(f"üîÑ Updating {len(update_rows)} existing records...")
        try:
            with target_engine.begin() as conn:
                for i, record in enumerate(update_rows.to_dict(orient="records")):
                    if i % 100 == 0:
                        print(f"   Progress: {i}/{len(update_rows)} records processed")
                    stmt = pg_insert(table).values(**record)
                    update_dict = {
                        c.name: stmt.excluded[c.name]
                        for c in table.columns
                        if c.name not in [pk_column, 'create_at', 'update_at']
                    }
                    update_dict['update_at'] = datetime.now()
                    stmt = stmt.on_conflict_do_update(
                        index_elements=[pk_column],
                        set_=update_dict
                    )
                    conn.execute(stmt)
            print(f"‚úÖ Update completed successfully")
        except Exception as e:
            print(f"‚ùå Update failed: {str(e)}")
            raise

    print("‚úÖ Insert/update completed.")

    try:
        target_engine.dispose()
        print("üîí Target database connection closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing target connection: {str(e)}")

    try:
        source_engine.dispose()
        task_engine.dispose()
        print("üîí All database connections closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error closing all connections: {str(e)}")

# -------------------------
# üß± Dagster Job
# -------------------------

@job
def fact_insurance_motor_etl():
    load_motor_data(clean_motor_data(extract_motor_data()))

# -------------------------
# ‚ñ∂Ô∏è Main (standalone run)
# -------------------------

if __name__ == "__main__":
    try:
        print("üöÄ Starting fact_insurance_motor ETL process...")
        print("üì• Extracting data from source databases...")
        data_tuple = extract_motor_data()
        print("‚úÖ Data extraction completed")

        print("üßπ Cleaning and transforming data...")
        df_clean = clean_motor_data(data_tuple)
        print("‚úÖ Data cleaning completed")
        print("‚úÖ Cleaned columns:", df_clean.columns)

        # output_path = "fact_insurance_motor.xlsx"
        # df_export = purge_na_tokens(df_clean.copy())
        # df_export.to_excel(output_path, index=False, engine='openpyxl')
        # print(f"üíæ Saved to {output_path}")

        # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏ê‡∏≤‡∏ô (‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°)
        print("üì§ Loading data to target database...")
        load_motor_data(df_clean)
        print("üéâ ETL process completed! Data upserted to fact_insurance_motor.")

    except Exception as e:
        print(f"‚ùå ETL process failed: {str(e)}")
        raise
    finally:
        close_engines()
