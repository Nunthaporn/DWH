from dagster import op, job
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, Table, MetaData, inspect
from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime, timedelta
import re

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ Source: MariaDB
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
)
source_engine_task  = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task"
)
# ‚úÖ Target: PostgreSQL
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance?connect_timeout=60"
)

@op
def extract_order_type_data():
    # now = datetime.now()

    # start_time = now.replace(minute=0, second=0, microsecond=0)
    # end_time = now.replace(minute=59, second=59, microsecond=999999)

    # start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
    # end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')

    start_str = '2025-08-13'
    end_str = '2025-08-31'

    query_plan = f"""
        SELECT quo_num, type_insure, type_work, type_status, type_key, app_type, chanel_key
        FROM fin_system_select_plan
        WHERE datestart BETWEEN '{start_str}' AND '{end_str}'

    """
    query_order = """
        SELECT quo_num, worksend
        FROM fin_order
    """

    df_plan = pd.read_sql(query_plan, source_engine)
    df_order = pd.read_sql(query_order, source_engine_task)

    df_merged = pd.merge(df_plan, df_order, on='quo_num', how='left')

    print("üì¶ df_plan:", df_plan.shape)
    print("üì¶ df_order:", df_order.shape)
    print("üì¶ df_merged:", df_merged.shape)

    return df_merged

@op
def clean_order_type_data(df: pd.DataFrame):
    import re

    # ---------- helpers ----------
    def _norm(s):
        if pd.isna(s):
            return ""
        return str(s).strip()

    def _lower(s):
        return _norm(s).lower()

    BASE_MAP = [
        (r"\bapp\b|application|mobile|‡πÅ‡∏≠‡∏õ", "APP"),
        (r"\bweb\b|website|‡πÄ‡∏ß‡∏ö|‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå", "WEB"),
    ]

    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö subtype ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á
    SUBTYPE_SET = {"B2B", "B2C", "TELE", "THAIPOST", "THAICARE"}

    # default subtype ‡∏ï‡πà‡∏≠‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ï‡πå ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ subtype ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ
    DEFAULT_SUBTYPE_APP = {
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏¥‡∏î": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÄ‡∏ö‡πá‡∏î‡πÄ‡∏ï‡∏•‡πá‡∏î": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏≠‡∏±‡∏Ñ‡∏Ñ‡∏µ‡∏†‡∏±‡∏¢sme": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏≠‡∏±‡∏Ñ‡∏Ñ‡∏µ‡∏†‡∏±‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏•‡∏∏‡πà‡∏°": "B2B",
    }
    DEFAULT_SUBTYPE_WEB = {
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏¥‡∏î": "B2B",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï": "B2B",
    }

    def base_from_type_key(text: str) -> str | None:
        low = _lower(text)
        for pat, label in BASE_MAP:
            if re.search(pat, low):
                return label
        return None

    def normalize_special_subtype(raw: str) -> str:
        """ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏ä‡πà‡∏ô WEB-AFF -> AFF """
        s = _norm(raw).upper()
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢
        s = s.replace("WEB-AFF", "AFF").replace("WEB AFF", "AFF")
        return s

    def extract_subtype(raw: str) -> str | None:
        """
        ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å subtype ‡∏à‡∏≤‡∏Å raw:
        1) ‡πÅ‡∏õ‡∏•‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏© (WEB-AFF -> AFF)
        2) ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ token ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô SUBTYPE_SET
        3) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö (‡∏•‡∏ö‡∏Ñ‡∏≥ BASE ‡∏≠‡∏≠‡∏Å) ‡πÄ‡∏õ‡πá‡∏ô subtype
        """
        s = normalize_special_subtype(raw)
        if not s:
            return None

        tokens = re.split(r"[ \-_/]+", s.upper())
        for tok in tokens:
            t = tok.strip()
            if t in SUBTYPE_SET:
                return t

        # ‡πÉ‡∏ä‡πâ‡∏™‡∏ï‡∏£‡∏¥‡∏á‡∏î‡∏¥‡∏ö‡πÄ‡∏õ‡πá‡∏ô subtype (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ base ‡∏≠‡∏≠‡∏Å)
        s_up = s.upper()
        for _, base_label in BASE_MAP:
            s_up = re.sub(rf"\b{re.escape(base_label)}\b", "", s_up, flags=re.IGNORECASE)
        s_up = re.sub(r"\s+", " ", s_up).strip()
        return s_up if s_up else None

    def derive_base(row) -> str | None:
        # ‚úÖ ‡πÉ‡∏ä‡πâ type_key ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        return base_from_type_key(row.get("type_key", ""))

    def derive_subtype(row) -> str | None:
        # ‚úÖ ‡πÄ‡∏≠‡∏≤‡∏à‡∏≤‡∏Å chanel_key ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≠‡∏¢‡πÉ‡∏ä‡πâ app_type
        ch_val = row.get("chanel_key", "")
        app_val = row.get("app_type", "")
        sub = extract_subtype(ch_val)
        if sub:
            return sub
        sub = extract_subtype(app_val)
        if sub:
            return sub

        # fallback: ‡∏û‡∏ö‡∏Ñ‡∏≥ vif/‡∏ï‡∏£‡∏≠ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô VIF (‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏∏‡∏î SUBTYPE_SET ‡πÑ‡∏´‡∏°? ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Å‡πá‡πÄ‡∏û‡∏¥‡πà‡∏°)
        blob = " ".join([
            _lower(ch_val), _lower(app_val),
            _lower(row.get("type_key", "")),
            _lower(row.get("type_insure", "")),
            _lower(row.get("worksend", "")),
        ])
        if ("vif" in blob) or ("‡∏ï‡∏£‡∏≠" in blob):
            return "VIF" if "VIF" in SUBTYPE_SET else None
        return None

    def default_subtype_by_product(base: str | None, type_insure: str | None) -> str | None:
        if not base:
            return None
        name = _lower(type_insure)
        if base == "APP":
            # map ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏õ‡πá‡∏ô lower ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡∏™‡∏∞‡∏Å‡∏î
            for k, v in DEFAULT_SUBTYPE_APP.items():
                if _lower(k) == name:
                    return v
        if base == "WEB":
            for k, v in DEFAULT_SUBTYPE_WEB.items():
                if _lower(k) == name:
                    return v
        return None

    def parse_channel(row):
        base = derive_base(row)
        subtype = derive_subtype(row)

        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ subtype ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ default ‡∏ï‡πà‡∏≠‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ï‡πå
        if not subtype:
            subtype = default_subtype_by_product(base, row.get("type_insure", ""))

        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ base ‡πÅ‡∏ï‡πà subtype == VIF -> default base = WEB (‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ)
        if not base and subtype == "VIF":
            base = "WEB"

        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ï‡∏≤‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á: APP/WEB + subtype ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ)
        if base in {"APP", "WEB"} and subtype:
            return f"{base} {subtype}"
        if base and not subtype:
            return base
        if subtype and not base:
            return subtype
        return ""

    # ---------- build key_channel ----------
    if "worksend" not in df.columns:
        df["worksend"] = None

    df["key_channel"] = df.apply(parse_channel, axis=1)

    # ‡πÅ‡∏Å‡πâ‡πÄ‡∏Ñ‡∏™‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ "TELE" ‡πÄ‡∏â‡∏¢ ‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô "APP TELE"
    df["key_channel"] = df["key_channel"].apply(lambda x: "APP TELE" if str(x).strip().upper() == "TELE" else x)
    df["key_channel"] = df["key_channel"].apply(lambda x: "APP B2B" if str(x).strip().upper() == "B2B" else x)
    df["key_channel"] = df["key_channel"].apply(lambda x: "WEB B2C" if str(x).strip().upper() == "WEB AFF" else x)
    df["key_channel"] = df["key_channel"].apply(lambda x: "WEB THAICARE" if str(x).strip().upper() == "THAICARE" else x)
    df["key_channel"] = df["key_channel"].apply(lambda x: "WEB ADMIN-B2C" if str(x).strip().upper() == "WEB ADMIN" else x)

    # ‡∏•‡∏ö‡∏Ç‡∏µ‡∏î (-) ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô key_channel
    df["key_channel"] = df["key_channel"].astype(str).str.replace("-", " ", regex=False).str.replace(r"\s+", " ", regex=True).str.strip()

    # ---------- tidy & rename ----------
    obj_cols = df.select_dtypes(include=["object"]).columns
    df[obj_cols] = df[obj_cols].apply(
        lambda s: s.replace(r"^\s*$", np.nan, regex=True)
                  .replace(r"^\s*(nan|NaN)\s*$", np.nan, regex=True)
    )

    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß
    df.drop(columns=["type_key", "app_type"], inplace=True, errors="ignore")

    df.loc[df['order_type'] == 1, 'order_type'] = np.nan

    df.rename(columns={
        "quo_num": "quotation_num",
        "type_insure": "type_insurance",
        "type_work": "order_type",
        "type_status": "check_type",
        "worksend": "work_type",
    }, inplace=True)

    if "quotation_num" in df.columns:
        df.drop_duplicates(subset=["quotation_num"], keep="first", inplace=True)

    df = df.where(pd.notnull(df), None)
    print("üìä Cleaning completed (key_channel mapped)")
    return df

@op
def load_order_type_data(df: pd.DataFrame):
    table_name = 'dim_order_type'
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    with target_engine.connect() as conn:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå quotation_num ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        result = conn.execute(text(f"""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
            AND column_name = 'quotation_num'
        """))
        has_quotation_num = result.fetchone() is not None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö unique constraint ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        result = conn.execute(text(f"""
            SELECT conname, contype
            FROM pg_constraint
            WHERE conrelid = '{table_name}'::regclass
            AND contype = 'u'
        """))
        unique_constraints = result.fetchall()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö primary key
        result = conn.execute(text(f"""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
            AND column_name = 'order_type_id'
        """))
        has_order_type_id = result.fetchone() is not None

    print(f"üîç Debug: has_quotation_num = {has_quotation_num}")
    print(f"üîç Debug: unique_constraints = {unique_constraints}")
    print(f"üîç Debug: has_order_type_id = {has_order_type_id}")

    if has_quotation_num and unique_constraints:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ quotation_num column ‡πÅ‡∏•‡∏∞‡∏°‡∏µ unique constraint (‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡πà‡∏≤)
        pk_column = 'quotation_num'
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ unique constraint ‡∏ö‡∏ô quotation_num ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        has_quotation_unique = any('quotation_num' in str(constraint) for constraint in unique_constraints)
        
        if not has_quotation_unique:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ unique constraint ‡∏ö‡∏ô quotation_num ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
            with target_engine.connect() as conn:
                conn.execute(text(f"""
                    DO $$
                    BEGIN
                        IF NOT EXISTS (SELECT 1 FROM pg_constraint WHERE conname = 'unique_quotation_num') THEN
                            ALTER TABLE {table_name} ADD CONSTRAINT unique_quotation_num UNIQUE ({pk_column});
                        END IF;
                    END $$;
                """))
                conn.commit()

        df = df[~df[pk_column].duplicated(keep='first')].copy()

        # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
        today_str = datetime.now().strftime('%Y-%m-%d')

        # ‚úÖ Load ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å PostgreSQL
        with target_engine.connect() as conn:
            df_existing = pd.read_sql(
                f"SELECT * FROM {table_name} WHERE update_at >= '{today_str}'",
                conn
            )

        df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()

        new_ids = set(df[pk_column]) - set(df_existing[pk_column])
        df_to_insert = df[df[pk_column].isin(new_ids)].copy()

        common_ids = set(df[pk_column]) & set(df_existing[pk_column])
        df_common_new = df[df[pk_column].isin(common_ids)].copy()
        df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()

        merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))

        exclude_columns = [pk_column, 'order_type_id', 'create_at', 'update_at']

        # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì column ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á df ‡πÅ‡∏•‡∏∞ df_existing ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        all_columns = set(df_common_new.columns) & set(df_common_old.columns)
        compare_cols = [
            col for col in all_columns
            if col not in exclude_columns
            and f"{col}_new" in merged.columns
            and f"{col}_old" in merged.columns
        ]

        def is_different(row):
            for col in compare_cols:
                val_new = row.get(f"{col}_new")
                val_old = row.get(f"{col}_old")
                if pd.isna(val_new) and pd.isna(val_old):
                    continue
                if val_new != val_old:
                    return True
            return False

        # Filter rows that have differences
        df_diff = merged[merged.apply(is_different, axis=1)].copy()

        if not df_diff.empty and compare_cols:
            update_cols = [f"{col}_new" for col in compare_cols]
            all_cols = [pk_column] + update_cols

            # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
            existing_cols = [c for c in all_cols if c in df_diff.columns]
            
            if len(existing_cols) > 1:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ pk_column ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô
                df_diff_renamed = df_diff.loc[:, existing_cols].copy()
                # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ column ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
                new_col_names = [pk_column] + [col.replace('_new', '') for col in existing_cols if col != pk_column]
                df_diff_renamed.columns = new_col_names
            else:
                df_diff_renamed = pd.DataFrame()
        else:
            df_diff_renamed = pd.DataFrame()

        print(f"üÜï Insert: {len(df_to_insert)} rows")
        print(f"üîÑ Update: {len(df_diff_renamed)} rows")

        metadata = Table(table_name, MetaData(), autoload_with=target_engine)

        # Insert only the new records
        if not df_to_insert.empty:
            df_to_insert_valid = df_to_insert[df_to_insert[pk_column].notna()].copy()
            dropped = len(df_to_insert) - len(df_to_insert_valid)
            if dropped > 0:
                print(f"‚ö†Ô∏è Skipped {dropped}")
            if not df_to_insert_valid.empty:
                with target_engine.begin() as conn:
                    conn.execute(metadata.insert(), df_to_insert_valid.to_dict(orient='records'))

        # Update only the records where there is a change
        if not df_diff_renamed.empty and compare_cols:
            with target_engine.begin() as conn:
                for record in df_diff_renamed.to_dict(orient='records'):
                    stmt = pg_insert(metadata).values(**record)
                    update_columns = {
                        c.name: stmt.excluded[c.name]
                        for c in metadata.columns
                        if c.name not in [pk_column, 'order_type_id', 'create_at', 'update_at']
                    }
                    # update_at ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    update_columns['update_at'] = datetime.now()
                    stmt = stmt.on_conflict_do_update(
                        index_elements=[pk_column],
                        set_=update_columns
                    )
                    conn.execute(stmt)

    else:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ quotation_num column ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ unique constraint (‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏´‡∏°‡πà) - ‡πÉ‡∏ä‡πâ INSERT ‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        print("üìù No quotation_num column or unique constraint found, using simple INSERT")
        
        metadata = Table(table_name, MetaData(), autoload_with=target_engine)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° timestamp columns ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        if 'create_at' not in df.columns:
            df['create_at'] = pd.Timestamp.now()
        if 'update_at' not in df.columns:
            df['update_at'] = pd.Timestamp.now()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô DataFrame ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        table_columns = [c.name for c in metadata.columns]
        df_columns = [col for col in df.columns if col in table_columns]
        df_filtered = df[df_columns].copy()
        
        # ‡∏•‡∏ö duplicates ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        df_filtered = df_filtered.drop_duplicates()
        
        print(f"üÜï Insert: {len(df_filtered)} rows")
        
        # Insert records
        if not df_filtered.empty:
            with target_engine.begin() as conn:
                conn.execute(metadata.insert(), df_filtered.to_dict(orient='records'))

    print("‚úÖ Insert/update completed.")

@job
def dim_order_type_etl():
    load_order_type_data(clean_order_type_data(extract_order_type_data()))

if __name__ == "__main__":
    df_row = extract_order_type_data()
    # print("‚úÖ Extracted logs:", df_row.shape)

    df_clean = clean_order_type_data((df_row))
    # print("‚úÖ Cleaned columns:", df_clean.columns)

    # output_path = "dim_order_type.xlsx"
    # df_clean.to_excel(output_path, index=False, engine='openpyxl')
    # print(f"üíæ Saved to {output_path}")

    load_order_type_data(df_clean)
    print("üéâ completed! Data upserted to dim_order_type.")