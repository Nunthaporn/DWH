from dagster import op, job
import pandas as pd
import numpy as np
import os
import re
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, text, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert
from datetime import datetime, timedelta

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î env
load_dotenv()

# ‚úÖ DB Connections
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
)

target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
)

@op
def extract_sales_data():
    # now = datetime.now()

    # start_time = now.replace(minute=0, second=0, microsecond=0)
    # end_time = now.replace(minute=59, second=59, microsecond=999999)

    # start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
    # end_str = end_time.strftime('%Y-%m-%d %H:%M:%S') 

    query_main = text("""
        SELECT cuscode, name, rank, user_registered,
            status, fin_new_group, fin_new_mem,
            type_agent, typebuy, user_email, name_store, address, city, district,
            province, province_cur, area_cur, postcode, tel, date_active,
            card_ins_type, file_card_ins, card_ins_type_life, file_card_ins_life
        FROM wp_users
        WHERE user_login NOT IN ('FINTEST-01', 'FIN-TestApp', 'Admin-VIF', 'adminmag_fin', 'FNG00-00001')
        AND name NOT LIKE '%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%'
        AND name NOT LIKE '%tes%'
        AND cuscode NOT LIKE '%FNG%'
        AND cuscode NOT LIKE '%FTR%'
        AND cuscode NOT LIKE '%FIN%'
        AND cuscode NOT LIKE '%FIT%'
        AND name NOT LIKE '%‡∏ó‡∏î‡∏™‡∏≠‡∏ö%'
        AND name NOT LIKE '%test%'
        AND name NOT LIKE '%‡πÄ‡∏ó‡∏™‡∏£‡∏∞‡∏ö‡∏ö%'
        AND name NOT LIKE '%Tes ‡∏£‡∏∞‡∏ö‡∏ö%'
        AND name NOT LIKE '%‡∏ó‡∏î‡πà‡∏ó%'
        AND name NOT LIKE '%‡∏ó‡∏î ‡∏™‡∏≠‡∏ö%'
        AND name NOT LIKE '%‡∏õ‡∏±‡∏ç‡∏ç‡∏ß‡∏±‡∏í‡∏ô‡πå ‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏®‡∏£‡∏µ‡∏ó‡∏≠‡∏á%'
        AND name NOT LIKE '%‡πÄ‡∏≠‡∏Å‡∏®‡∏¥‡∏©‡∏é‡πå ‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏ò‡∏±‡∏ô‡∏¢‡∏ö‡∏π‡∏£‡∏ì‡πå%'
        AND cuscode NOT LIKE '%%@gmail%%'
        AND cuscode NOT LIKE '%%@mail%%'
        AND cuscode NOT LIKE '%%@hotmail%%'
        AND cuscode NOT LIKE '%%@icloud%%'
        AND cuscode NOT LIKE '%%@windowslive%%'
        AND cuscode NOT LIKE '%%@outlook%%'
        AND cuscode NOT LIKE '%%@lcloud%%'
        AND cuscode NOT LIKE '%%@outloot%%'
        AND cuscode NOT LIKE '%%@yahoo%%'
        AND cuscode NOT LIKE '%%TROAgency%%'
        AND cuscode NOT LIKE '%%Direct%%'
        AND cuscode NOT LIKE '%%direct%%'
        AND cuscode NOT LIKE '%%Client%%'
        AND cuscode NOT LIKE '%%Account%%'
        AND cuscode NOT LIKE '%%ADMIN%%'
        AND cuscode NOT LIKE '%%WEB-B2C%%'
        AND cuscode NOT LIKE '%%WEB-B2CQQ%%'
        AND cuscode NOT LIKE '%%Website%%'
        AND cuscode NOT LIKE '%%CUS%%'
        AND cuscode NOT LIKE '%%REGISTER%%'
        AND cuscode NOT LIKE '%FIN-TestApp%'
        AND cuscode NOT LIKE '%FIN-Tester1%'
        AND cuscode NOT LIKE '%FIN-Tester2%';
    """)
                      
    df_main = pd.read_sql(query_main, source_engine)
    # ‡πÅ‡∏õ‡∏•‡∏á NaN string ‡πÄ‡∏õ‡πá‡∏ô None
    df_main = df_main.replace('NaN', None)
    df_main = df_main.replace('nan', None)
    df_main = df_main.replace('NULL', None)
    df_main = df_main.replace('null', None)
    
    # ‡πÅ‡∏õ‡∏•‡∏á pandas NaN ‡πÄ‡∏õ‡πá‡∏ô None
    df_main = df_main.where(pd.notna(df_main), None)
    
    query_career = text("SELECT cuscode, career FROM policy_register")
    df_career = pd.read_sql(query_career, source_engine)
    
    # ‡πÅ‡∏õ‡∏•‡∏á NaN string ‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏ô career data
    df_career = df_career.replace('NaN', None)
    df_career = df_career.replace('nan', None)
    df_career = df_career.replace('NULL', None)
    df_career = df_career.replace('null', None)
    
    # ‡πÅ‡∏õ‡∏•‡∏á pandas NaN ‡πÄ‡∏õ‡πá‡∏ô None
    df_career = df_career.where(pd.notna(df_career), None)

    return pd.merge(df_main, df_career, on='cuscode', how='left')

@op
def clean_sales_data(df: pd.DataFrame):
    # ‡πÅ‡∏õ‡∏•‡∏á NaN string ‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    df = df.replace('NaN', None)
    df = df.replace('nan', None)
    df = df.replace('NULL', None)
    df = df.replace('null', None)
    
    # ‡πÅ‡∏õ‡∏•‡∏á pandas NaN ‡πÄ‡∏õ‡πá‡∏ô None
    df = df.where(pd.notna(df), None)
    
    # Combine region columns
    def combine_columns(a, b):
        a_str = str(a).strip() if a is not None else ''
        b_str = str(b).strip() if b is not None else ''
        if a_str == '' and b_str == '':
            return ''
        elif a_str == '':
            return b_str
        elif b_str == '':
            return a_str
        elif a_str == b_str:
            return a_str
        else:
            return f"{a_str} + {b_str}"

    df['agent_region'] = df.apply(lambda row: combine_columns(row['fin_new_group'], row['fin_new_mem']), axis=1)
    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á row ‡∏ó‡∏µ‡πà agent_region = 'TEST' ‡∏≠‡∏≠‡∏Å
    df = df[df['agent_region'] != 'TEST']
    df = df.drop(columns=['fin_new_group', 'fin_new_mem'])

    # Clean date_active and status_agent
    df['date_active'] = pd.to_datetime(df['date_active'], errors='coerce')
    now = pd.Timestamp.now()
    one_month_ago = now - pd.DateOffset(months=1)

    def check_condition(row):
        if row['status'] == 'defect':
            return 'inactive'
        elif pd.notna(row['date_active']) and row['date_active'] < one_month_ago:
            return 'inactive'
        else:
            return 'active'

    df['status_agent'] = df.apply(check_condition, axis=1)
    df = df.drop(columns=['status', 'date_active'])

    df['defect_status'] = np.where(df['cuscode'].str.contains('-defect', na=False), 'defect', None)

    # Rename columns
    rename_columns = {
        "cuscode": "agent_id",
        "name": "agent_name",
        "rank": "agent_rank",
        "user_registered": "hire_date",
        "status_agent": "status_agent",
        "type_agent": "type_agent",
        "typebuy": "is_experienced",
        "user_email": "agent_email",
        "name_store": "store_name",
        "address": "agent_address",
        "city": "subdistrict",
        "district": "district",
        "province": "province",
        "province_cur": "current_province",
        "area_cur": "current_area",
        "postcode": "zipcode",
        "tel": "mobile_number",
        "career": "job",
        "agent_region": "agent_region",
        "defect_status": "defect_status",
        "card_ins_type": "card_ins_type",
        "file_card_ins": "file_card_ins",
        "card_ins_type_life": "card_ins_type_life",
        "file_card_ins_life": "file_card_ins_life"
    }
    df = df.rename(columns=rename_columns)
    df['defect_status'] = np.where(df['agent_id'].str.contains('-defect', na=False), 'defect', None)

    # Clean fields
    df['card_ins_type_life'] = df['card_ins_type_life'].apply(lambda x: 'B' if isinstance(x, str) and '‡πÅ‡∏ó‡∏ô' in x else x)
    df['is_experienced_fix'] = df['is_experienced'].apply(lambda x: '‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' if str(x).strip().lower() == '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' else '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢')
    df = df.drop(columns=['is_experienced'])
    df.rename(columns={'is_experienced_fix': 'is_experienced'}, inplace=True)

    valid_rank = [str(i) for i in range(1, 11)]
    df.loc[~df['agent_rank'].isin(valid_rank), 'agent_rank'] = None

    df['agent_address_cleaned'] = df['agent_address'].apply(lambda addr: re.sub(r'(‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà|‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô|‡∏ã‡∏≠‡∏¢|‡∏ñ‡∏ô‡∏ô)[\s\-]*', '', str(addr)).strip())
    df = df.drop(columns=['agent_address'])
    df.rename(columns={'agent_address_cleaned': 'agent_address'}, inplace=True)

    df['mobile_number'] = df['mobile_number'].str.replace(r'[^0-9]', '', regex=True)
    # ‡πÅ‡∏õ‡∏•‡∏á empty string ‡πÄ‡∏õ‡πá‡∏ô None
    df = df.replace(r'^\s*$', None, regex=True)

    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None
    df['non_empty_count'] = df.notna().sum(axis=1)

    valid_mask = df['agent_id'].astype(str).str.strip().ne('') & df['agent_id'].notna() & (df['agent_id'] != None)
    df_with_id = df[valid_mask]
    df_without_id = df[~valid_mask]
    df_with_id_cleaned = df_with_id.sort_values('non_empty_count', ascending=False).drop_duplicates(subset='agent_id', keep='first')
    df_cleaned = pd.concat([df_with_id_cleaned, df_without_id], ignore_index=True)
    df_cleaned = df_cleaned.drop(columns=['non_empty_count'])
    df_cleaned.columns = df_cleaned.columns.str.lower()

    df_cleaned["hire_date"] = pd.to_datetime(df_cleaned["hire_date"], errors='coerce')
    df_cleaned["hire_date"] = df_cleaned["hire_date"].dt.strftime('%Y%m%d')
    df_cleaned["hire_date"] = df_cleaned["hire_date"].where(df_cleaned["hire_date"].notna(), None)
    df_cleaned["hire_date"] = df_cleaned["hire_date"].astype('Int64')

    df_cleaned["zipcode"] = df_cleaned["zipcode"].where(df_cleaned["zipcode"].str.len() == 5, None)
    df_cleaned["agent_name"] = df_cleaned["agent_name"].str.lstrip()
    df_cleaned["is_experienced"] = df_cleaned["is_experienced"].apply(lambda x: 'no' if str(x).strip().lower() == '‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≤‡∏¢' else 'yes')
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î job/career column - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÅ‡∏•‡∏∞ None ‡πÄ‡∏õ‡πá‡∏ô None
    df_cleaned["job"] = df_cleaned["job"].where(pd.notna(df_cleaned["job"]), None)
    df_cleaned["job"] = df_cleaned["job"].replace('NaN', None)
    df_cleaned["job"] = df_cleaned["job"].replace('nan', None)
    df_cleaned["job"] = df_cleaned["job"].replace('NULL', None)
    df_cleaned["job"] = df_cleaned["job"].replace('null', None)

    # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô "Others" ‡πÄ‡∏õ‡πá‡∏ô "‡∏≠‡∏∑‡πà‡∏ô‡πÜ" ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå province
    df_cleaned["province"] = df_cleaned["province"].replace("Others", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ")

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î agent_email - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö email ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    def clean_email(email):
        if email is None or email == '' or str(email).strip() == '':
            return None
        
        email_str = str(email).strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        thai_chars = re.findall(r'[‡∏Å-‡πô]', email_str)
        if thai_chars:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö email ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if re.match(email_pattern, email_str):
            return email_str.lower()  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
        else:
            return None
    
    df_cleaned["agent_email"] = df_cleaned["agent_email"].apply(clean_email)

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î agent_name - ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏™‡∏£‡∏∞‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠
    def clean_agent_name(name):
        if name is None or name == '' or str(name).strip() == '':
            return None
        
        name_str = str(name).strip()
        
        # ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ (‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏™‡∏£‡∏∞‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏£‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if name_str.startswith(('‡∏¥', '‡∏µ', '‡∏∂', '‡∏∑', '‡∏∏', '‡∏π', '‡πà', '‡πâ', '‡πä', '‡πã')): 
            # ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            cleaned_name = re.sub(r'^[‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πà‡πâ‡πä‡πã]+', '', name_str)
        else:
            cleaned_name = name_str
        
        # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        cleaned_name = re.sub(r'[^\u0E00-\u0E7F\u0020\u0041-\u005A\u0061-\u007A]', '', cleaned_name)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        cleaned_name = re.sub(r'\s+', ' ', cleaned_name)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
        cleaned_name = cleaned_name.strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        if len(cleaned_name) < 2:
            return None
        
        return cleaned_name

    df_cleaned["agent_name"] = df_cleaned["agent_name"].apply(clean_agent_name)

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î store_name - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏£‡πâ‡∏≤‡∏ô
    def clean_store_name(store_name):
        if store_name is None or store_name == '' or str(store_name).strip() == '':
            return None
        
        store_str = str(store_name).strip()
        
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏£‡πâ‡∏≤‡∏ô" ‡∏´‡∏£‡∏∑‡∏≠ "shop" ‡∏´‡∏£‡∏∑‡∏≠ "store" ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
        if any(keyword in store_str.lower() for keyword in ['‡∏£‡πâ‡∏≤‡∏ô', 'shop', 'store', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', 'company']):
            return store_str
        else:
            return None
    
    df_cleaned["store_name"] = df_cleaned["store_name"].apply(clean_store_name)

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
    def check_thai_text(text):
        if text is None or text == '' or str(text).strip() == '':
            return None
        
        text_str = str(text).strip()
        
        # ‡∏•‡∏ö space ‡πÅ‡∏•‡∏∞ ] ‡∏´‡∏£‡∏∑‡∏≠ * ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        cleaned_text = re.sub(r'^[\s\]\*]+', '', text_str)
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        unwanted_patterns = [
            r'^\d+$',    # ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏•‡πâ‡∏ß‡∏ô‡πÜ
            r'^[A-Za-z\s]+$',  # ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏•‡πâ‡∏ß‡∏ô‡πÜ
        ]
        
        for pattern in unwanted_patterns:
            if re.match(pattern, cleaned_text):
                return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        thai_chars = re.findall(r'[‡∏Å-‡πô]', cleaned_text)
        if thai_chars:
            return cleaned_text
        else:
            return None
    
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
    location_columns = ['subdistrict', 'district', 'province', 'current_province', 'current_area']
    for col in location_columns:
        if col in df_cleaned.columns:
            df_cleaned[col] = df_cleaned[col].apply(check_thai_text)

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î zipcode - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 5 ‡∏´‡∏•‡∏±‡∏Å
    def clean_zipcode(zipcode):
        if zipcode is None or zipcode == '' or str(zipcode).strip() == '':
            return None
        
        zipcode_str = str(zipcode).strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 5 ‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if re.match(r'^\d{5}$', zipcode_str):
            return zipcode_str
        else:
            return None
    
    df_cleaned["zipcode"] = df_cleaned["zipcode"].apply(clean_zipcode)

    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ card_ins_type ‡πÄ‡∏õ‡πá‡∏ô "‡∏ç‡∏≤‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤"
    df_cleaned = df_cleaned[df_cleaned["card_ins_type"] != "‡∏ç‡∏≤‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤"]

    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î agent_address - ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤, -, :, . ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©
    def clean_address(address):
        if address is None or address == '' or str(address).strip() == '':
            return None
        
        address_str = str(address).strip()
        
        # ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏™‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á)
        # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ -, :, . ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©
        # ‡πÉ‡∏ä‡πâ regex ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏£‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        cleaned_address = re.sub(r'^[\u0E30-\u0E3A\u0E47-\u0E4E]+', '', address_str)  # ‡∏•‡∏ö‡∏™‡∏£‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        cleaned_address = re.sub(r'[-:.,]', '', cleaned_address)  # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©
        
        # ‚úÖ ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "undefined" (‡πÑ‡∏°‡πà‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å-‡πÉ‡∏´‡∏ç‡πà)
        cleaned_address = re.sub(r'\bundefined\b', '', cleaned_address, flags=re.IGNORECASE)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        cleaned_address = re.sub(r'\s+', ' ', cleaned_address)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
        cleaned_address = cleaned_address.strip()
        
        # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÉ‡∏´‡πâ return None
        if cleaned_address == '':
            return None
        
        return cleaned_address
    
    df_cleaned["agent_address"] = df_cleaned["agent_address"].apply(clean_address)
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î agent_address column - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÅ‡∏•‡∏∞ None ‡πÄ‡∏õ‡πá‡∏ô None
    df_cleaned["agent_address"] = df_cleaned["agent_address"].where(pd.notna(df_cleaned["agent_address"]), None)
    df_cleaned["agent_address"] = df_cleaned["agent_address"].replace('NaN', None)
    df_cleaned["agent_address"] = df_cleaned["agent_address"].replace('None', None)
    df_cleaned["agent_address"] = df_cleaned["agent_address"].replace('NULL', None)

    # ‚úÖ ‡∏•‡∏ö space ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    def clean_leading_spaces(text):
        if text is None or text == '' or str(text).strip() == '':
            return None
        
        text_str = str(text).strip()
        # ‡∏•‡∏ö space ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        cleaned_text = re.sub(r'^\s+', '', text_str)
        
        return cleaned_text if cleaned_text != '' else None
    
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    for col in df_cleaned.columns:
        if df_cleaned[col].dtype == 'object':  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            df_cleaned[col] = df_cleaned[col].apply(clean_leading_spaces)

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ - ‡πÅ‡∏õ‡∏•‡∏á pandas NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    for col in df_cleaned.columns:
        if df_cleaned[col].dtype == 'object':
            df_cleaned[col] = df_cleaned[col].where(pd.notna(df_cleaned[col]), None)
            df_cleaned[col] = df_cleaned[col].replace('NaN', None)
            df_cleaned[col] = df_cleaned[col].replace('nan', None)
            df_cleaned[col] = df_cleaned[col].replace('NULL', None)
            df_cleaned[col] = df_cleaned[col].replace('null', None)
    
    print("\nüìä Cleaning completed")
    print(f"üìä Final data shape: {df_cleaned.shape}")
    print(f"üìä Final NaN/None count: {df_cleaned.isna().sum().sum()}")

    return df_cleaned

@op
def load_sales_data(df: pd.DataFrame):
    table_name = 'dim_sales'
    pk_column = 'agent_id'
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô load - ‡πÅ‡∏õ‡∏•‡∏á pandas NaN ‡πÄ‡∏õ‡πá‡∏ô None
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].where(pd.notna(df[col]), None)
            df[col] = df[col].replace('NaN', None)
            df[col] = df[col].replace('nan', None)
            df[col] = df[col].replace('NULL', None)
            df[col] = df[col].replace('null', None)
    
    print(f"üîç Before load - NaN/None count: {df.isna().sum().sum()}")
    print(f"üîç Before load - job column NaN/None count: {df['job'].isna().sum()}")
    print(f"üîç Before load - agent_address column NaN/None count: {df['agent_address'].isna().sum()}")

    df = df[~df[pk_column].duplicated(keep='first')].copy()

    # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
    today_str = datetime.now().strftime('%Y-%m-%d')

    # ‚úÖ Load ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å PostgreSQL
    with target_engine.connect() as conn:
        df_existing = pd.read_sql(
            f"SELECT * FROM {table_name} WHERE update_at >= '{today_str}'",
            conn
        )

    df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()

    new_ids = set(df[pk_column]) - set(df_existing[pk_column])
    df_to_insert = df[df[pk_column].isin(new_ids)].copy()

    common_ids = set(df[pk_column]) & set(df_existing[pk_column])
    df_common_new = df[df[pk_column].isin(common_ids)].copy()
    df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()

    # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô
    print(f"üîç Total new data: {len(df)}")
    print(f"üîç Existing data today: {len(df_existing)}")
    print(f"üîç New IDs to insert: {len(new_ids)}")
    print(f"üîç Common IDs to compare: {len(common_ids)}")
    print(f"üîç Data to insert: {len(df_to_insert)}")
    print(f"üîç Common new data: {len(df_common_new)}")
    print(f"üîç Common old data: {len(df_common_old)}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df_common_new.empty or df_common_old.empty:
        print("‚ÑπÔ∏è No common data to compare, skipping update logic")
        merged = pd.DataFrame()
    else:
        merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ merged DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if merged.empty:
        print("‚ÑπÔ∏è No merged data, skipping comparison")
        df_diff = pd.DataFrame()
        df_diff_renamed = pd.DataFrame()
    else:
        # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô merged DataFrame
        print(f"üîç Merged columns: {list(merged.columns)}")
        print(f"üîç New data columns: {list(df.columns)}")

    exclude_columns = [pk_column, 'id_contact', 'create_at', 'update_at']
    compare_cols = [
        col for col in df.columns
        if col not in exclude_columns
        and f"{col}_new" in merged.columns
        and f"{col}_old" in merged.columns
    ]

    # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    print(f"üîç Compare columns: {compare_cols}")
        
    # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df ‡πÅ‡∏•‡∏∞ merged
    print(f"üîç df columns: {list(df.columns)}")
    print(f"üîç merged columns with _new suffix: {[col for col in merged.columns if col.endswith('_new')]}") 
    print(f"üîç merged columns with _old suffix: {[col for col in merged.columns if col.endswith('_old')]}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not compare_cols:
        print("‚ö†Ô∏è No columns to compare, skipping update")
        df_diff_renamed = pd.DataFrame()
    else:
        def is_different(row):
            for col in compare_cols:
                val_new = row.get(f"{col}_new")
                val_old = row.get(f"{col}_old")
                if val_new is None and val_old is None:
                    continue
                elif val_new is None or val_old is None:
                    return True
                elif val_new != val_old:
                    return True
            return False

        df_diff = merged[merged.apply(is_different, axis=1)].copy()
            
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if df_diff.empty:
            print("‚ÑπÔ∏è No differences found, skipping update")
            df_diff_renamed = pd.DataFrame()
        else:
            update_cols = [f"{col}_new" for col in compare_cols]
            all_cols = [pk_column] + update_cols
                
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_diff ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            missing_cols = [col for col in all_cols if col not in df_diff.columns]
            if missing_cols:
                print(f"‚ö†Ô∏è Missing columns in df_diff: {missing_cols}")
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                available_cols = [col for col in all_cols if col in df_diff.columns]
                df_diff_renamed = df_diff[available_cols].copy()
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Å‡∏•‡∏±‡∏ö
                available_compare_cols = [col.replace('_new', '') for col in available_cols if col != pk_column]
                df_diff_renamed.columns = [pk_column] + available_compare_cols
            else:
                df_diff_renamed = df_diff[all_cols].copy()
            df_diff_renamed.columns = [pk_column] + compare_cols

    print(f"üÜï Insert: {len(df_to_insert)} rows")
    print(f"üîÑ Update: {len(df_diff_renamed)} rows")

    metadata_table = Table(table_name, MetaData(), autoload_with=target_engine)

    def chunk_dataframe(df, chunk_size=500):
        for i in range(0, len(df), chunk_size):
            yield df.iloc[i:i + chunk_size]

    # ‚úÖ Insert batch (‡πÉ‡∏ä‡πâ UPSERT ‡πÅ‡∏ó‡∏ô INSERT)
    if not df_to_insert.empty:
        # ‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÉ‡∏ô df_to_insert
        df_to_insert = df_to_insert[~df_to_insert[pk_column].duplicated(keep='first')].copy()
        print(f"üîç After removing duplicates in insert data: {len(df_to_insert)}")
        
        df_to_insert_valid = df_to_insert[df_to_insert[pk_column].notna() & (df_to_insert[pk_column] != None)].copy()
        dropped = len(df_to_insert) - len(df_to_insert_valid)
        if dropped > 0:
            print(f"‚ö†Ô∏è Skipped {dropped} rows")
        if not df_to_insert_valid.empty:
            with target_engine.begin() as conn:
                for batch_df in chunk_dataframe(df_to_insert_valid):
                    stmt = pg_insert(metadata_table).values(batch_df.to_dict(orient="records"))
                    valid_column_names = [c.name for c in metadata_table.columns]
                    update_columns = {
                        c: stmt.excluded[c]
                        for c in valid_column_names
                        if c != pk_column and c in batch_df.columns
                    }
                    stmt = stmt.on_conflict_do_update(
                        index_elements=[pk_column],
                        set_=update_columns
                    )
                    conn.execute(stmt)

    # ‚úÖ Update batch
    if not df_diff_renamed.empty:
        with target_engine.begin() as conn:
            for batch_df in chunk_dataframe(df_diff_renamed):
                stmt = pg_insert(metadata_table).values(batch_df.to_dict(orient="records"))
                valid_column_names = [c.name for c in metadata_table.columns]
                update_columns = {
                    c.name: stmt.excluded[c.name]
                    for c in metadata_table.columns
                    if c.name not in pk_column + ['id_contact', 'create_at', 'update_at']
                }
                # update_at ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                update_columns['update_at'] = datetime.now()
                stmt = stmt.on_conflict_do_update(
                    index_elements=[pk_column],
                    set_=update_columns
                )
                conn.execute(stmt)

    print("‚úÖ Insert/update completed.")

@job
def dim_sales_etl():
    load_sales_data(clean_sales_data(extract_sales_data()))


if __name__ == "__main__":
    df_raw = extract_sales_data()
    print("‚úÖ Extracted logs:", df_raw.shape)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NaN ‡πÅ‡∏•‡∏∞ None
    print("üîç Checking for NaN/None values in raw data...")
    nan_count = df_raw.isna().sum().sum()
    print(f"   - Total NaN/None values: {nan_count}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô string 'NaN', 'nan', 'NULL', 'null'
    string_nan_count = 0
    for col in df_raw.columns:
        if df_raw[col].dtype == 'object':
            string_nan_count += (df_raw[col] == 'NaN').sum()
            string_nan_count += (df_raw[col] == 'nan').sum()
            string_nan_count += (df_raw[col] == 'NULL').sum()
            string_nan_count += (df_raw[col] == 'null').sum()
    
    print(f"   - String NaN/NULL values: {string_nan_count}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None object
    none_count = 0
    for col in df_raw.columns:
        if df_raw[col].dtype == 'object':
            none_count += (df_raw[col] == None).sum()
    
    print(f"   - None object values: {none_count}")

    df_clean = clean_sales_data((df_raw))
    print("‚úÖ Cleaned columns:", df_clean.columns)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NaN ‡πÅ‡∏•‡∏∞ None ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    print("üîç Checking for NaN/None values in cleaned data...")
    nan_count_clean = df_clean.isna().sum().sum()
    print(f"   - Total NaN/None values: {nan_count_clean}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô string 'NaN', 'nan', 'NULL', 'null' ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    string_nan_count_clean = 0
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            string_nan_count_clean += (df_clean[col] == 'NaN').sum()
            string_nan_count_clean += (df_clean[col] == 'nan').sum()
            string_nan_count_clean += (df_clean[col] == 'NULL').sum()
            string_nan_count_clean += (df_clean[col] == 'null').sum()
    
    print(f"   - String NaN/NULL values: {string_nan_count_clean}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None object ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    none_count_clean = 0
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            none_count_clean += (df_clean[col] == None).sum()
    
    print(f"   - None object values: {none_count_clean}")

    # output_path = "dim_sales.xlsx"
    # df_clean.to_excel(output_path, index=False, engine='openpyxl')
    # print(f"üíæ Saved to {output_path}")

    load_sales_data(df_clean)
    print("üéâ completed! Data upserted to dim_sales.")


