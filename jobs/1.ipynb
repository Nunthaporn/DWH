{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "113f0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagster import op, job\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.dialects.postgresql import insert as pg_insert\n",
    "from sqlalchemy import create_engine, MetaData, Table, inspect, text\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ‚úÖ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ DB source (MariaDB) - ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÅ‡∏•‡∏∞ connection pool\n",
    "source_engine = create_engine(\n",
    "    f\"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance\",\n",
    "    pool_size=10,\n",
    "    max_overflow=20,\n",
    "    pool_timeout=30,\n",
    "    pool_recycle=3600,\n",
    "    connect_args={\n",
    "        'connect_timeout': 60,\n",
    "        'read_timeout': 300,\n",
    "        'write_timeout': 300\n",
    "    }\n",
    ")\n",
    "source_engine_task = create_engine(\n",
    "    f\"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task\",\n",
    "    pool_size=10,\n",
    "    max_overflow=20,\n",
    "    pool_timeout=30,\n",
    "    pool_recycle=3600,\n",
    "    connect_args={\n",
    "        'connect_timeout': 60,\n",
    "        'read_timeout': 300,\n",
    "        'write_timeout': 300\n",
    "    }\n",
    ")\n",
    "\n",
    "# ‚úÖ DB target (PostgreSQL) - ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÅ‡∏•‡∏∞ connection pool\n",
    "target_engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance\",\n",
    "    pool_size=10,\n",
    "    max_overflow=20,\n",
    "    pool_timeout=30,\n",
    "    pool_recycle=3600,\n",
    "    connect_args={\n",
    "        'connect_timeout': 60,\n",
    "        'options': '-c statement_timeout=300000'  # 5 minutes timeout\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed392d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sales_quotation_data():\n",
    "    \"\"\"Extract data from source databases\"\"\"\n",
    "    try:\n",
    "        logger.info(\"üì¶ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å source databases...\")\n",
    "        \n",
    "        df_plan = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, type_insure, datestart, id_government_officer, status_gpf, quo_num_old,\n",
    "                   status AS status_fssp, type_car, chanel_key, id_cus  \n",
    "            FROM fin_system_select_plan \n",
    "            WHERE datestart BETWEEN '2025-01-01' AND '2025-08-31'\n",
    "                AND id_cus NOT LIKE '%%FIN-TestApp%%'\n",
    "                AND id_cus NOT LIKE '%%FIN-TestApp3%%'\n",
    "                AND id_cus NOT LIKE '%%FIN-TestApp2%%'\n",
    "                AND id_cus NOT LIKE '%%FIN-TestApp-2025%%'\n",
    "                AND id_cus NOT LIKE '%%FIN-Tester1%%'\n",
    "                AND id_cus NOT LIKE '%%FIN-Tester2%%'\n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        df_order = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, order_number, chanel, datekey, status AS status_fo\n",
    "            FROM fin_order\n",
    "            WHERE quo_num IS NOT NULL\n",
    "        \"\"\", source_engine_task)\n",
    "\n",
    "        df_pay = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, datestart, numpay, show_price_ins, show_price_prb, show_price_total,\n",
    "                   show_price_check, show_price_service, show_price_taxcar, show_price_fine,\n",
    "                   show_price_addon, show_price_payment, distax, show_ems_price, show_discount_ins,\n",
    "                   discount_mkt, discount_government, discount_government_fin,\n",
    "                   discount_government_ins, coupon_addon, status AS status_fsp\n",
    "            FROM fin_system_pay \n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        df_risk = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, type \n",
    "            FROM fin_detail_plan_risk  \n",
    "            WHERE type = '‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î'\n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        df_pa = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, special_package \n",
    "            FROM fin_detail_plan_pa  \n",
    "            WHERE special_package = 'CHILD'\n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        df_health = pd.read_sql(\"\"\"\n",
    "            SELECT quo_num, special_package \n",
    "            FROM fin_detail_plan_health  \n",
    "            WHERE special_package = 'CHILD'\n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        df_wp = pd.read_sql(\"\"\"\n",
    "            SELECT cuscode as id_cus, display_permission\n",
    "            FROM wp_users \n",
    "            WHERE display_permission IN ('‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô', '‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô')\n",
    "                AND cuscode NOT LIKE '%%FIN-TestApp%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-TestApp3%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-TestApp2%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-TestApp-2025%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-TestApp%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-Tester1%%'\n",
    "                AND cuscode NOT LIKE '%%FIN-Tester2%%';\n",
    "        \"\"\", source_engine)\n",
    "\n",
    "        logger.info(f\"üì¶ Shapes: plan={df_plan.shape}, order={df_order.shape}, pay={df_pay.shape}, risk={df_risk.shape}, pa={df_pa.shape}, health={df_health.shape}, wp={df_wp.shape}\")\n",
    "        return df_plan, df_order, df_pay, df_risk, df_pa, df_health, df_wp\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16694456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sales_quotation_data(inputs):\n",
    "    try:\n",
    "        df_plan, df_order, df_pay, df_risk, df_pa, df_health, df_wp = inputs\n",
    "        logger.info(\"üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "\n",
    "        # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÉ‡∏ô df_pay\n",
    "        df_pay['datestart'] = pd.to_datetime(df_pay['datestart'], errors='coerce')\n",
    "        df_pay = df_pay.sort_values('datestart').drop_duplicates(subset='quo_num', keep='last')\n",
    "\n",
    "        # ‚úÖ Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        df_merged = df_plan.merge(df_order, on='quo_num', how='left')\n",
    "        df_merged = df_merged.merge(df_pay, on='quo_num', how='left', suffixes=('', '_pay'))\n",
    "        df_merged = df_merged.merge(df_risk, on='quo_num', how='left', suffixes=('', '_risk'))\n",
    "        df_merged = df_merged.merge(df_pa, on='quo_num', how='left', suffixes=('', '_pa'))\n",
    "        df_merged = df_merged.merge(df_health, on='quo_num', how='left', suffixes=('', '_health'))\n",
    "        df_merged = df_merged.merge(df_wp, on='id_cus', how='left')\n",
    "\n",
    "        logger.info(f\"üìä Shape after merge: {df_merged.shape}\")\n",
    "\n",
    "        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ null ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None\n",
    "        df_merged.replace(['nan', 'NaN', 'null', '', 'NULL'], np.nan, inplace=True)\n",
    "        df_merged.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "        df_merged = df_merged.where(pd.notnull(df_merged), None)\n",
    "\n",
    "        # ‚úÖ Rename columns ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
    "        col_map = {\n",
    "            'quo_num': 'quotation_num',\n",
    "            'datestart': 'quotation_date',\n",
    "            'datestart_pay': 'transaction_date',\n",
    "            'datekey': 'order_time',\n",
    "            'type_insure': 'type_insurance',\n",
    "            'id_government_officer': 'rights_government',\n",
    "            'status_gpf': 'goverment_type',\n",
    "            'quo_num_old': 'quotation_num_old',\n",
    "            'numpay': 'installment_number',\n",
    "            'show_price_ins': 'ins_amount',\n",
    "            'show_price_prb': 'prb_amount',\n",
    "            'show_price_total': 'total_amount',\n",
    "            'show_price_check': 'show_price_check',\n",
    "            'show_price_service': 'service_price',\n",
    "            'show_price_taxcar': 'tax_car_price',\n",
    "            'show_price_fine': 'overdue_fine_price',\n",
    "            'show_price_addon': 'price_addon',\n",
    "            'show_price_payment': 'payment_amount',\n",
    "            'distax': 'tax_amount',\n",
    "            'show_ems_price': 'ems_amount',\n",
    "            'show_discount_ins': 'ins_discount',\n",
    "            'discount_mkt': 'mkt_discount',\n",
    "            'discount_government': 'goverment_discount',\n",
    "            'discount_government_fin': 'fin_goverment_discount',\n",
    "            'discount_government_ins': 'ins_goverment_discount',\n",
    "            'coupon_addon': 'discount_addon',\n",
    "            'chanel': 'contact_channel'\n",
    "        }\n",
    "        df_merged.rename(columns=col_map, inplace=True)\n",
    "\n",
    "        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏ï‡∏¥‡∏° default\n",
    "        for col in ['quotation_date', 'transaction_date', 'order_time']:\n",
    "            if col in df_merged.columns:\n",
    "                df_merged[col] = pd.to_datetime(df_merged[col], errors='coerce').dt.strftime('%Y%m%d')\n",
    "                df_merged[col] = df_merged[col].where(pd.notnull(df_merged[col]), None)\n",
    "\n",
    "        # ‚úÖ ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ transaction_date\n",
    "        if 'transaction_date' in df_merged.columns:\n",
    "            before_drop = len(df_merged)\n",
    "            df_merged = df_merged[df_merged['transaction_date'].notna()].copy()\n",
    "            after_drop = len(df_merged)\n",
    "            print(f\"üßπ ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ transaction_date: {before_drop - after_drop} ‡πÅ‡∏ñ‡∏ß‡∏ñ‡∏π‡∏Å‡∏•‡∏ö\")\n",
    "\n",
    "        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á sale_team\n",
    "        def assign_sale_team(row):\n",
    "            id_cus = str(row.get('id_cus') or '')\n",
    "            type_insurance = str(row.get('type_insurance') or '').strip().lower()\n",
    "            type_car = str(row.get('type_car') or '').strip().lower()\n",
    "            chanel_key = str(row.get('chanel_key') or '').strip()\n",
    "            special_package = str(row.get('special_package') or '').strip().upper()\n",
    "            special_package_health = str(row.get('special_package_health') or '').strip().upper()\n",
    "\n",
    "            if id_cus.startswith('FTR'):\n",
    "                return 'Telesales'\n",
    "            if type_car == 'fleet':\n",
    "                return 'fleet'\n",
    "            if type_car == '‡∏ï‡∏∞‡∏Å‡∏≤‡∏ü‡∏∏‡∏•':\n",
    "                return '‡∏ï‡∏∞‡∏Å‡∏≤‡∏ü‡∏∏‡∏•'\n",
    "            if type_insurance == '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏£‡∏ñ':\n",
    "                return 'Motor agency'\n",
    "            if type_insurance == '‡∏ï‡∏£‡∏≠':\n",
    "                return '‡∏ï‡∏£‡∏≠'\n",
    "            if chanel_key == 'CHILD':\n",
    "                return '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÄ‡∏î‡πá‡∏Å'\n",
    "            if chanel_key in ['‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô', '‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô']:\n",
    "                return '‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô'\n",
    "            if chanel_key == 'WEB-SUBBROKER':\n",
    "                return 'Subbroker'\n",
    "            if str(row.get('type') or '').strip() == '‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î':\n",
    "                return '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î'\n",
    "            if special_package == 'CHILD' or special_package_health == 'CHILD':\n",
    "                return '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡πÄ‡∏î‡πá‡∏Å'\n",
    "            if row.get('display_permission') in ['‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ü‡∏¥‡∏ô']:\n",
    "                return '‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô'\n",
    "            if not type_insurance and not type_car:\n",
    "                return 'N/A'\n",
    "            return 'Non Motor'\n",
    "\n",
    "        df_merged['sale_team'] = df_merged.apply(assign_sale_team, axis=1)\n",
    "\n",
    "        # ‚úÖ ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "        df_merged.drop(columns=[\n",
    "            'id_cus', 'type_car', 'chanel_key', 'special_package',\n",
    "            'special_package_health', 'type', 'display_permission'\n",
    "        ], errors='ignore', inplace=True)\n",
    "\n",
    "        # ‚úÖ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤ installment_number ‡πÅ‡∏ö‡∏ö vectorized\n",
    "        if 'installment_number' in df_merged.columns:\n",
    "            installment_mapping = {'0': '1', '03': '3', '06': '6', '08': '8'}\n",
    "            df_merged['installment_number'] = df_merged['installment_number'].replace(installment_mapping)\n",
    "\n",
    "        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå status ‡πÅ‡∏ö‡∏ö vectorized\n",
    "        def create_status_mapping():\n",
    "            \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö status\"\"\"\n",
    "            return {\n",
    "                ('wait', ''): '1',\n",
    "                ('wait-key', ''): '1',\n",
    "                ('sendpay', 'sendpay'): '2',\n",
    "                ('sendpay', 'verify-wait'): '2',\n",
    "                ('tran-succ', 'sendpay'): '2',\n",
    "                ('tran-succ', 'verify-wait'): '2',\n",
    "                ('cancel', '88'): 'cancel',\n",
    "                ('delete', ''): 'delete',\n",
    "                ('wait', 'sendpay'): '2',\n",
    "                ('delete', 'sendpay'): 'delete',\n",
    "                ('delete', 'wait'): 'delete',\n",
    "                ('delete', 'wait-key'): 'delete',\n",
    "                ('wait', 'wait'): '1',\n",
    "                ('wait', 'wait-key'): '1',\n",
    "                ('', 'wait'): '1',\n",
    "                ('cancel', ''): 'cancel',\n",
    "                ('cancel', 'cancel'): 'cancel',\n",
    "                ('delete', 'delete'): 'delete',\n",
    "                ('active', 'verify'): '3',\n",
    "                ('active', 'success'): '8',\n",
    "                ('active', ''): '8',\n",
    "                ('active', 'success-waitinstall'): '8',\n",
    "                ('active', 'sendpay'): '2',\n",
    "                ('delete', 'verify'): 'delete',\n",
    "                ('wait', 'verify'): '2',\n",
    "                ('active', 'cancel'): 'cancel',\n",
    "                ('wait-pay', ''): '1',\n",
    "                ('', 'verify'): '2',\n",
    "                ('tran-succ', 'wait-key'): '2',\n",
    "                ('', 'cancel'): 'cancel',\n",
    "                ('delelte', 'sendpay'): 'delete',  # ‡πÅ‡∏Å‡πâ typo 'delelte'\n",
    "                ('cancel', 'success'): 'cancel',\n",
    "                ('sendpay', ''): '2',\n",
    "                ('wait-cancel', 'wait'): 'cancel',\n",
    "                ('cancel', 'sendpay'): 'cancel',\n",
    "                ('cancel', 'wait'): 'cancel',\n",
    "                ('active', 'wait'): '1',\n",
    "                ('tran-succ', 'verify'): '2',\n",
    "                ('active', 'verify-wait'): '1',\n",
    "                ('cancel', 'verify'): 'cancel',\n",
    "                ('wait', 'cancel'): 'cancel',\n",
    "                ('tran-succ', 'cancel'): 'cancel',\n",
    "                ('', 'success'): '8',\n",
    "                ('tran-succ', 'wait-confirm'): '2',\n",
    "                ('wait-key', 'sendpay'): '2',\n",
    "                ('wait-key', 'wait-key'): '1',\n",
    "                ('wait-pay', 'sendpay'): '2',\n",
    "            }\n",
    "\n",
    "        status_mapping = create_status_mapping()\n",
    "\n",
    "        # ‚úÖ key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mapping\n",
    "        df_merged['status_key'] = df_merged.apply(\n",
    "            lambda row: (\n",
    "                str(row.get('status_fssp') or '').strip(),\n",
    "                str(row.get('status_fsp') or '').strip()\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "        # ‚úÖ ‡πÅ‡∏°‡∏õ‡∏õ‡∏¥‡πâ‡∏á status ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\n",
    "        df_merged['status'] = df_merged['status_key'].map(status_mapping)\n",
    "\n",
    "        # ‚úÖ override ‡∏î‡πâ‡∏ß‡∏¢ status_fo (‡πÄ‡∏î‡∏¥‡∏°)\n",
    "        fo_mask = df_merged['status_fo'].notna()\n",
    "        df_merged.loc[fo_mask, 'status'] = df_merged.loc[fo_mask, 'status_fo'].apply(\n",
    "            lambda x: 'cancel' if x == '88' else x\n",
    "        )\n",
    "\n",
    "        # ‚úÖ ‡∏Å‡∏é‡∏ó‡∏±‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Priority ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î): ‡∏ñ‡πâ‡∏≤ status_key ‡∏°‡∏µ 'delete' ‡∏´‡∏£‡∏∑‡∏≠ 'cancel' ‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
    "        #    ‡πÄ‡∏ä‡πà‡∏ô ('delete','success') -> delete, ('cancel','success') -> cancel\n",
    "        df_merged['has_delete'] = df_merged['status_key'].apply(lambda t: isinstance(t, tuple) and ('delete' in t))\n",
    "        df_merged['has_cancel'] = df_merged['status_key'].apply(lambda t: isinstance(t, tuple) and ('cancel' in t))\n",
    "\n",
    "        # delete ‡∏ä‡∏ô‡∏∞‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ\n",
    "        df_merged.loc[df_merged['has_delete'] == True, 'status'] = 'delete'\n",
    "        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ delete ‡πÅ‡∏ï‡πà‡∏°‡∏µ cancel ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô cancel\n",
    "        df_merged.loc[(df_merged['has_delete'] != True) & (df_merged['has_cancel'] == True), 'status'] = 'cancel'\n",
    "\n",
    "        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\n",
    "        df_merged.drop(columns=['has_delete', 'has_cancel'], inplace=True, errors='ignore')\n",
    "\n",
    "        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ numeric\n",
    "        numeric_cols = [\n",
    "            'installment_number', 'show_price_check', 'price_product', 'ems_amount', 'service_price',\n",
    "            'ins_amount', 'prb_amount', 'total_amount', 'tax_car_price', 'overdue_fine_price',\n",
    "            'ins_discount', 'mkt_discount', 'payment_amount', 'price_addon', 'discount_addon',\n",
    "            'goverment_discount', 'tax_amount', 'fin_goverment_discount', 'ins_goverment_discount'\n",
    "        ]\n",
    "        for col in numeric_cols:\n",
    "            if col in df_merged.columns:\n",
    "                df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce').replace([np.inf, -np.inf], None)\n",
    "\n",
    "        logger.info(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "        return df_merged\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea952c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_quotation_data(df: pd.DataFrame):\n",
    "    table_name = 'fact_sales_quotation_temp'\n",
    "    pk_column = 'quotation_num'\n",
    "    \n",
    "    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏ô DataFrame ‡πÉ‡∏´‡∏°‡πà\n",
    "    df = df[~df[pk_column].duplicated(keep='first')].copy()\n",
    "    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà key ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô None\n",
    "    df = df[df[pk_column].notna()].copy()\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No valid data to process\")\n",
    "        return\n",
    "    \n",
    "    with target_engine.connect() as conn:\n",
    "        df_existing = pd.read_sql(f\"SELECT {pk_column} FROM {table_name}\", conn)\n",
    "\n",
    "    print(f\"üìä New data: {len(df)} rows\")\n",
    "    print(f\"üìä Existing data found: {len(df_existing)} rows\")\n",
    "\n",
    "    if not df_existing.empty:\n",
    "        df_existing = df_existing[~df_existing[pk_column].duplicated(keep='first')].copy()\n",
    "\n",
    "    existing_ids = set(df_existing[pk_column]) if not df_existing.empty else set()\n",
    "    new_ids = set(df[pk_column]) - existing_ids\n",
    "    df_to_insert = df[df[pk_column].isin(new_ids)].copy()\n",
    "\n",
    "    common_ids = set(df[pk_column]) & existing_ids\n",
    "    df_common_new = df[df[pk_column].isin(common_ids)].copy()\n",
    "    df_common_old = df_existing[df_existing[pk_column].isin(common_ids)].copy()\n",
    "\n",
    "    exclude_columns = [pk_column, 'create_at', 'datestart']\n",
    "    compare_cols = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    print(f\"üîç Columns to compare for updates: {compare_cols}\")\n",
    "    print(f\"üîç Excluded columns (audit fields): {exclude_columns}\")\n",
    "\n",
    "    df_to_update = pd.DataFrame()\n",
    "    if not df_common_new.empty and not df_common_old.empty:\n",
    "        merged = df_common_new.merge(df_common_old, on=pk_column, suffixes=('_new', '_old'))\n",
    "        available_cols = [col for col in compare_cols if f\"{col}_new\" in merged.columns and f\"{col}_old\" in merged.columns]\n",
    "\n",
    "        if available_cols:\n",
    "            diff_mask = pd.Series(False, index=merged.index)\n",
    "            for col in available_cols:\n",
    "                new_vals = merged[f\"{col}_new\"]\n",
    "                old_vals = merged[f\"{col}_old\"]\n",
    "                both_nan = (pd.isna(new_vals) & pd.isna(old_vals))\n",
    "                different = (new_vals != old_vals) & ~both_nan\n",
    "                diff_mask |= different\n",
    "\n",
    "            df_diff = merged[diff_mask].copy()\n",
    "            if not df_diff.empty:\n",
    "                update_cols = [f\"{col}_new\" for col in available_cols]\n",
    "                all_cols = [pk_column] + update_cols\n",
    "                existing_cols = [c for c in all_cols if c in df_diff.columns]\n",
    "                if len(existing_cols) > 1:\n",
    "                    df_to_update = df_diff[existing_cols].copy()\n",
    "                    new_col_names = [pk_column] + [c.replace('_new', '') for c in existing_cols if c != pk_column]\n",
    "                    df_to_update.columns = new_col_names\n",
    "\n",
    "    print(f\"üÜï Insert: {len(df_to_insert)} rows\")\n",
    "    print(f\"üîÑ Update: {len(df_to_update)} rows\")\n",
    "    \n",
    "    if not df_to_insert.empty:\n",
    "        print(\"üîç Sample data to INSERT:\")\n",
    "        sample_insert = df_to_insert.head(2)\n",
    "        for col in ['sale_team', 'transaction_date', 'type_insurance']:\n",
    "            if col in sample_insert.columns:\n",
    "                print(f\"   {col}: {sample_insert[col].tolist()}\")\n",
    "    \n",
    "    if not df_to_update.empty:\n",
    "        print(\"üîç Sample data to UPDATE:\")\n",
    "        sample_update = df_to_update.head(2)\n",
    "        for col in ['sale_team', 'transaction_date', 'type_insurance']:\n",
    "            if col in sample_update.columns:\n",
    "                print(f\"   {col}: {sample_update[col].tolist()}\")\n",
    "\n",
    "    metadata = Table(table_name, MetaData(), autoload_with=target_engine)\n",
    "\n",
    "    # ‚úÖ Insert (Batch)\n",
    "    if not df_to_insert.empty:\n",
    "        records = []\n",
    "        current_time = pd.Timestamp.now()\n",
    "        for _, row in df_to_insert.iterrows():\n",
    "            record = {}\n",
    "            for col, value in row.items():\n",
    "                if pd.isna(value) or value == pd.NaT or value == '':\n",
    "                    record[col] = None\n",
    "                else:\n",
    "                    record[col] = value\n",
    "            record['create_at'] = current_time\n",
    "            record['datestart'] = current_time\n",
    "            records.append(record)\n",
    "        with target_engine.begin() as conn:\n",
    "            conn.execute(metadata.insert(), records)\n",
    "\n",
    "    # ‚úÖ Update (Batch upsert)\n",
    "    if not df_to_update.empty:\n",
    "        records = []\n",
    "        for _, row in df_to_update.iterrows():\n",
    "            record = {}\n",
    "            for col, value in row.items():\n",
    "                if pd.isna(value) or value == pd.NaT or value == '':\n",
    "                    record[col] = None\n",
    "                else:\n",
    "                    record[col] = value\n",
    "            records.append(record)\n",
    "\n",
    "        with target_engine.begin() as conn:\n",
    "            for record in records:\n",
    "                stmt = pg_insert(metadata).values(**record)\n",
    "                update_columns = {\n",
    "                    c.name: stmt.excluded[c.name]\n",
    "                    for c in metadata.columns\n",
    "                    if c.name not in [pk_column, 'create_at', 'datestart']\n",
    "                }\n",
    "                update_columns['datestart'] = pd.Timestamp.now()\n",
    "                print(f\"üîç Updating columns for fact_sales_quotation {record.get(pk_column)}: {list(update_columns.keys())}\")\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=[pk_column],\n",
    "                    set_=update_columns\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "\n",
    "    print(\"‚úÖ Insert/update completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2069ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
