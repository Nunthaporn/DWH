from dagster import op, job
import pandas as pd
import numpy as np
import os
import re
import time
import gc
from datetime import datetime, timedelta
from dotenv import load_dotenv
from sqlalchemy import create_engine, MetaData, Table, inspect, text
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.sql.elements import TextClause
from typing import Union, List

# ‚úÖ Load env
load_dotenv()

# ‚úÖ DB connections
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_size=10,
    max_overflow=20,
    pool_timeout=30,
    pool_recycle=1800,
    pool_pre_ping=True,
    echo=False
)
task_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task",
    pool_size=10,
    max_overflow=20,
    pool_timeout=30,
    pool_recycle=1800,
    pool_pre_ping=True,
    echo=False
)
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance",
    pool_size=10,
    max_overflow=20,
    pool_timeout=30,
    pool_recycle=1800,
    pool_pre_ping=True,
    echo=False,
    connect_args={
        "connect_timeout": 30,
        "application_name": "fact_installment_payments_etl",
        "options": "-c statement_timeout=300000 -c idle_in_transaction_session_timeout=300000"
    }
)

def chunk_list(lst, chunk_size):
    for i in range(0, len(lst), chunk_size):
        yield lst[i:i + chunk_size]

def load_query_raw(engine, query: str) -> pd.DataFrame:
    raw_conn = None
    try:
        raw_conn = engine.raw_connection()
        cursor = raw_conn.cursor()
        cursor.execute(query)
        return pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])
    except Exception as e:
        print(f"‚ùå Error loading query: {e}")
        return pd.DataFrame()
    finally:
        if raw_conn:
            try: raw_conn.close()
            except Exception as e: print(f"‚ö†Ô∏è Error closing: {e}")
        engine.dispose()

def load_data_in_batches(engine, table_name: str, order_numbers: List[str], batch_size=10000) -> pd.DataFrame:
    all_results = []
    for batch in chunk_list(order_numbers, batch_size):
        formatted = "', '".join(batch)
        query = f"SELECT * FROM {table_name} WHERE order_number IN ('{formatted}')"
        df_batch = load_query_raw(engine, query)
        all_results.append(df_batch)
    return pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()

@op
def extract_installment_data():
    print("üîÑ Loading data from databases...")

    # ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ connection ‡∏°‡∏µ transaction ‡πÅ‡∏ö‡∏ö manual
    def create_fresh_engine(engine_url, engine_name):
        try:
            return create_engine(
                engine_url,
                pool_size=1,
                max_overflow=2,
                pool_timeout=30,
                pool_recycle=300,
                pool_pre_ping=True,
                echo=False,
                connect_args={
                    'connect_timeout': 60,
                    'autocommit': False,  # üîÑ ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å True ‡πÄ‡∏õ‡πá‡∏ô False
                    'charset': 'utf8mb4',
                    'sql_mode': 'STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO'
                }
            )
        except Exception as e:
            print(f"‚ùå Error creating {engine_name} engine: {e}")
            return None

    # ‚úÖ Connection URLs
    source_url = f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance"
    task_url = f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance_task"

    # ‚úÖ Load installment plan data
    print("üìä Loading plan + installment via JOIN...")
    df_plan = pd.DataFrame()
    df_installment = pd.DataFrame()

    fresh_source = create_fresh_engine(source_url, "source")
    if fresh_source:
        try:
            with fresh_source.begin() as conn:
                df_installment = pd.read_sql("""
                    SELECT p.quo_num,
                        i.money_one, i.money_two, i.money_three, i.money_four,
                        i.money_five, i.money_six, i.money_seven, i.money_eight, i.money_nine,
                        i.money_ten, i.money_eleven, i.money_twelve,
                        i.date_one, i.date_two, i.date_three, i.date_four, i.date_five,
                        i.date_six, i.date_seven, i.date_eight, i.date_nine, i.date_ten,
                        i.date_eleven, i.date_twelve, i.numpay
                    FROM fin_system_select_plan p
                    JOIN fin_installment i ON p.quo_num = i.quo_num
                    WHERE p.update_at BETWEEN '2025-01-01' AND '2025-08-06'
                """, conn)
                print(f"‚úÖ Loaded {len(df_installment)} installment records")
                df_plan = df_installment[['quo_num']].drop_duplicates()
        except Exception as e:
            print(f"‚ùå Error loading plan/installment data: {e}")
        finally:
            fresh_source.dispose()
    else:
        print("‚ö†Ô∏è Failed to create fresh source engine for plan/installment")

    # ‚úÖ Load order data
    print("üìä Loading order data...")
    df_order = pd.DataFrame()
    fresh_task_1 = create_fresh_engine(task_url, "task_1")
    if fresh_task_1:
        try:
            with fresh_task_1.begin() as conn:
                df_order = pd.read_sql("""
                    SELECT quo_num, order_number
                    FROM fin_order
                """, conn)
                print(f"‚úÖ Loaded {len(df_order)} order records")
        except Exception as e:
            print(f"‚ùå Error loading order data: {e}")
        finally:
            fresh_task_1.dispose()

    order_list = list(df_order["order_number"].dropna().astype(str).unique())
    print(f"üì¶ Loading bill data in batches... ({len(order_list)} orders)")

    if not df_order.empty:
        print("üìä Loading finance and bill data...")

        order_nums = "','".join(df_order['order_number'].dropna().astype(str))

        # ‚úÖ Create engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö finance
        task_engine_finance = create_fresh_engine(task_url, "task_engine_finance")

        # ‚úÖ Query
        query_finance = f"""
            SELECT order_number, numpay,
                moneypay_one, datepay_one,
                moneypay_two, datepay_two,
                moneypay_three, datepay_three,
                moneypay_four, datepay_four,
                moneypay_five, datepay_five,
                moneypay_six, datepay_six,
                moneypay_seven, datepay_seven,
                moneypay_eight, datepay_eight,
                moneypay_nine, datepay_nine,
                moneypay_ten, datepay_ten,
                moneypay_eleven, datepay_eleven,
                moneypay_twelve, datepay_twelve
            FROM fin_finance
            WHERE order_number IN ('{order_nums}')
        """
        df_finance = load_data_in_batches(task_engine_finance, "fin_finance", order_list)
        print(f"‚úÖ Loaded {len(df_finance)} finance records")

    task_engine_bill = create_fresh_engine(task_url, "task_engine_bill")
    if task_engine_bill:
        conn = None
        try:
            conn = task_engine_bill.connect()
            df_bill = pd.read_sql(f"""
                SELECT ...
                FROM fin_bill
                WHERE order_number IN ('{order_nums}')
            """, conn)
            print(f"‚úÖ Loaded {len(df_bill)} bill records")
        except Exception as e:
            print(f"‚ùå Error loading bill: {e}")
            if conn:
                try:
                    conn.rollback()
                except Exception as rb_e:
                    print(f"‚ö†Ô∏è Rollback error: {rb_e}")
        finally:
            if conn:
                try:
                    conn.close()
                except Exception as close_e:
                    print(f"‚ö†Ô∏è Error closing conn: {close_e}")
            task_engine_bill.dispose()

        # ‚úÖ Query
        query_bill = f"""
            SELECT order_number,
                bill_receipt, bill_receipt2, bill_receipt3, bill_receipt4,
                bill_receipt5, bill_receipt6, bill_receipt7, bill_receipt8,
                bill_receipt9, bill_receipt10, bill_receipt11, bill_receipt12
            FROM fin_bill
            WHERE order_number IN ('{order_nums}')
        """

        df_bill = load_data_in_batches(task_engine_bill, "fin_bill", order_list)
        print(f"‚úÖ Loaded {len(df_bill)} bill records")

    # ‚úÖ Load late fee data
    print("üìä Loading late fee data...")
    df_late_fee = pd.DataFrame()
    fresh_task_3 = create_fresh_engine(task_url, "task_3")
    if fresh_task_3:
        try:
            with fresh_task_3.begin() as conn:
                df_late_fee = pd.read_sql("""
                    SELECT orderNumber, penaltyPay, numPay
                    FROM FIN_Account_AttachSlip_PathImageSlip
                    WHERE checkPay IN ('‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö', '‡∏Ñ‡πà‡∏≤‡∏á‡∏ß‡∏î/‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö')
                """, conn)
                print(f"‚úÖ Loaded {len(df_late_fee)} late fee records")
        except Exception as e:
            print(f"‚ùå Error loading late fee data: {e}")
        finally:
            fresh_task_3.dispose()

    # ‚úÖ Load test data
    print("üìä Loading test data...")
    df_test = pd.DataFrame()
    fresh_source_2 = create_fresh_engine(source_url, "source_2")
    if fresh_source_2:
        try:
            with fresh_source_2.begin() as conn:
                df_test = pd.read_sql("""
                    SELECT quo_num
                    FROM fin_system_select_plan
                    WHERE name IN ('‡∏ó‡∏î‡∏™‡∏≠‡∏ö', 'test')
                """, conn)
                print(f"‚úÖ Loaded {len(df_test)} test records")
        except Exception as e:
            print(f"‚ùå Error loading test data: {e}")
        finally:
            fresh_source_2.dispose()

    # ‚úÖ Done
    print(f"üì¶ Data loaded: plan({df_plan.shape[0]}), installment({df_installment.shape[0]}), "
          f"order({df_order.shape[0]}), finance({df_finance.shape[0]}), "
          f"bill({df_bill.shape[0]}), late_fee({df_late_fee.shape[0]}), test({df_test.shape[0]})")

    return df_plan, df_installment, df_order, df_finance, df_bill, df_late_fee, df_test

@op
def clean_installment_data(inputs):
    df_plan, df_inst, df_order, df_fin, df_bill, df_fee, df_test = inputs

    print("üîÑ Processing installment data...")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
    print(f"üìä Input data summary:")
    print(f"  - Plan: {df_plan.shape[0]} records, columns: {list(df_plan.columns) if not df_plan.empty else 'empty'}")
    print(f"  - Installment: {df_inst.shape[0]} records, columns: {list(df_inst.columns) if not df_inst.empty else 'empty'}")
    print(f"  - Order: {df_order.shape[0]} records, columns: {list(df_order.columns) if not df_order.empty else 'empty'}")
    print(f"  - Finance: {df_fin.shape[0]} records, columns: {list(df_fin.columns) if not df_fin.empty else 'empty'}")
    print(f"  - Bill: {df_bill.shape[0]} records, columns: {list(df_bill.columns) if not df_bill.empty else 'empty'}")
    print(f"  - Late fee: {df_fee.shape[0]} records, columns: {list(df_fee.columns) if not df_fee.empty else 'empty'}")
    print(f"  - Test: {df_test.shape[0]} records, columns: {list(df_test.columns) if not df_test.empty else 'empty'}")

    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏á‡∏ß‡∏î + ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    if df_inst.empty:
        print("‚ö†Ô∏è Installment DataFrame is empty. Creating empty combined records.")
        df_combined = pd.DataFrame(columns=['quo_num', 'numpay', 'installment_period', 'installment_amount', 'due_date', 'installment_number'])
    elif 'numpay' not in df_inst.columns:
        print("‚ö†Ô∏è Installment DataFrame does not have 'numpay' column. Creating empty combined records.")
        df_combined = pd.DataFrame(columns=['quo_num', 'numpay', 'installment_period', 'installment_amount', 'due_date', 'installment_number'])
    elif 'quo_num' not in df_inst.columns:
        print("‚ö†Ô∏è Installment DataFrame does not have 'quo_num' column. Creating empty combined records.")
        df_combined = pd.DataFrame(columns=['quo_num', 'numpay', 'installment_period', 'installment_amount', 'due_date', 'installment_number'])
    else:
        df_inst['numpay'] = pd.to_numeric(df_inst['numpay'], errors='coerce')
        df_filtered = df_inst[df_inst['numpay'].notna() & (df_inst['numpay'] > 0)]

        money_cols = [f'money_{n}' for n in ['one', 'two', 'three', 'four', 'five', 'six',
                                             'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']]
        date_cols = [f'date_{n}' for n in ['one', 'two', 'three', 'four', 'five', 'six',
                                           'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']]

        # ‚úÖ ‡∏•‡∏î debug prints ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        print(f"üìä Processing {df_filtered.shape[0]} installment records")

        df_money = df_filtered.melt(id_vars=['quo_num', 'numpay'], value_vars=money_cols,
                                     var_name='installment_period', value_name='installment_amount')
        df_date = df_filtered.melt(id_vars=['quo_num', 'numpay'], value_vars=date_cols,
                                    var_name='due_date_period', value_name='due_date')

        df_combined = pd.concat([df_money.reset_index(drop=True), df_date['due_date']], axis=1)
        df_combined['installment_number'] = df_combined.groupby('quo_num').cumcount() + 1
        df_combined = df_combined[df_combined['installment_number'] <= df_combined['numpay']]
        df_combined = df_combined.sort_values(by=['quo_num', 'due_date'])
        df_combined['installment_number'] = df_combined.groupby('quo_num').cumcount() + 1

        print(f"üìä Combined {df_combined.shape[0]} installment records")

    # 2. ‡∏ú‡∏π‡∏Å order_number
    if df_order.empty:
        print("‚ö†Ô∏è Order DataFrame is empty. Creating empty joined records.")
        df_join = df_combined.copy()
        df_join['order_number'] = None
    elif 'quo_num' not in df_order.columns:
        print("‚ö†Ô∏è Order DataFrame does not have 'quo_num' column. Creating empty joined records.")
        df_join = df_combined.copy()
        df_join['order_number'] = None
    elif 'order_number' not in df_order.columns:
        print("‚ö†Ô∏è Order DataFrame does not have 'order_number' column. Creating empty joined records.")
        df_join = df_combined.copy()
        df_join['order_number'] = None
    else:
        df_join = pd.merge(df_combined, df_order, on='quo_num', how='left')
        print(f"üìä Joined with orders: {df_join.shape[0]} records")

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ä‡∏≥‡∏£‡∏∞
    num_to_name = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
                   'nine', 'ten', 'eleven', 'twelve']
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ df_fin ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df_fin.empty:
        print("‚ö†Ô∏è Finance DataFrame is empty. Creating empty payment records.")
        df_payment = pd.DataFrame(columns=['order_number', 'payment_amount', 'payment_date', 'installment_number'])
    elif 'numpay' not in df_fin.columns:
        print("‚ö†Ô∏è Finance DataFrame does not have 'numpay' column. Creating empty payment records.")
        df_payment = pd.DataFrame(columns=['order_number', 'payment_amount', 'payment_date', 'installment_number'])
    elif 'order_number' not in df_fin.columns:
        print("‚ö†Ô∏è Finance DataFrame does not have 'order_number' column. Creating empty payment records.")
        df_payment = pd.DataFrame(columns=['order_number', 'payment_amount', 'payment_date', 'installment_number'])
    else:
        df_fin['numpay'] = pd.to_numeric(df_fin['numpay'], errors='coerce')
        df_fin = df_fin[df_fin['numpay'].notna() & (df_fin['numpay'] > 0)]

        print(f"üìä Processing {df_fin.shape[0]} finance records")

        # ‡πÉ‡∏ä‡πâ vectorized operations ‡πÅ‡∏ó‡∏ô iterrows()
        rows_list = []
        for i, sfx in enumerate(num_to_name, start=1):
            money_col = f'moneypay_{sfx}'
            date_col = f'datepay_{sfx}'
            
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if money_col not in df_fin.columns or date_col not in df_fin.columns:
                print(f"‚ö†Ô∏è Missing columns {money_col} or {date_col} in finance data. Skipping installment {i}.")
                continue
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ installment
            temp_df = df_fin[['order_number', money_col, date_col]].copy()
            temp_df['installment_number'] = i
            temp_df['payment_amount'] = temp_df[money_col]
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            temp_df['raw_date'] = temp_df[date_col].astype(str)
            temp_df['payment_date'] = pd.to_datetime(
                temp_df['raw_date'].str.extract(r'(\d{4}-\d{1,2}-\d{1,2})')[0], 
                errors='coerce'
            )
            
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏µ 2026 ‡πÄ‡∏õ‡πá‡∏ô 2025 - ‡πÉ‡∏ä‡πâ vectorized operations
            mask_2026 = temp_df['payment_date'].dt.year == 2026
            if mask_2026.any():
                # ‡πÉ‡∏ä‡πâ pd.to_datetime() ‡πÅ‡∏ó‡∏ô apply
                temp_df.loc[mask_2026, 'payment_date'] = pd.to_datetime(
                    temp_df.loc[mask_2026, 'payment_date'].dt.strftime('2025-%m-%d'),
                    errors='coerce'
                )
            
            rows_list.append(temp_df[['order_number', 'payment_amount', 'payment_date', 'installment_number']])

        df_payment = pd.concat(rows_list, ignore_index=True)
        print(f"üìä Created {df_payment.shape[0]} payment records")

    # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° payment proof
    if df_bill.empty:
        print("‚ö†Ô∏è Bill DataFrame is empty. Creating empty proof records.")
        df_proof = pd.DataFrame(columns=['order_number', 'installment_number', 'payment_proof'])
    elif 'order_number' not in df_bill.columns:
        print("‚ö†Ô∏è Bill DataFrame does not have 'order_number' column. Creating empty proof records.")
        df_proof = pd.DataFrame(columns=['order_number', 'installment_number', 'payment_proof'])
    else:
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå bill_receipt ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        bill_receipt_cols = [f'bill_receipt{i if i > 1 else ""}' for i in range(1, 13)]
        missing_cols = [col for col in bill_receipt_cols if col not in df_bill.columns]
        if missing_cols:
            print(f"‚ö†Ô∏è Bill DataFrame missing columns: {missing_cols}. Creating empty proof records.")
            df_proof = pd.DataFrame(columns=['order_number', 'installment_number', 'payment_proof'])
        else:
            df_proof = df_bill.melt(id_vars=['order_number'],
                value_vars=[f'bill_receipt{i if i > 1 else ""}' for i in range(1, 13)],
                value_name='payment_proof')
            df_proof = df_proof[df_proof['payment_proof'].notna()].reset_index()
            df_proof['installment_number'] = df_proof.groupby('order_number').cumcount() + 1
            df_proof = df_proof[['order_number', 'installment_number', 'payment_proof']]

            print(f"üìä Created {df_proof.shape[0]} proof records")

    # 5. ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    df = pd.merge(df_join, df_payment, on=['order_number', 'installment_number'], how='left')
    df = pd.merge(df, df_proof, on=['order_number', 'installment_number'], how='left')

    print(f"üìä Final merged data: {df.shape[0]} records")

    # 6. ‡πÄ‡∏û‡∏¥‡πà‡∏° late_fee
    if df_fee.empty:
        print("‚ö†Ô∏è Late fee DataFrame is empty. Adding empty late_fee column.")
        df['late_fee'] = 0
    elif 'orderNumber' not in df_fee.columns or 'penaltyPay' not in df_fee.columns or 'numPay' not in df_fee.columns:
        print("‚ö†Ô∏è Late fee DataFrame missing required columns. Adding empty late_fee column.")
        df['late_fee'] = 0
    else:
        df_fee = df_fee.rename(columns={
            'orderNumber': 'order_number',
            'penaltyPay': 'late_fee',
            'numPay': 'installment_number'
        })
        df_fee = df_fee.drop_duplicates()
        df['installment_number'] = pd.to_numeric(df['installment_number'], errors='coerce')
        df_fee['installment_number'] = pd.to_numeric(df_fee['installment_number'], errors='coerce')
        df = pd.merge(df, df_fee, on=['order_number', 'installment_number'], how='left')
        df['late_fee'] = df['late_fee'].fillna(0).astype(int)
    
    # 7. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total_paid - ‡πÉ‡∏ä‡πâ vectorized operations ‡πÅ‡∏ó‡∏ô apply
    # ‚úÖ ‡∏•‡∏ö comma ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric
    if df['payment_amount'].dtype == 'object':
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö comma
        df['payment_amount'] = df['payment_amount'].astype(str)
        
        # ‡∏•‡∏ö comma ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN string
        mask_not_nan = ~df['payment_amount'].str.lower().isin(['nan', 'null', 'none', 'undefined'])
        df.loc[mask_not_nan, 'payment_amount'] = df.loc[mask_not_nan, 'payment_amount'].str.replace(',', '')
    
    if df['installment_amount'].dtype == 'object':
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö comma
        df['installment_amount'] = df['installment_amount'].astype(str)
        
        # ‡∏•‡∏ö comma ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN string
        mask_not_nan = ~df['installment_amount'].str.lower().isin(['nan', 'null', 'none', 'undefined'])
        df.loc[mask_not_nan, 'installment_amount'] = df.loc[mask_not_nan, 'installment_amount'].str.replace(',', '')
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN
    df['payment_amount'] = pd.to_numeric(df['payment_amount'], errors='coerce')
    df['payment_amount'] = df['payment_amount'].where(pd.notna(df['payment_amount']), None)
    
    df['installment_amount'] = pd.to_numeric(df['installment_amount'], errors='coerce')
    df['late_fee'] = pd.to_numeric(df['late_fee'], errors='coerce').fillna(0)
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
    df['installment_amount'] = df['installment_amount'].where(pd.notna(df['installment_amount']), None)
    df['late_fee'] = df['late_fee'].where(pd.notna(df['late_fee']), None)
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Infinity ‡∏´‡∏£‡∏∑‡∏≠ -Infinity
    df['payment_amount'] = df['payment_amount'].replace([np.inf, -np.inf], np.nan)
    df['installment_amount'] = df['installment_amount'].replace([np.inf, -np.inf], np.nan)
    df['late_fee'] = df['late_fee'].replace([np.inf, -np.inf], 0)
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Infinity
    df['payment_amount'] = df['payment_amount'].where(pd.notna(df['payment_amount']), None)
    df['installment_amount'] = df['installment_amount'].where(pd.notna(df['installment_amount']), None)
    df['late_fee'] = df['late_fee'].where(pd.notna(df['late_fee']), None)
    
    # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total_paid ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    df['total_paid'] = np.where(
        df['late_fee'] == 0,
        df['installment_amount'].fillna(0),
        # ‡∏ñ‡πâ‡∏≤ late_fee != 0 ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ payment_amount + late_fee ‡∏´‡∏£‡∏∑‡∏≠ installment_amount + late_fee
        np.where(
            df['payment_amount'].notna(),
            df['payment_amount'].fillna(0) + df['late_fee'],
            df['installment_amount'].fillna(0) + df['late_fee']
        )
    )
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö total_paid
    df['total_paid'] = df['total_paid'].where(pd.notna(df['total_paid']), None)

    print(f"üìä Calculated total_paid for {df['total_paid'].notna().sum()} records")

    # 8. payment_status
    df['due_date'] = pd.to_datetime(df['due_date'], errors='coerce')
    df['payment_date'] = pd.to_datetime(df['payment_date'], errors='coerce')
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡πà‡∏≤‡∏á‡πÜ
    no_payment = df['payment_amount'].isna() & df['payment_date'].isna() & df['payment_proof'].isna()
    has_payment = df['payment_amount'].notna() | df['payment_proof'].notna()
    has_due_date = df['due_date'].notna()
    has_payment_date = df['payment_date'].notna()
    is_overdue = has_due_date & (datetime.now().date() > df['due_date'].dt.date)
    is_late_payment = has_payment_date & has_due_date & (df['payment_date'].dt.date > df['due_date'].dt.date)
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î payment_status
    df['payment_status'] = '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ä‡∏≥‡∏£‡∏∞'  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    df.loc[no_payment & is_overdue, 'payment_status'] = '‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏≥‡∏£‡∏∞'
    df.loc[has_payment & ~is_late_payment, 'payment_status'] = '‡∏ä‡∏≥‡∏£‡∏∞‡πÅ‡∏•‡πâ‡∏ß'
    df.loc[has_payment & is_late_payment, 'payment_status'] = '‡∏ä‡∏≥‡∏£‡∏∞‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤'

    # 9. ‡∏•‡πâ‡∏≤‡∏á test ‡πÅ‡∏•‡∏∞ undefined
    if not df_test.empty and 'quo_num' in df_test.columns:
        df = df[~df['quo_num'].isin(df_test['quo_num'])]
        print(f"üìä Filtered out {len(df_test)} test records")
    else:
        print("‚ö†Ô∏è Test DataFrame is empty or missing 'quo_num' column. Skipping test record filtering.")
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ quo_num ‡πÄ‡∏õ‡πá‡∏ô undefined
    df = df[df['quo_num'] != 'undefined']
    df = df[df['quo_num'].notna()]  # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ quo_num ‡πÄ‡∏õ‡πá‡∏ô null/NaN ‡∏î‡πâ‡∏ß‡∏¢
    
    df = df.rename(columns={'quo_num': 'quotation_num'})
    df['installment_number'] = df['installment_number'].replace({'0': '1'})
    df['installment_number'] = pd.to_numeric(df['installment_number'], errors='coerce')
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö installment_number
    df['installment_number'] = df['installment_number'].where(pd.notna(df['installment_number']), None)
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    df['due_date'] = pd.to_datetime(df['due_date'], errors='coerce')
    df['due_date'] = df['due_date'].dt.strftime('%Y%m%d')
    df['due_date'] = df['due_date'].where(pd.notna(df['due_date']), None)
    
    df['payment_date'] = pd.to_datetime(df['payment_date'], errors='coerce')
    df['payment_date'] = df['payment_date'].dt.strftime('%Y%m%d')
    df['payment_date'] = df['payment_date'].where(pd.notna(df['payment_date']), None)
    
    # ‚úÖ ‡πÉ‡∏ä‡πâ sanitize_dataframe function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
    df = sanitize_dataframe(df.copy())

    # ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
    final_columns = [
        'quotation_num', 'installment_number', 'due_date', 'installment_amount',
        'payment_date', 'payment_amount', 'late_fee', 'total_paid', 
        'payment_proof', 'payment_status', 'order_number'
    ]
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    available_columns = [col for col in final_columns if col in df.columns]
    missing_columns = [col for col in final_columns if col not in df.columns]
    
    if missing_columns:
        print(f"‚ö†Ô∏è Missing columns: {missing_columns}")
    
    df_final = df[available_columns].copy()
    print(f"üìä Final data ready: {df_final.shape[0]} records")
    
    # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á NaN string ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PostgreSQL
    for col in df_final.columns:
        if df_final[col].dtype == 'object':
            df_final[col] = df_final[col].replace(['nan', 'null', 'none', 'undefined', 'NaN', 'NULL', 'NONE', 'UNDEFINED'], None)
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    for col in df_final.columns:
        if df_final[col].dtype in ['float64', 'int64']:
            # ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
            df_final[col] = df_final[col].where(pd.notna(df_final[col]), None)
        elif df_final[col].dtype == 'object':
            # ‡πÅ‡∏õ‡∏•‡∏á string 'nan', 'null' ‡πÄ‡∏õ‡πá‡∏ô None
            df_final[col] = df_final[col].astype(str)
            nan_mask = df_final[col].str.lower().isin(['nan', 'null', 'none', 'undefined'])
            df_final.loc[nan_mask, col] = None

    print("‚úÖ Data cleaning completed for PostgreSQL")

    return df_final

def sanitize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ 'NaN', 'null', 'none' string ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á float NaN ‡πÄ‡∏õ‡πá‡∏ô None - ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û"""
    
    # üîÅ ‡∏™‡∏£‡πâ‡∏≤‡∏á copy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    df_clean = df.copy()
    
    # üîÅ ‡πÅ‡∏õ‡∏•‡∏á float NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏Å‡πà‡∏≠‡∏ô
    df_clean = df_clean.where(pd.notna(df_clean), None)
    
    # üîÅ ‡∏•‡πâ‡∏≤‡∏á string columns ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    string_columns = df_clean.select_dtypes(include=['object']).columns
    
    for col in string_columns:
        if col in df_clean.columns:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            non_null_count_before = df_clean[col].notna().sum()
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            df_clean[col] = df_clean[col].astype(str)
            
            # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            nan_mask = (
                df_clean[col].str.lower().isin(['nan', 'null', 'none', 'nonetype', 'nulltype', 'undefined']) |
                df_clean[col].str.strip().isin(['', '[null]', 'undefined']) |
                df_clean[col].str.contains('^\\[null\\]$', case=False, na=False) |
                df_clean[col].str.contains('^undefined$', case=False, na=False)
            )
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤ NaN string ‡∏î‡πâ‡∏ß‡∏¢ None
            df_clean.loc[nan_mask, col] = None
            
            # ‡∏•‡∏ö whitespace ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None
            mask_not_none = df_clean[col].notna()
            if mask_not_none.any():
                df_clean.loc[mask_not_none, col] = df_clean.loc[mask_not_none, col].str.strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å strip
                nan_mask_after = (
                    df_clean[col].str.lower().isin(['nan', 'null', 'none', '', 'undefined']) |
                    df_clean[col].str.contains('^\\[null\\]$', case=False, na=False) |
                    df_clean[col].str.contains('^undefined$', case=False, na=False)
                )
                df_clean.loc[nan_mask_after, col] = None
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            non_null_count_after = df_clean[col].notna().sum()
            if non_null_count_before != non_null_count_after:
                print(f"‚ö†Ô∏è Column {col}: {non_null_count_before} ‚Üí {non_null_count_after} non-null values")

    # üîÅ ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ comma - ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    numeric_cols = ['installment_amount', 'payment_amount', 'total_paid', 'late_fee']
    for col in numeric_cols:
        if col in df_clean.columns:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            non_null_count_before = df_clean[col].notna().sum()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô numeric column ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if df_clean[col].dtype in ['int64', 'float64']:
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô numeric ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
                df_clean[col] = df_clean[col].where(pd.notna(df_clean[col]), None)
                print(f"üîç Column {col} is numeric, converted NaN to None")
                continue
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö comma
            df_clean[col] = df_clean[col].astype(str)
            
            # ‡∏•‡∏ö comma ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN string
            mask_not_nan = ~df_clean[col].str.lower().isin(['nan', 'null', 'none', 'undefined'])
            mask_has_comma = df_clean[col].str.contains(',', na=False)
            mask_to_clean = mask_not_nan & mask_has_comma
            
            if mask_to_clean.any():
                df_clean.loc[mask_to_clean, col] = df_clean.loc[mask_to_clean, col].str.replace(',', '')
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            # ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
            df_clean[col] = df_clean[col].where(pd.notna(df_clean[col]), None)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            non_null_count_after = df_clean[col].notna().sum()
            if non_null_count_before != non_null_count_after:
                print(f"‚ö†Ô∏è Numeric column {col}: {non_null_count_before} ‚Üí {non_null_count_after} non-null values")

    return df_clean

def clean_records_for_db(records: list) -> list:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î records ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    cleaned_records = []
    
    for i, record in enumerate(records):
        cleaned_record = {}
        for key, value in record.items():
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡πà‡∏≤
            if pd.isna(value):
                cleaned_record[key] = None
            elif isinstance(value, str):
                if value.lower() in ['nan', 'null', 'none', 'undefined', '']:
                    cleaned_record[key] = None
                else:
                    cleaned_record[key] = value.strip()
            elif isinstance(value, (int, float)):
                if pd.isna(value) or value in [np.inf, -np.inf]:
                    cleaned_record[key] = None
                else:
                    cleaned_record[key] = value
            else:
                cleaned_record[key] = value
        
        cleaned_records.append(cleaned_record)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 3 records ‡πÅ‡∏£‡∏Å
        if i < 3:
            print(f"    üîç Record {i} cleaned: {cleaned_record}")
    
    return cleaned_records

@op
def load_installment_data(df: pd.DataFrame):
    table_name = 'fact_installment_payments'
    pk_column = ['quotation_num', 'installment_number']

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î
    print(f"üîç Before loading - DataFrame shape: {df.shape}")
    print(f"üîç Before loading - Columns: {list(df.columns)}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NaN values ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î
    print("\nüîç NaN check before loading:")
    for col in df.columns:
        nan_count = df[col].isna().sum()
        if nan_count > 0:
            print(f"  - {col}: {nan_count} NaN values")
    
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
    print("\nüßπ Final data cleaning before loading...")
    df_clean = df.copy()
    for col in df_clean.columns:
        if df_clean[col].dtype in ['float64', 'int64']:
            # ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
            nan_count = df_clean[col].isna().sum()
            if nan_count > 0:
                print(f"  üîÑ Converting {nan_count} NaN values to None in {col}")
                df_clean[col] = df_clean[col].where(pd.notna(df_clean[col]), None)
        elif df_clean[col].dtype == 'object':
            # ‡πÅ‡∏õ‡∏•‡∏á string 'nan', 'null' ‡πÄ‡∏õ‡πá‡∏ô None
            df_clean[col] = df_clean[col].astype(str)
            nan_mask = df_clean[col].str.lower().isin(['nan', 'null', 'none', 'undefined'])
            nan_count = nan_mask.sum()
            if nan_count > 0:
                print(f"  üîÑ Converting {nan_count} string NaN values to None in {col}")
                df_clean.loc[nan_mask, col] = None
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    print("\nüîç NaN check after cleaning:")
    for col in df_clean.columns:
        nan_count = df_clean[col].isna().sum()
        if nan_count > 0:
            print(f"  - {col}: {nan_count} NaN values")
    
    # ‡πÉ‡∏ä‡πâ df_clean ‡πÅ‡∏ó‡∏ô df ‡∏ï‡πà‡∏≠‡πÑ‡∏õ
    df = df_clean
    
    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å DataFrame ‡πÉ‡∏´‡∏°‡πà
    df = df[~df[pk_column].duplicated(keep='first')].copy()
    print(f"üîç After removing duplicates: {len(df)} rows")

    # ‚úÖ ‡∏ß‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 00:00:00)
    today_str = datetime.now().strftime('%Y-%m-%d')

    # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á fresh target engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    target_url = f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance"
    
    def create_fresh_target_engine():
        """‡∏™‡∏£‡πâ‡∏≤‡∏á fresh target engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PostgreSQL"""
        try:
            fresh_target = create_engine(
                target_url,
                pool_size=5,
                max_overflow=10,
                pool_timeout=30,
                pool_recycle=1800,
                pool_pre_ping=True,
                echo=False,
                connect_args={
                    "connect_timeout": 30,
                    "application_name": "fact_installment_payments_etl",
                    "options": "-c statement_timeout=300000 -c idle_in_transaction_session_timeout=300000"
                }
            )
            return fresh_target
        except Exception as e:
            print(f"‚ùå Error creating fresh target engine: {e}")
            return None

    # ‚úÖ Load ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å PostgreSQL - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ connection
    df_existing = pd.DataFrame()
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        fresh_target = create_fresh_target_engine()
        if not fresh_target:
            print("‚ö†Ô∏è Failed to create fresh target engine. Proceeding without existing data comparison.")
            df_existing = pd.DataFrame()
            break
            
        try:
            print(f"üîÑ Attempting to load existing data (attempt {retry_count + 1}/{max_retries})...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á connection ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            with fresh_target.connect() as conn:
                # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ timeout ‡πÅ‡∏•‡∏∞ connection parameters
                conn.execute(text("SET statement_timeout = 300000"))  # 5 minutes
                conn.execute(text("SET idle_in_transaction_session_timeout = 300000"))  # 5 minutes
                
                df_existing = pd.read_sql(
                    f"SELECT * FROM {table_name} WHERE update_at >= '{today_str}'",
                    conn
                )
            
            print(f"üìä Existing data loaded successfully: {len(df_existing)} rows")
            fresh_target.dispose()
            break
            
        except Exception as e:
            retry_count += 1
            print(f"‚ùå Error loading existing data (attempt {retry_count}/{max_retries}): {e}")
            
            # ‡∏õ‡∏¥‡∏î fresh engine ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î error
            try:
                fresh_target.dispose()
                print("‚úÖ Fresh target engine disposed successfully after error")
            except Exception as dispose_error:
                print(f"‚ö†Ô∏è Error disposing fresh target engine: {dispose_error}")
                
                # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô exception handler
                try:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
                    for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update', 'fresh_target_ops']:
                        if var_name in locals():
                            engine = locals()[var_name]
                            if engine is not None:
                                try:
                                    engine.dispose()
                                    print(f"‚úÖ {var_name} engine disposed successfully in exception handler")
                                except Exception as dispose_error2:
                                    print(f"‚ö†Ô∏è {var_name} engine disposal failed in exception handler: {dispose_error2}")
                except Exception as cleanup_error:
                    print(f"‚ö†Ô∏è Error during exception handler engine cleanup: {cleanup_error}")
            
            if retry_count >= max_retries:
                print("‚ö†Ô∏è Max retries reached. Proceeding without existing data comparison.")
                df_existing = pd.DataFrame()
                break
            
            # ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
            time.sleep(2 ** retry_count)  # Exponential backoff

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô DataFrame ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df.empty:
        print("‚ö†Ô∏è DataFrame is empty. No data to process.")
        return
    
    missing_pk_cols = [col for col in pk_column if col not in df.columns]
    if missing_pk_cols:
        print(f"‚ùå Missing primary key columns in DataFrame: {missing_pk_cols}")
        print(f"Available columns: {list(df.columns)}")
        return
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ NaN ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    for col in pk_column:
        null_count = df[col].isna().sum()
        if null_count > 0:
            print(f"‚ö†Ô∏è Column {col} has {null_count} null values. These will be excluded from processing.")
    
    # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ NaN ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key
    df_clean = df[df[pk_column].notna().all(axis=1)].copy()
    if len(df_clean) < len(df):
        print(f"‚ö†Ô∏è Filtered out {len(df) - len(df_clean)} rows with null primary key values.")
        df = df_clean
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df.empty:
        print("‚ö†Ô∏è DataFrame is empty after filtering null primary key values. No data to process.")
        return
    
    missing_pk_cols_existing = [col for col in pk_column if col not in df_existing.columns]
    if missing_pk_cols_existing:
        print(f"‚ùå Missing primary key columns in existing DataFrame: {missing_pk_cols_existing}")
        print(f"Available columns: {list(df_existing.columns)}")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á empty composite key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö existing data
        df_existing['composite_key'] = ''
        print("üìä Created empty composite key for existing DataFrame due to missing primary key columns")
    elif df_existing.empty:
        print("‚ö†Ô∏è Existing DataFrame is empty. Creating empty composite key.")
        df_existing['composite_key'] = ''
        print("üìä Created empty composite key for empty existing DataFrame")
    else:
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key ‡∏Ç‡∏≠‡∏á existing data ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ NaN ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for col in pk_column:
            if col in df_existing.columns:
                null_count = df_existing[col].isna().sum()
                if null_count > 0:
                    print(f"‚ö†Ô∏è Existing data column {col} has {null_count} null values.")
        
        # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ NaN ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key ‡∏Ç‡∏≠‡∏á existing data
        df_existing_clean = df_existing[df_existing[pk_column].notna().all(axis=1)].copy()
        if len(df_existing_clean) < len(df_existing):
            print(f"‚ö†Ô∏è Filtered out {len(df_existing) - len(df_existing_clean)} rows with null primary key values from existing data.")
            df_existing = df_existing_clean
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ existing DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if df_existing.empty:
            print("‚ö†Ô∏è Existing DataFrame is empty after filtering null primary key values.")
            df_existing['composite_key'] = ''
            print("üìä Created empty composite key for existing DataFrame after filtering")
        else:
            # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á dtype ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á df ‡πÅ‡∏•‡∏∞ df_existing
            for col in pk_column:
                if col in df.columns and col in df_existing.columns:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                    df[col] = df[col].astype(str)
                    df_existing[col] = df_existing[col].astype(str)

            # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á composite key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ None/NaN
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô df ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if 'composite_key' not in df.columns:
                df['composite_key'] = df[pk_column[0]].fillna('').astype(str) + '|' + df[pk_column[1]].fillna('').astype(str)
                print(f"üìä Created composite key for main DataFrame in existing data section: {len(df)} records")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô df_existing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if 'composite_key' not in df_existing.columns:
                df_existing['composite_key'] = df_existing[pk_column[0]].fillna('').astype(str) + '|' + df_existing[pk_column[1]].fillna('').astype(str)
                print(f"üìä Created composite key for existing DataFrame: {len(df_existing)} records")
            else:
                print(f"üìä Composite key already exists in existing DataFrame: {len(df_existing)} records")

    # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á composite key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö df ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏™‡∏°‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ existing data ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
    if 'composite_key' not in df.columns:
        # ‡πÅ‡∏õ‡∏•‡∏á dtype ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á composite key
        for col in pk_column:
            if col in df.columns:
                df[col] = df[col].astype(str)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á composite key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö df ‡∏´‡∏•‡∏±‡∏Å
        df['composite_key'] = df[pk_column[0]].fillna('').astype(str) + '|' + df[pk_column[1]].fillna('').astype(str)
        print(f"üìä Created composite key for main DataFrame: {len(df)} records")
    else:
        print(f"üìä Composite key already exists in main DataFrame: {len(df)} records")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'composite_key' not in df.columns:
        print("‚ùå Failed to create composite_key for main DataFrame. Exiting.")
        return

    # ‚úÖ ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    existing_keys = set(df_existing['composite_key']) if not df_existing.empty else set()
    df_to_insert = df[~df['composite_key'].isin(existing_keys)].copy()
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite key ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    print(f"üìä Data summary:")
    print(f"  - Total records: {len(df)}")
    print(f"  - Existing records: {len(df_existing)}")
    print(f"  - New records to insert: {len(df_to_insert)}")
    print(f"  - Records to update: {len(df) - len(df_to_insert)}")

    # ‚úÖ ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    common_keys = set(df['composite_key']) & existing_keys if existing_keys else set()
    df_common_new = df[df['composite_key'].isin(common_keys)].copy() if common_keys else pd.DataFrame()
    df_common_old = df_existing[df_existing['composite_key'].isin(common_keys)].copy() if common_keys and not df_existing.empty else pd.DataFrame()

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not df_common_new.empty and not df_common_old.empty:
        try:
            # ‡∏ï‡∏±‡πâ‡∏á index ‡∏î‡πâ‡∏ß‡∏¢ composite_key
            df_common_new.set_index('composite_key', inplace=True)
            df_common_old.set_index('composite_key', inplace=True)

            # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° pk columns ‡πÅ‡∏•‡∏∞ composite_key)
            compare_cols = [col for col in df_common_new.columns if col not in pk_column + ['composite_key']]
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á DataFrame ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            available_cols = [col for col in compare_cols if col in df_common_old.columns]
            
            if available_cols:
                df_common_new_compare = df_common_new[available_cols]
                df_common_old_compare = df_common_old[available_cols]

                # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                df_diff_mask = ~(df_common_new_compare.eq(df_common_old_compare, axis=1).all(axis=1))
                df_diff = df_common_new[df_diff_mask].reset_index()
            else:
                df_diff = pd.DataFrame()
        except Exception as e:
            print(f"‚ö†Ô∏è Error during data comparison: {e}")
            df_diff = pd.DataFrame()
    else:
        df_diff = pd.DataFrame()

    print(f"üÜï Insert: {len(df_to_insert)} rows")
    print(f"üîÑ Update: {len(df_diff)} rows")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ insert ‡∏´‡∏£‡∏∑‡∏≠ update ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if df_to_insert.empty and df_diff.empty:
        print("‚ö†Ô∏è No new data to insert or update. Exiting.")
        return
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô DataFrame ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    if 'composite_key' not in df.columns:
        print("‚ùå composite_key not found in main DataFrame. This should not happen.")
        return
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_existing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'composite_key' not in df_existing.columns:
        print("‚ùå composite_key not found in existing DataFrame. This should not happen.")
        return

    # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á fresh target engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö metadata ‡πÅ‡∏•‡∏∞ operations
    fresh_target_ops = create_fresh_target_engine()
    if not fresh_target_ops:
        print("‚ùå Failed to create fresh target engine for operations. Exiting.")
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î
        engines_to_check = []
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
        for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update']:
            if var_name in locals():
                engine = locals()[var_name]
                if engine is not None:
                    engines_to_check.append((var_name, engine))
        
        for engine_name, engine in engines_to_check:
            try:
                engine.dispose()
                print(f"‚úÖ {engine_name} engine disposed successfully")
            except Exception as dispose_error:
                print(f"‚ö†Ô∏è {engine_name} engine disposal failed: {dispose_error}")
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô exception handler
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
            for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update', 'fresh_target_ops']:
                if var_name in locals():
                    engine = locals()[var_name]
                    if engine is not None:
                        try:
                            engine.dispose()
                            print(f"‚úÖ {var_name} engine disposed successfully in exception handler")
                        except Exception as dispose_error:
                            print(f"‚ö†Ô∏è {var_name} engine disposal failed in exception handler: {dispose_error}")
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è Error during exception handler engine cleanup: {cleanup_error}")
        
        return
    
    # ‚úÖ Load table metadata
    metadata = Table(table_name, MetaData(), autoload_with=fresh_target_ops)

    # ‚úÖ Insert - ‡πÉ‡∏ä‡πâ Batch UPSERT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    if not df_to_insert.empty:
        print(f"üì§ Starting insert process for {len(df_to_insert)} records...")
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_to_insert ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'composite_key' in df_to_insert.columns:
            df_to_insert = df_to_insert.drop(columns=['composite_key'])
        else:
            print("‚ö†Ô∏è composite_key not found in df_to_insert. Proceeding without dropping.")
        
        # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô insert - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
        for col in df_to_insert.columns:
            if df_to_insert[col].dtype in ['float64', 'int64']:
                df_to_insert[col] = df_to_insert[col].where(pd.notna(df_to_insert[col]), None)
            elif df_to_insert[col].dtype == 'object':
                df_to_insert[col] = df_to_insert[col].astype(str)
                nan_mask = df_to_insert[col].str.lower().isin(['nan', 'null', 'none', 'undefined'])
                df_to_insert.loc[nan_mask, col] = None
        
        df_to_insert_valid = df_to_insert[df_to_insert[pk_column].notna().all(axis=1)].copy()
        dropped = len(df_to_insert) - len(df_to_insert_valid)
        if dropped > 0:
            print(f"‚ö†Ô∏è Skipped {dropped} rows with null primary keys")
        
        if not df_to_insert_valid.empty:
            print(f"üì§ Processing {len(df_to_insert_valid)} valid records for insertion...")
            
            # ‡πÉ‡∏ä‡πâ batch size ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ connection
            batch_size = 5000
            total_batches = (len(df_to_insert_valid) + batch_size - 1) // batch_size
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ connection ‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch
            for i in range(0, len(df_to_insert_valid), batch_size):
                batch_df = df_to_insert_valid.iloc[i:i+batch_size]
                batch_num = (i // batch_size) + 1
                print(f"  üì¶ Processing batch {batch_num}/{total_batches} ({len(batch_df)} records)")
                
                # ‡πÉ‡∏ä‡πâ executemany ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch insert
                records = batch_df.to_dict(orient='records')
                
                # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î records ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                records = clean_records_for_db(records)
                
                # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á fresh engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch
                fresh_target_batch = create_fresh_target_engine()
                if not fresh_target_batch:
                    print(f"    ‚ùå Failed to create fresh target engine for batch {batch_num}. Skipping this batch.")
                    continue
                
                # ‚úÖ ‡πÉ‡∏ä‡πâ connection ‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch ‡∏û‡∏£‡πâ‡∏≠‡∏° retry logic
                max_retries = 3
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        with fresh_target_batch.begin() as conn:
                            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ timeout
                            conn.execute(text("SET statement_timeout = 300000"))  # 5 minutes
                            conn.execute(text("SET idle_in_transaction_session_timeout = 300000"))  # 5 minutes
                            
                            stmt = pg_insert(metadata).values(records)
                            update_columns = {
                                c.name: stmt.excluded[c.name]
                                for c in metadata.columns
                                if c.name not in pk_column
                            }
                            # update_at ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                            update_columns['update_at'] = datetime.now()

                            stmt = stmt.on_conflict_do_update(
                                index_elements=pk_column,
                                set_=update_columns
                            )
                            conn.execute(stmt)
                        
                        print(f"    ‚úÖ Batch {batch_num} inserted successfully")
                        fresh_target_batch.dispose()
                        break
                        
                    except Exception as e:
                        retry_count += 1
                        print(f"    ‚ùå Error inserting batch {batch_num} (attempt {retry_count}/{max_retries}): {e}")
                        
                        if retry_count >= max_retries:
                            print(f"    ‚ö†Ô∏è Max retries reached for batch {batch_num}. Skipping this batch.")
                            try:
                                fresh_target_batch.dispose()
                                print(f"    ‚úÖ Fresh target batch engine disposed successfully after max retries")
                            except Exception as dispose_error:
                                print(f"    ‚ö†Ô∏è Error disposing fresh target engine for batch {batch_num}: {dispose_error}")
                                
                                # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô exception handler
                                try:
                                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
                                    for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update', 'fresh_target_ops']:
                                        if var_name in locals():
                                            engine = locals()[var_name]
                                            if engine is not None:
                                                try:
                                                    engine.dispose()
                                                    print(f"    ‚úÖ {var_name} engine disposed successfully in exception handler")
                                                except Exception as dispose_error2:
                                                    print(f"    ‚ö†Ô∏è {var_name} engine disposal failed in exception handler: {dispose_error2}")
                                except Exception as cleanup_error:
                                    print(f"    ‚ö†Ô∏è Error during exception handler engine cleanup: {cleanup_error}")
                            break
                        
                        # ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
                        time.sleep(2 ** retry_count)  # Exponential backoff

    # ‚úÖ Update - ‡πÉ‡∏ä‡πâ Batch UPSERT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    if not df_diff.empty:
        print(f"üìù Starting update process for {len(df_diff)} records...")
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ composite_key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô df_diff ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'composite_key' in df_diff.columns:
            df_diff = df_diff.drop(columns=['composite_key'])
        else:
            print("‚ö†Ô∏è composite_key not found in df_diff. Proceeding without dropping.")
        
        # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô update - ‡πÅ‡∏õ‡∏•‡∏á NaN ‡πÄ‡∏õ‡πá‡∏ô None
        for col in df_diff.columns:
            if df_diff[col].dtype in ['float64', 'int64']:
                df_diff[col] = df_diff[col].where(pd.notna(df_diff[col]), None)
            elif df_diff[col].dtype == 'object':
                df_diff[col] = df_diff[col].astype(str)
                nan_mask = df_diff[col].str.lower().isin(['nan', 'null', 'none', 'undefined'])
                df_diff.loc[nan_mask, col] = None
        
        print(f"üìù Updating {len(df_diff)} existing records...")
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå primary key ‡∏Ç‡∏≠‡∏á df_diff ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ NaN ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        df_diff_valid = df_diff[df_diff[pk_column].notna().all(axis=1)].copy()
        dropped = len(df_diff) - len(df_diff_valid)
        if dropped > 0:
            print(f"‚ö†Ô∏è Skipped {dropped} rows with null primary keys in update data")
        
        if not df_diff_valid.empty:
            print(f"üìù Processing {len(df_diff_valid)} valid records for update...")
            
            # ‡πÉ‡∏ä‡πâ batch size ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ connection
            batch_size = 5000
            total_batches = (len(df_diff_valid) + batch_size - 1) // batch_size
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ connection ‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch
            for i in range(0, len(df_diff_valid), batch_size):
                batch_df = df_diff_valid.iloc[i:i+batch_size]
                batch_num = (i // batch_size) + 1
                print(f"  üì¶ Processing update batch {batch_num}/{total_batches} ({len(batch_df)} records)")
                
                # ‡πÉ‡∏ä‡πâ executemany ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch update
                records = batch_df.to_dict(orient='records')
                
                # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î records ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                records = clean_records_for_db(records)
                
                # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á fresh engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ update batch
                fresh_target_update = create_fresh_target_engine()
                if not fresh_target_update:
                    print(f"    ‚ùå Failed to create fresh target engine for update batch {batch_num}. Skipping this batch.")
                    continue
                
                # ‚úÖ ‡πÉ‡∏ä‡πâ connection ‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ batch ‡∏û‡∏£‡πâ‡∏≠‡∏° retry logic
                max_retries = 3
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        with fresh_target_update.begin() as conn:
                            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ timeout
                            conn.execute(text("SET statement_timeout = 300000"))  # 5 minutes
                            conn.execute(text("SET idle_in_transaction_session_timeout = 300000"))  # 5 minutes
                            
                            stmt = pg_insert(metadata).values(records)
                            update_columns = {
                                c.name: stmt.excluded[c.name]
                                for c in metadata.columns
                                if c.name not in pk_column
                            }
                            # update_at ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                            update_columns['update_at'] = datetime.now()

                            stmt = stmt.on_conflict_do_update(
                                index_elements=pk_column,
                                set_=update_columns
                            )
                            conn.execute(stmt)
                        
                        print(f"    ‚úÖ Update batch {batch_num} completed successfully")
                        fresh_target_update.dispose()
                        break
                        
                    except Exception as e:
                        retry_count += 1
                        print(f"    ‚ùå Error updating batch {batch_num} (attempt {retry_count}/{max_retries}): {e}")
                        
                        if retry_count >= max_retries:
                            print(f"    ‚ö†Ô∏è Max retries reached for update batch {batch_num}. Skipping this batch.")
                            try:
                                fresh_target_update.dispose()
                            except Exception as dispose_error:
                                print(f"    ‚ö†Ô∏è Error disposing fresh target engine for update batch {batch_num}: {dispose_error}")
                            break
                        
                        # ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
                        time.sleep(2 ** retry_count)  # Exponential backoff
        else:
            print("‚ö†Ô∏è No valid records to update after filtering null primary keys.")
    else:
        print("‚ö†Ô∏è No records to update.")

    print("‚úÖ Insert/update completed.")
    
    # ‚úÖ ‡∏õ‡∏¥‡∏î fresh target engine ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
    try:
        if 'fresh_target_ops' in locals() and fresh_target_ops:
            fresh_target_ops.dispose()
            print("‚úÖ Fresh target engine disposed successfully")
    except Exception as e:
        print(f"‚ö†Ô∏è Error disposing fresh target engine: {e}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î
    engines_to_check = []
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
    for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update']:
        if var_name in locals():
            engine = locals()[var_name]
            if engine is not None:
                engines_to_check.append((var_name, engine))
    
    for engine_name, engine in engines_to_check:
        try:
            engine.dispose()
            print(f"‚úÖ {engine_name} engine disposed successfully")
        except Exception as dispose_error:
            print(f"‚ö†Ô∏è {engine_name} engine disposal failed: {dispose_error}")
    
    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö engines ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô local scope
        for var_name in ['fresh_target', 'fresh_target_batch', 'fresh_target_update', 'fresh_target_ops']:
            if var_name in locals():
                engine = locals()[var_name]
                if engine is not None:
                    try:
                        engine.dispose()
                        print(f"‚úÖ {var_name} engine disposed successfully")
                    except Exception as dispose_error:
                        print(f"‚ö†Ô∏è {var_name} engine disposal failed: {dispose_error}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error during final engine cleanup: {e}")

@job
def fact_installment_payments_etl():
    load_installment_data(clean_installment_data(extract_installment_data()))
        
if __name__ == "__main__":
    df_raw = extract_installment_data()

    df_clean = clean_installment_data((df_raw))

    # output_path = "fact_installment_payments.csv"
    # df_clean.to_csv(output_path, index=False, encoding='utf-8-sig')
    # print(f"üíæ Saved to {output_path}")

    load_installment_data(df_clean)
    print("üéâ completed! Data upserted to fact_installment_payments.")