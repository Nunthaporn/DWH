from dagster import op, job
import pandas as pd
import numpy as np
import os
import re
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.dialects.postgresql import insert as pg_insert

# =========================
# üîß ENV & DB CONNECTIONS
# =========================
load_dotenv()
PG_SCHEMA = os.getenv("PG_SCHEMA", "public")

# MariaDB (source)
source_engine = create_engine(
    f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
    f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/fininsurance",
    pool_pre_ping=True
)

# PostgreSQL (target)
target_engine = create_engine(
    f"postgresql+psycopg2://{os.getenv('DB_USER_test')}:{os.getenv('DB_PASSWORD_test')}@"
    f"{os.getenv('DB_HOST_test')}:{os.getenv('DB_PORT_test')}/fininsurance",
    connect_args={
        "keepalives": 1,
        "keepalives_idle": 30,
        "keepalives_interval": 10,
        "keepalives_count": 5,
        # üëá ‡∏ï‡∏±‡πâ‡∏á search_path + statement_timeout (‡∏à‡∏∞‡∏õ‡∏¥‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á UPDATE ‡∏î‡πâ‡∏ß‡∏¢ SET LOCAL)
        "options": f"-c search_path={PG_SCHEMA} -c statement_timeout=300000"
    },
    pool_pre_ping=True
)

# =========================
# üîß HELPERS
# =========================
def base_id_series(s: pd.Series) -> pd.Series:
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ agent_id ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î suffix '-defect' ‡∏≠‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á"""
    return s.astype(str).str.strip().str.replace(r"-defect$", "", regex=True)

def normalize_str_col(s: pd.Series) -> pd.Series:
    s = s.astype("string")
    s = s.str.strip()
    s = s.mask(s.str.len() == 0)
    s = s.mask(s.str.lower().isin(["nan", "none", "null", "undefined"]))
    return s

# =========================
# üß≤ EXTRACT + TRANSFORM
# =========================
@op
def extract_agent_mapping() -> pd.DataFrame:
    """
    ‡∏î‡∏∂‡∏á agent ‡∏à‡∏≤‡∏Å fin_system_pay (source) + quotation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å fact_sales_quotation (target)
    ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏∏‡∏ö agent_id ‡∏ù‡∏±‡πà‡∏á‡∏°‡∏¥‡∏ï‡∏¥ (dim_agent) ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ standard ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠ base_id
    ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å agent_id ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ quotation_num
    """
    # 1) fin_system_pay (source)
    df_career = pd.read_sql(text("SELECT quo_num, id_cus FROM fin_system_pay"), source_engine)
    df_career = df_career.rename(columns={"id_cus": "agent_id", "quo_num": "quotation_num"})
    df_career["agent_id"] = normalize_str_col(df_career["agent_id"])

    # 2) quotation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô fact_sales_quotation (target)
    df_fact = pd.read_sql(text(f"SELECT quotation_num FROM {PG_SCHEMA}.fact_sales_quotation"), target_engine)

    # 3) right-join ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ó‡∏∏‡∏Å quotation ‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô pay
    df_m1 = pd.merge(df_career, df_fact, on="quotation_num", how="right")

    # 4) ‡∏°‡∏¥‡∏ï‡∏¥ agent (target) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ standardization
    df_main = pd.read_sql(text(f"SELECT agent_id FROM {PG_SCHEMA}.dim_agent"), target_engine)
    df_main["agent_id"] = normalize_str_col(df_main["agent_id"]).dropna()

    # 4.1) ‡∏ó‡∏≥‡∏Ñ‡∏µ‡∏¢‡πå base ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô/‡∏ï‡∏±‡∏ß defect
    dfm = df_main.copy()
    dfm["__base"] = base_id_series(dfm["agent_id"])
    dfm["__is_defect"] = dfm["agent_id"].str.contains(r"-defect$", case=False, na=False)
    dup_mask = dfm["__base"].duplicated(keep=False)

    main_single = dfm[~dup_mask].copy()
    # sort: non-defect ‡∏Å‡πà‡∏≠‡∏ô, defect ‡∏´‡∏•‡∏±‡∏á ‚Üí keep='last' ‡∏à‡∏∞‡πÑ‡∏î‡πâ defect ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    main_dups = (
        dfm[dup_mask]
        .sort_values(["__base", "__is_defect"])
        .drop_duplicates("__base", keep="last")
    )
    df_main_norm = pd.concat([main_single, main_dups], ignore_index=True)

    # 5) ‡∏ó‡∏≥‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô mapping
    df_m1["__base"] = base_id_series(df_m1["agent_id"])

    # 6) ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ base
    df_join = pd.merge(
        df_m1,
        df_main_norm.drop(columns=["__is_defect"], errors="ignore"),
        on="__base",
        how="left",
        suffixes=("_m1", "_main"),
        indicator=False,
    )

    # 7) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å agent_id_final
    df_join["agent_id_final"] = np.where(
        df_join["agent_id_main"].notna(), df_join["agent_id_main"], df_join["agent_id_m1"]
    )

    # 8) ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
    df_out = df_join[["quotation_num", "agent_id_final"]].rename(columns={"agent_id_final": "agent_id"})
    df_out["agent_id"] = normalize_str_col(df_out["agent_id"])

    # ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥ quotation_num ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà agent_id ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô
    df_out["__has_agent"] = df_out["agent_id"].notna().astype(int)
    df_out = (
        df_out.sort_values(["quotation_num", "__has_agent"], ascending=[True, False])
        .drop_duplicates("quotation_num", keep="first")
        .drop(columns="__has_agent")
    )
    return df_out

# =========================
# üßπ STAGE TEMP TABLE
# =========================
@op
def stage_dim_agent_temp(df_map: pd.DataFrame) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß {PG_SCHEMA}.dim_agent_temp (quotation_num, agent_id)
    ‡πÅ‡∏•‡∏∞ normalize ‡∏ï‡∏±‡∏ß‡∏™‡∏∞‡∏Å‡∏î agent_id ‡∏ï‡∏≤‡∏° {PG_SCHEMA}.dim_agent (case-insensitive)
    ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô + ANALYZE
    """
    if df_map.empty:
        df_map = pd.DataFrame({
            "quotation_num": pd.Series(dtype="string"),
            "agent_id": pd.Series(dtype="string")
        })

    df_tmp = df_map.copy()
    df_tmp["quotation_num"] = normalize_str_col(df_tmp["quotation_num"])
    df_tmp["agent_id"] = normalize_str_col(df_tmp["agent_id"])

    # üëá ‡πÉ‡∏™‡πà schema ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    df_tmp.to_sql(
        "dim_agent_temp",
        target_engine,
        if_exists="replace",
        index=False,
        method="multi",
        chunksize=20_000,
        schema=PG_SCHEMA,
    )
    print(f"‚úÖ staged to {PG_SCHEMA}.dim_agent_temp: {len(df_tmp):,} rows")

    # üëá normalize casing ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö dim_agent
    normalize_query = text(f"""
        UPDATE {PG_SCHEMA}.dim_agent_temp t
        SET agent_id = da.agent_id
        FROM {PG_SCHEMA}.dim_agent da
        WHERE LOWER(da.agent_id) = LOWER(t.agent_id)
          AND t.agent_id IS DISTINCT FROM da.agent_id;
    """)

    # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á indexes (‡∏ñ‡∏≤‡∏ß‡∏£‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß) + ANALYZE
    # ddl = f"""
    # DO $$
    # BEGIN
    #     -- ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ index ‡∏ù‡∏±‡πà‡∏á‡∏ñ‡∏≤‡∏ß‡∏£ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á (‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    #     EXECUTE 'CREATE INDEX IF NOT EXISTS idx_fsq_quotation_num ON {PG_SCHEMA}.fact_sales_quotation(quotation_num)';
    #     EXECUTE 'CREATE INDEX IF NOT EXISTS idx_dim_agent_agent_id ON {PG_SCHEMA}.dim_agent(agent_id)';
    # END$$;

    # CREATE INDEX IF NOT EXISTS idx_dim_agent_temp_q ON {PG_SCHEMA}.dim_agent_temp(quotation_num);
    # CREATE INDEX IF NOT EXISTS idx_dim_agent_temp_a ON {PG_SCHEMA}.dim_agent_temp(agent_id);

    # ANALYZE {PG_SCHEMA}.dim_agent_temp;
    # """

    # with target_engine.begin() as conn:
    #     res = conn.execute(normalize_query)
    #     print(f"üîÑ normalized agent_id casing: {res.rowcount} rows")
    #     conn.execute(text(ddl))
    #     print("üîß indexes ready + ANALYZE on temp done")

    return f"{PG_SCHEMA}.dim_agent_temp"

# =========================
# üöÄ APPLY UPDATE TO FACT
# =========================
@op
def update_fact_from_temp(temp_table_name: str) -> int:
    """
    ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {PG_SCHEMA}.fact_sales_quotation.agent_id ‡∏à‡∏≤‡∏Å temp table (‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà quotation_num)
    ‡πÇ‡∏î‡∏¢ **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á join dim_agent ‡∏≠‡∏µ‡∏Å** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ agent_id ‡πÉ‡∏ô temp ‡∏ñ‡∏π‡∏Å normalize ‡πÅ‡∏•‡πâ‡∏ß
    ‡∏õ‡∏¥‡∏î statement_timeout ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏£‡∏≤‡∏ô‡πÅ‡∏ã‡∏Å‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á
    """
    if not temp_table_name:
        print("‚ö†Ô∏è temp table name missing, skip update.")
        return 0

    if "." not in temp_table_name:
        temp_table_name = f"{PG_SCHEMA}.{temp_table_name}"

    update_sql = f"""
        -- ‡∏õ‡∏¥‡∏î timeout ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏£‡∏≤‡∏ô‡πÅ‡∏ã‡∏Å‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ
        SET LOCAL statement_timeout = 0;

        WITH cand AS (
            SELECT fsq.ctid AS fsq_ctid, dc.agent_id AS new_agent
            FROM {PG_SCHEMA}.fact_sales_quotation fsq
            JOIN {temp_table_name} dc
              ON fsq.quotation_num = dc.quotation_num
            WHERE fsq.agent_id IS DISTINCT FROM dc.agent_id
        )
        UPDATE {PG_SCHEMA}.fact_sales_quotation fsq
        SET agent_id = cand.new_agent
        FROM cand
        WHERE fsq.ctid = cand.fsq_ctid;
    """
    with target_engine.begin() as conn:
        res = conn.execute(text(update_sql))
        updated = res.rowcount or 0

    print(f"‚úÖ fact_sales_quotation updated: {updated} rows")
    return updated

# =========================
# üßπ CLEANUP TEMP
# =========================
@op
def drop_dim_agent_temp(temp_table_name: str) -> None:
    if not temp_table_name:
        return
    if "." not in temp_table_name:
        temp_table_name = f"{PG_SCHEMA}.{temp_table_name}"
    with target_engine.begin() as conn:
        conn.execute(text(f"DROP TABLE IF EXISTS {temp_table_name};"))
    print(f"üóëÔ∏è dropped {temp_table_name}")

# =========================
# üß± DAGSTER JOB
# =========================
@job
def update_agent_id_on_fact():
    temp = stage_dim_agent_temp(extract_agent_mapping())
    _ = update_fact_from_temp(temp)
    drop_dim_agent_temp(temp)

# =========================
# ‚ñ∂Ô∏è LOCAL RUN (optional)
# =========================
if __name__ == "__main__":
    df_map = extract_agent_mapping()
    tname = stage_dim_agent_temp(df_map)
    updated = update_fact_from_temp(tname)
    drop_dim_agent_temp(tname)
    print(f"üéâ done. updated rows = {updated}")
